---
title: Machine Learning with caret in R
author: ''
date: '2021-09-01'
slug: []
categories: []
tags:
  - Machine Learning
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidyverse)
library(caret)
Boston &lt;- read_csv(&quot;Boston.csv&quot;)</code></pre>
<div id="in-sample-rmse-for-linear-regression-on-diamonds" class="section level2">
<h2>1-3 In-sample RMSE for linear regression on diamonds</h2>
<p>As you saw in the video, included in the course is the diamonds dataset, which is a classic dataset from the ggplot2 package. The dataset contains physical attributes of diamonds as well as the price they sold for. One interesting modeling challenge is predicting diamond price based on their attributes using something like a linear regression.</p>
<p>Recall that to fit a linear regression, you use the lm() function in the following format:</p>
<p>mod &lt;- lm(y ~ x, my_data)
To make predictions using mod on the original data, you call the predict() function:</p>
<p>pred &lt;- predict(mod, my_data)</p>
<pre class="r"><code># Fit lm model: model
model &lt;- lm(price~., diamonds)

# Predict on full data: p
p &lt;- predict(model, diamonds)

# Compute errors: error
error &lt;- p-diamonds$price

# Calculate RMSE
sqrt(mean(error ^ 2))</code></pre>
<pre><code>## [1] 1129.843</code></pre>
<pre class="r"><code>#rmse(model,diamonds)</code></pre>
</div>
<div id="randomly-order-the-data-frame" class="section level2">
<h2>1-6 Randomly order the data frame</h2>
<p>One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands.</p>
<p>First, you set a random seed so that your work is reproducible and you get the same random split each time you run your script:</p>
<p>set.seed(42)</p>
<p>Next, you use the sample() function to shuffle the row indices of the diamonds dataset. You can later use these indices to reorder the dataset.</p>
<p>rows &lt;- sample(nrow(diamonds))</p>
<p>Finally, you can use this random vector to reorder the diamonds dataset:</p>
<p>diamonds &lt;- diamonds[rows, ]</p>
<pre class="r"><code># Set seed
set.seed(42)

# Shuffle row indices: rows
rows &lt;- sample(nrow(diamonds))

# Randomly order data
shuffled_diamonds &lt;- diamonds &lt;- diamonds[rows, ]</code></pre>
</div>
<div id="try-an-8020-split" class="section level2">
<h2>1-7 Try an 80/20 split</h2>
<p>Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set. You can do this by choosing a split point approximately 80% of the way through your data:</p>
<p>split &lt;- round(nrow(mydata) * 0.80)</p>
<p>You can then use this point to break off the first 80% of the dataset as a training set:</p>
<p>mydata[1:split, ]</p>
<p>And then you can use that same point to determine the test set:</p>
<p>mydata[(split + 1):nrow(mydata), ]</p>
<pre class="r"><code># Determine row to split on: split
split &lt;- round(nrow(diamonds) * 0.80)

# Create train
train &lt;- diamonds[1:split, ]

# Create test
test &lt;- diamonds[(split + 1):nrow(diamonds), ]</code></pre>
</div>
<div id="predict-on-test-set" class="section level2">
<h2>1-8 Predict on test set</h2>
<p>Now that you have a randomly split training set and test set, you can use the lm() function as you did in the first exercise to fit a model to your training set, rather than the entire dataset. Recall that you can use the formula interface to the linear regression function to fit a model with a specified target variable using all other variables in the dataset as predictors:</p>
<p>mod &lt;- lm(y ~ ., training_data)
You can use the predict() function to make predictions from that model on new data. The new dataset must have all of the columns from the training data, but they can be in a different order with different values. Here, rather than re-predicting on the training set, you can predict on the test set, which you did not use for training the model. This will allow you to determine the out-of-sample error for the model in the next exercise:</p>
<p>p &lt;- predict(model, new_data)</p>
<pre class="r"><code># Fit lm model on train: model
model &lt;- lm(price~., train)

# Predict on test: p
p &lt;- predict(model, test)</code></pre>
</div>
<div id="calculate-test-set-rmse-by-hand" class="section level2">
<h2>1-9 Calculate test set RMSE by hand</h2>
<p>Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise. You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values.</p>
<p>Once you have an error vector, calculating RMSE is as simple as squaring it, taking the mean, then taking the square root:</p>
<p>sqrt(mean(error^2))</p>
<pre class="r"><code># Compute errors: error
error &lt;- p-test$price

# Calculate RMSE
sqrt(mean(error^2))</code></pre>
<pre><code>## [1] 1137.466</code></pre>
</div>
<div id="fold-cross-validation" class="section level2">
<h2>1-13 10-fold cross-validation</h2>
<p>As you saw in the video, a better approach to validating models is to use multiple systematic test sets, rather than a single random train/test split. Fortunately, the caret package makes this very easy to do:</p>
<p>model &lt;- train(y ~ ., my_data)</p>
<p>caret supports many types of cross-validation, and you can specify which type of cross-validation and the number of cross-validation folds with the trainControl() function, which you pass to the trControl argument in train():</p>
<p>model &lt;- train(
y ~ .,
my_data,
method = “lm”,
trControl = trainControl(
method = “cv”,
number = 10,
verboseIter = TRUE
)
)</p>
<p>It’s important to note that you pass the method for modeling to the main train() function and the method for cross-validation to the trainControl() function.</p>
<pre class="r"><code># Fit lm model using 10-fold CV: model
model &lt;- train(
  price~., 
  diamonds,
  method = &quot;lm&quot;,
  trControl = trainControl(
    method = &quot;cv&quot;, 
    number = 10,
    verboseIter = TRUE
  )
)</code></pre>
<pre><code>## + Fold01: intercept=TRUE 
## - Fold01: intercept=TRUE 
## + Fold02: intercept=TRUE 
## - Fold02: intercept=TRUE 
## + Fold03: intercept=TRUE 
## - Fold03: intercept=TRUE 
## + Fold04: intercept=TRUE 
## - Fold04: intercept=TRUE 
## + Fold05: intercept=TRUE 
## - Fold05: intercept=TRUE 
## + Fold06: intercept=TRUE 
## - Fold06: intercept=TRUE 
## + Fold07: intercept=TRUE 
## - Fold07: intercept=TRUE 
## + Fold08: intercept=TRUE 
## - Fold08: intercept=TRUE 
## + Fold09: intercept=TRUE 
## - Fold09: intercept=TRUE 
## + Fold10: intercept=TRUE 
## - Fold10: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code># Print model to console
model</code></pre>
<pre><code>## Linear Regression 
## 
## 53940 samples
##     9 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 48547, 48546, 48546, 48547, 48545, 48547, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1130.633  0.9197711  740.5017
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
</div>
<div id="fold-cross-validation-1" class="section level2">
<h2>1-14 5-fold cross-validation</h2>
<p>In this course, you will use a wide variety of datasets to explore the full flexibility of the caret package. Here, you will use the famous Boston housing dataset, where the goal is to predict median home values in various Boston suburbs.</p>
<p>You can use exactly the same code as in the previous exercise, but change the dataset used by the model:</p>
<p>model &lt;- train(
medv ~ .,
Boston, # &lt;- new!
method = “lm”,
trControl = trainControl(
method = “cv”,
number = 10,
verboseIter = TRUE
)
)</p>
<p>Next, you can reduce the number of cross-validation folds from 10 to 5 using the number argument to the trainControl() argument:</p>
<p>trControl = trainControl(
method = “cv”,
number = 5,
verboseIter = TRUE
)</p>
<pre class="r"><code># Fit lm model using 5-fold CV: model
model &lt;- train(
  medv ~ ., 
  Boston,
  method = &quot;lm&quot;,
  trControl = trainControl(
    method = &quot;cv&quot;, 
    number = 5,
    verboseIter = TRUE
  )
)</code></pre>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code># Print model to console
print(model)</code></pre>
<pre><code>## Linear Regression 
## 
## 506 samples
##  14 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 405, 405, 406, 403, 405 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   4.864695  0.7206197  3.396854
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
</div>
<div id="x-5-fold-cross-validation" class="section level2">
<h2>1-15 5 x 5-fold cross-validation</h2>
<p>You can do more than just one iteration of cross-validation. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.</p>
<p>One of the awesome things about the train() function in caret is how easy it is to run very different models or methods of cross-validation just by tweaking a few simple arguments to the function call. For example, you could repeat your entire cross-validation procedure 5 times for greater confidence in your estimates of the model’s out-of-sample accuracy, e.g.:</p>
<p>trControl = trainControl(
method = “repeatedcv”,
number = 5,
repeats = 5,
verboseIter = TRUE
)</p>
<pre class="r"><code># Fit lm model using 5 x 5-fold CV: model
model &lt;- train(
  medv ~ ., 
  Boston,
  method = &quot;lm&quot;,
  trControl = trainControl(
    method = &quot;repeatedcv&quot;, 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
print(model)</code></pre>
</div>
<div id="making-predictions-on-new-data" class="section level2">
<h2>1-16 Making predictions on new data</h2>
<p>Finally, the model you fit with the train() function has the exact same predict() interface as the linear regression models you fit earlier in this chapter.</p>
<p>After fitting a model with train(), you can simply call predict() with new data, e.g:</p>
<p>predict(my_model, new_data)</p>
<pre class="r"><code># Predict on full Boston dataset
predict(model, Boston)</code></pre>
</div>
