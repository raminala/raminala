---
title: Unsupervised Learning in R
author: ''
date: '2021-08-26'
slug: []
categories: []
tags:
  - Unsupervised Learning
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.2     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>x &lt;- read.csv(&quot;x.csv&quot;) %&gt;%select(c(V1,V2))
pokemon&lt;-read.csv(&quot;pokemon.csv&quot;)%&gt;%select(-c(X))
x2 &lt;- read.csv(&quot;x2.csv&quot;)

x %&gt;% ggplot(aes(V1,V2))+geom_point(alpha=0.9, shape=&quot;x&quot;, size=2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div id="k-means-clustering" class="section level2">
<h2>1-4 k-means clustering</h2>
<p>We have created some two-dimensional data and stored it in a variable called x in your workspace. Above scatter plot is a visual representation of the data.</p>
<p>In this exercise, your task is to create a k-means model of the x data using 3 clusters, then to look at the structure of the resulting model using the summary() function.</p>
<pre class="r"><code># Create the k-means model: km.out
km.out &lt;- kmeans(x, centers = 3, nstart = 200)

# Inspect the result
summary(km.out)</code></pre>
<pre><code>##              Length Class  Mode   
## cluster      300    -none- numeric
## centers        6    -none- numeric
## totss          1    -none- numeric
## withinss       3    -none- numeric
## tot.withinss   1    -none- numeric
## betweenss      1    -none- numeric
## size           3    -none- numeric
## iter           1    -none- numeric
## ifault         1    -none- numeric</code></pre>
</div>
<div id="results-of-kmeans" class="section level2">
<h2>1-5 Results of kmeans()</h2>
<p>The kmeans() function produces several outputs. In the video, we discussed one output of modeling, the cluster membership.</p>
<p>In this exercise, you will access the cluster component directly. This is useful anytime you need the cluster membership for each observation of the data used to build the clustering model. A future exercise will show an example of how this cluster membership might be used to help communicate the results of k-means modeling.</p>
<p>k-means models also have a print method to give a human friendly output of basic modeling results. This is available by using print() or simply typing the name of the model.</p>
<pre class="r"><code># Print the cluster membership component of the model

print(km.out$cluster)</code></pre>
<pre><code>##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1
##  [38] 1 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1
##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3
## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [186] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [223] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 1 1 1 1
## [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1
## [297] 1 2 1 1</code></pre>
<pre class="r"><code># Print the km.out object
print(km.out)</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 52, 98, 150
## 
## Cluster means:
##           V1          V2
## 1  0.6642455 -0.09132968
## 2  2.2171113  2.05110690
## 3 -5.0556758  1.96991743
## 
## Clustering vector:
##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1
##  [38] 1 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1
##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3
## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [186] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [223] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 1 1 1 1
## [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1
## [297] 1 2 1 1
## 
## Within cluster sum of squares by cluster:
## [1]  95.50625 148.64781 295.16925
##  (between_SS / total_SS =  87.2 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
</div>
<div id="visualizing-and-interpreting-results-of-kmeans" class="section level2">
<h2>1-6 Visualizing and interpreting results of kmeans()</h2>
<p>One of the more intuitive ways to interpret the results of k-means models is by plotting the data as a scatter plot and using color to label the samples’ cluster membership. In this exercise, you will use the standard plot() function to accomplish this.</p>
<p>To create a scatter plot, you can pass data with two features (i.e. columns) to plot() with an extra argument col = km.out$cluster, which sets the color of each point in the scatter plot according to its cluster membership.</p>
<pre class="r"><code># Scatter plot of x
x %&gt;%
  ggplot(aes(V1,V2))+geom_point(color=km.out$cluster)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="handling-random-algorithms" class="section level2">
<h2>1-8 Handling random algorithms</h2>
<p>In the video, you saw how kmeans() randomly initializes the centers of clusters. This random initialization can result in assigning observations to different cluster labels. Also, the random initialization can result in finding different local minima for the k-means algorithm. This exercise will demonstrate both results.</p>
<p>At the top of each plot, the measure of model quality—total within cluster sum of squares error—will be plotted. Look for the model(s) with the lowest error to find models with the better model results.</p>
<p>Because kmeans() initializes observations to random clusters, it is important to set the random number generator seed for reproducibility.</p>
<pre class="r"><code># Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on x with three clusters and one start
  km.out &lt;- kmeans(x, centers = 3, nstart = 1)
  
  # Plot clusters
  plot(x, col = km.out$cluster, 
       main = km.out$tot.withinss)
}</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on x with three clusters and one start
  km.out &lt;- kmeans(x, centers = 3, nstart = 1)
  
  # Plot clusters
  plot(x, col = km.out$cluster, 
       main = km.out$tot.withinss)
}</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="selecting-number-of-clusters" class="section level2">
<h2>1-9 Selecting number of clusters</h2>
<p>The k-means algorithm assumes the number of clusters as part of the input. If you know the number of clusters in advance (e.g. due to certain business constraints) this makes setting the number of clusters easy. However, as you saw in the video, if you do not know the number of clusters and need to determine it, you will need to run the algorithm multiple times, each time with a different number of clusters. From this, you can observe how a measure of model quality changes with the number of clusters.</p>
<p>In this exercise, you will run kmeans() multiple times to see how model quality changes as the number of clusters changes. Plots displaying this information help to determine the number of clusters and are often referred to as scree plots.</p>
<p>The ideal plot will have an elbow where the quality measure improves more slowly as the number of clusters increases. This indicates that the quality of the model is no longer improving substantially as the model complexity (i.e. number of clusters) increases. In other words, the elbow indicates the number of clusters inherent in the data.</p>
<pre class="r"><code># Initialize total within sum of squares error: wss
wss &lt;- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out &lt;- kmeans(x, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] &lt;- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = &quot;b&quot;, 
     xlab = &quot;Number of Clusters&quot;, 
     ylab = &quot;Within groups sum of squares&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># Set k equal to the number of clusters corresponding to the elbow location
k &lt;- 2  # 3 is probably OK, too</code></pre>
</div>
<div id="practical-matters-working-with-real-data" class="section level2">
<h2>1-11 Practical matters: working with real data</h2>
<p>Dealing with real data is often more challenging than dealing with synthetic data. Synthetic data helps with learning new concepts and techniques, but the next few exercises will deal with data that is closer to the type of real data you might find in your professional or academic pursuits.</p>
<p>The first challenge with the Pokemon data is that there is no pre-determined number of clusters. You will determine the appropriate number of clusters, keeping in mind that in real data the elbow in the scree plot might be less of a sharp elbow than in synthetic data. Use your judgement on making the determination of the number of clusters.</p>
<p>The second part of this exercise includes plotting the outcomes of the clustering on two dimensions, or features, of the data. These features were chosen somewhat arbitrarily for this exercise. Think about how you would use plotting and clustering to communicate interesting groups of Pokemon to other people.</p>
<p>An additional note: this exercise utilizes the iter.max argument to kmeans(). As you’ve seen, kmeans() is an iterative algorithm, repeating over and over until some stopping criterion is reached. The default number of iterations for kmeans() is 10, which is not enough for the algorithm to converge and reach its stopping criterion, so we’ll set the number of iterations to 50 to overcome this issue. To see what happens when kmeans() does not converge, try running the example with a lower number of iterations (e.g. 3). This is another example of what might happen when you encounter real data and use real cases.</p>
<pre class="r"><code># Initialize total within sum of squares error: wss
wss &lt;- 0

# Look over 1 to 15 possible clusters
for (i in 1:15) {
  # Fit the model: km.out
  km.out &lt;- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] &lt;- km.out$tot.withinss
}

# Produce a scree plot
plot(1:15, wss, type = &quot;b&quot;, 
     xlab = &quot;Number of Clusters&quot;, 
     ylab = &quot;Within groups sum of squares&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Select number of clusters
k &lt;- 3

# Build model with k clusters: km.out
km.out &lt;- kmeans(pokemon, centers = k, nstart = 20, iter.max = 50)

# View the resulting model
km.out</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 270, 355, 175
## 
## Cluster means:
##   HitPoints   Attack   Defense SpecialAttack SpecialDefense    Speed
## 1  81.90370 96.15926  77.65556     104.12222       86.87778 94.71111
## 2  54.68732 56.93239  53.64507      52.02254       53.04789 53.58873
## 3  79.30857 97.29714 108.93143      66.71429       87.04571 57.29143
## 
## Clustering vector:
##   [1] 2 2 1 1 2 2 1 1 1 2 2 3 1 2 2 2 2 2 2 1 2 2 1 1 2 2 2 1 2 1 2 1 2 3 2 2 3
##  [38] 2 2 1 2 1 2 1 2 2 2 1 2 2 1 2 3 2 1 2 2 2 1 2 1 2 1 2 1 2 2 3 2 1 1 1 2 3
##  [75] 3 2 2 1 2 1 2 3 3 2 1 2 3 3 2 1 2 2 1 2 3 2 3 2 3 2 1 1 1 3 2 3 2 3 2 1 2
## [112] 1 2 3 3 3 2 2 3 2 3 2 3 3 3 2 1 2 3 2 1 1 1 1 1 1 3 3 3 2 3 3 3 2 2 1 1 1
## [149] 2 2 3 2 3 1 1 3 1 1 1 2 2 1 1 1 1 1 2 2 3 2 2 1 2 2 3 2 2 2 1 2 2 2 2 1 2
## [186] 1 2 2 2 2 2 2 1 2 2 1 1 3 2 2 3 1 2 2 1 2 2 2 2 2 3 1 3 2 3 1 2 2 1 2 3 2
## [223] 3 3 3 2 3 2 3 3 3 3 3 2 2 3 2 3 2 3 2 2 1 2 1 3 2 1 1 1 2 3 1 1 2 2 3 2 2
## [260] 2 3 1 1 1 3 2 2 3 3 1 1 1 2 2 1 1 2 2 1 1 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 1
## [297] 2 2 1 2 2 2 3 2 2 1 1 2 2 2 3 2 2 1 2 1 2 2 2 1 2 3 2 3 2 2 2 3 2 3 2 3 3
## [334] 3 2 2 1 2 1 1 2 2 2 2 2 2 3 2 1 1 2 1 2 1 3 3 2 1 2 2 2 1 2 1 2 3 1 1 1 1
## [371] 3 2 3 2 3 2 3 2 3 2 3 2 1 2 3 2 1 1 2 3 3 2 1 1 2 2 1 1 2 2 1 2 3 3 3 2 2
## [408] 3 1 1 2 3 3 1 3 3 3 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 2 3 3 2 2 1 2 2 1 2 2 1
## [445] 2 2 2 2 2 2 1 2 1 2 3 2 3 2 3 3 3 1 2 3 2 2 1 2 1 2 3 1 2 1 2 1 1 1 1 2 1
## [482] 2 2 1 2 3 2 2 2 2 3 2 2 1 1 2 2 1 1 2 3 2 3 2 1 3 2 1 2 2 1 3 1 1 3 3 3 1
## [519] 1 1 1 3 1 3 1 1 1 1 3 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2
## [556] 2 1 2 2 1 2 2 1 2 2 2 2 3 2 1 2 1 2 1 2 1 2 3 2 2 1 2 1 2 3 3 2 1 2 1 3 3
## [593] 2 3 3 2 2 1 3 3 2 2 1 2 2 1 2 1 2 1 1 2 2 1 2 3 1 1 2 3 2 3 1 2 3 2 3 2 1
## [630] 2 3 2 1 2 1 2 2 3 2 2 1 2 1 2 2 1 2 1 1 2 3 2 3 2 1 3 2 1 2 3 2 3 3 2 2 1
## [667] 2 1 2 2 1 2 3 3 2 3 1 2 1 3 2 1 3 2 3 2 3 3 2 3 2 3 1 3 2 2 1 2 1 1 1 1 1
## [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 2 2 1 2 2 1 2 2 2 2 1 2 2 2 2 1 2 2 1
## [741] 2 1 2 3 1 2 1 1 2 3 1 3 2 3 2 1 2 3 2 3 2 3 2 1 2 1 2 3 2 1 1 1 1 3 2 1 1
## [778] 3 2 3 2 2 2 2 3 3 3 3 2 3 2 1 1 1 3 3 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 1018348.0  812079.9  709020.5
##  (between_SS / total_SS =  40.8 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<pre class="r"><code># Plot of Defense vs. Speed by cluster membership
plot(pokemon[, c(&quot;Defense&quot;, &quot;Speed&quot;)],
     col = km.out$cluster,
     main = paste(&quot;k-means clustering of Pokemon with&quot;, k, &quot;clusters&quot;),
     xlab = &quot;Defense&quot;, ylab = &quot;Speed&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
</div>
<div id="hierarchical-clustering-with-results" class="section level2">
<h2>2-2 Hierarchical clustering with results</h2>
<p>In this exercise, you will create your first hierarchical clustering model using the hclust() function.</p>
<p>We have created some data that has two dimensions and placed it in a variable called x2. Your task is to create a hierarchical clustering model of x2. Remember from the video that the first step to hierarchical clustering is determining the similarity between observations, which you will do with the dist() function.</p>
<p>You will look at the structure of the resulting model using the summary() function.</p>
<pre class="r"><code># Create hierarchical clustering model: hclust.out
hclust.out &lt;- hclust(d=dist(x2) )

# Inspect the result
summary(hclust.out)</code></pre>
<pre><code>##             Length Class  Mode     
## merge       98     -none- numeric  
## height      49     -none- numeric  
## order       50     -none- numeric  
## labels       0     -none- NULL     
## method       1     -none- character
## call         2     -none- call     
## dist.method  1     -none- character</code></pre>
</div>
<div id="cutting-the-tree" class="section level2">
<h2>2-5 Cutting the tree</h2>
<p>Remember from the video that cutree() is the R function that cuts a hierarchical model. The h and k arguments to cutree() allow you to cut the tree based on a certain height h or a certain number of clusters k.</p>
<p>In this exercise, you will use cutree() to cut the hierarchical model you created earlier based on each of these two criteria.</p>
<pre class="r"><code># Cut by height
cutree(hclust.out, h = 7)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 2 2 2
## [39] 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<pre class="r"><code># Cut by number of clusters
cutree(hclust.out, k = 3)</code></pre>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 2 2 2
## [39] 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
</div>
<div id="linkage-methods" class="section level2">
<h2>2-7 Linkage methods</h2>
<p>In this exercise, you will produce hierarchical clustering models using different linkages and plot the dendrogram for each, observing the overall structure of the trees.</p>
<p>You’ll be asked to interpret the results in the next exercise.</p>
<pre class="r"><code># Cluster using complete linkage: hclust.complete
hclust.complete &lt;- hclust(dist(x2), method = &quot;complete&quot;)

# Cluster using average linkage: hclust.average
hclust.average &lt;- hclust(dist(x2), method = &quot;average&quot;)

# Cluster using single linkage: hclust.single
hclust.single &lt;- hclust(dist(x2), method = &quot;single&quot;)

# Plot dendrogram of hclust.complete
plot(hclust.complete, main = &quot;Complete&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code># Plot dendrogram of hclust.average
plot(hclust.average, main = &quot;Average&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code># Plot dendrogram of hclust.single
plot(hclust.single, main = &quot;Single&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
</div>
<div id="practical-matters-scaling" class="section level2">
<h2>2-9 Practical matters: scaling</h2>
<p>Recall from the video that clustering real data may require scaling the features if they have different distributions. So far in this chapter, you have been working with synthetic data that did not need scaling.</p>
<p>In this exercise, you will go back to working with “real” data, the pokemon dataset introduced in the first chapter. You will observe the distribution (mean and standard deviation) of each feature, scale the data accordingly, then produce a hierarchical clustering model using the complete linkage method.</p>
<pre class="r"><code># View column means
colMeans(pokemon)</code></pre>
<pre><code>##      HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
##       69.25875       79.00125       73.84250       72.82000       71.90250 
##          Speed 
##       68.27750</code></pre>
<pre class="r"><code># View column standard deviations
 apply(pokemon, 2, sd)</code></pre>
<pre><code>##      HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
##       25.53467       32.45737       31.18350       32.72229       27.82892 
##          Speed 
##       29.06047</code></pre>
<pre class="r"><code># Scale the data
pokemon.scaled &lt;- scale(pokemon)

# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon &lt;- hclust(dist(pokemon.scaled), method = &quot;complete&quot;)</code></pre>
</div>
<div id="comparing-kmeans-and-hclust" class="section level2">
<h2>2-10 Comparing kmeans() and hclust()</h2>
<p>Comparing k-means and hierarchical clustering, you’ll see the two methods produce different cluster memberships. This is because the two algorithms make different assumptions about how the data is generated. In a more advanced course, we could choose to use one model over another based on the quality of the models’ assumptions, but for now, it’s enough to observe that they are different.</p>
<p>This exercise will have you compare results from the two models on the pokemon dataset to see how they differ.</p>
<pre class="r"><code># Apply cutree() to hclust.pokemon: cut.pokemon
cut.pokemon &lt;- cutree(hclust.pokemon, k = 3)

# Compare methods
table(km.out$cluster, cut.pokemon)</code></pre>
<pre><code>##    cut.pokemon
##       1   2   3
##   1 267   3   0
##   2 350   5   0
##   3 171   3   1</code></pre>
</div>
<div id="pca-using-prcomp" class="section level2">
<h2>3-2 PCA using prcomp()</h2>
<p>In this exercise, you will create your first PCA model and observe the diagnostic results.</p>
<p>We have loaded the Pokemon data from earlier, which has four dimensions, and placed it in a variable called pokemon. Your task is to create a PCA model of the data, then to inspect the resulting model using the summary() function.</p>
</div>
<div id="variance-explained" class="section level2">
<h2>3-8 Variance explained</h2>
<p>The second common plot type for understanding PCA models is a scree plot. A scree plot shows the variance explained as the number of principal components increases. Sometimes the cumulative variance explained is plotted as well.</p>
<p>In this and the next exercise, you will prepare data from the pr.out model you created at the beginning of the chapter for use in a scree plot. Preparing the data for plotting is required because there is not a built-in function in R to create this type of plot.</p>
</div>
<div id="visualize-variance-explained" class="section level2">
<h2>3-9 Visualize variance explained</h2>
<p>Now you will create a scree plot showing the proportion of variance explained by each principal component, as well as the cumulative proportion of variance explained.</p>
<p>Recall from the video that these plots can help to determine the number of principal components to retain. One way to determine the number of principal components to retain is by looking for an elbow in the scree plot showing that as the number of principal components increases, the rate at which variance is explained decreases substantially. In the absence of a clear elbow, you can use the scree plot as a guide for setting a threshold.</p>
</div>
<div id="practical-issues-scaling" class="section level2">
<h2>3-11 Practical issues: scaling</h2>
<p>You saw in the video that scaling your data before doing PCA changes the results of the PCA modeling. Here, you will perform PCA with and without scaling, then visualize the results using biplots.</p>
<p>Sometimes scaling is appropriate when the variances of the variables are substantially different. This is commonly the case when variables have different units of measurement, for example, degrees Fahrenheit (temperature) and miles (distance). Making the decision to use scaling is an important step in performing a principal component analysis.</p>
</div>
