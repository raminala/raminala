---
title: 'Assignment A04: Tree based Method'
author: Ramin Ala
date: '2021-07-29'
slug: []
categories: []
tags:
  - Assignment
  - Tree-Based Models
---

***Ramin Ala***

## Introduction

Random decision forests correct for decision trees' habit of overfitting to their training set.

Random Forest algorithm is a special type of bagging applied to decision trees.

the random forest provides a strong improvement, which consists of applying bagging to the data and bootstrap sampling to the predictor variables at each split. This means that at each splitting step of the tree algorithm, a random sample of n predictors is chosen as split candidates from the full set of the predictors.

## libraries

```{r, message=FALSE}
library(tidyverse)
library(tidymodels)

library(caret)
library(randomForest)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

bank <- read_csv("bank.csv") %>%
  select(c(age, job, marital, education, default, balance, housing, loan, y)) %>%
  mutate(subscription_status=ifelse(y == 'yes', 1, 0)) %>%
  subset(select=-y)
 
#bank %>% group_by(y) %>%  count()
```

## Splitting data

Splitting the data to training (80%) and test set (20%)

```{r}
set.seed(20)

# Split 80% of data as training set
training_bank <- bank$subscription_status %>% 
  createDataPartition(p = 0.8, list = FALSE)

train.data  <- bank[training_bank, ]
test.data <- bank[-training_bank, ]

```

## Computing random forest classifier

```{r}
set.seed(20)

# Fit the model on the training set
model <- train(
  factor(subscription_status) ~., data = train.data, method = "rf",
  trControl = trainControl("cv", number = 10),
  importance = TRUE
  )

# Best tuning parameter
model$bestTune
```

```{r}
# Final model
model$finalModel
```

## Make predictions on the test data

```{r}
predicted.classes <- model %>% predict(test.data)
length(predicted.classes)


fg <- test.data %>%
   bind_cols(predicted.classes)
```

