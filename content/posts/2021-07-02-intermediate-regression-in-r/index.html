---
title: Intermediate Regression in R
author: ''
date: '2021-07-02'
slug: []
categories: []
tags:
  - Regression
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><strong>Standard error: tells us who the estimated average amount differs from the actual value average</strong></p>
<p><strong>A low p-value means that, assuming the null hypothesis is true, there is a very low likelihood that this outcome was a result of luck. A high p-value means that, assuming the null hypothesis is true, this outcome was very likely.</strong></p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.2     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(moderndive)
library(broom)
library(yardstick)</code></pre>
<pre><code>## For binary classification, the first factor level is assumed to be the event.
## Use the argument `event_level = &quot;second&quot;` to alter this as needed.</code></pre>
<pre><code>## 
## Attaching package: &#39;yardstick&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:readr&#39;:
## 
##     spec</code></pre>
<pre class="r"><code>taiwan_real_estate &lt;- read_csv(&quot;taiwan_real_estate.csv&quot;)</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   X1 = col_double(),
##   dist_to_mrt_m = col_double(),
##   n_convenience = col_double(),
##   house_age_years = col_character(),
##   price_twd_msq = col_double()
## )</code></pre>
<pre class="r"><code>churn &lt;- read_csv(&quot;churn.csv&quot;)</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   X1 = col_double(),
##   has_churned = col_double(),
##   time_since_first_purchase = col_double(),
##   time_since_last_purchase = col_double()
## )</code></pre>
<pre class="r"><code>names(churn)</code></pre>
<pre><code>## [1] &quot;X1&quot;                        &quot;has_churned&quot;              
## [3] &quot;time_since_first_purchase&quot; &quot;time_since_last_purchase&quot;</code></pre>
<pre class="r"><code>names(taiwan_real_estate)</code></pre>
<pre><code>## [1] &quot;X1&quot;              &quot;dist_to_mrt_m&quot;   &quot;n_convenience&quot;   &quot;house_age_years&quot;
## [5] &quot;price_twd_msq&quot;</code></pre>
<div id="section" class="section level1">
<h1>1-2</h1>
<pre class="r"><code># Fit a linear regr&#39;n of price_twd_msq vs. n_convenience
mdl_price_vs_conv &lt;- lm(price_twd_msq~n_convenience,taiwan_real_estate)

# See the result
mdl_price_vs_conv</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_real_estate)
## 
## Coefficients:
##   (Intercept)  n_convenience  
##        8.2242         0.7981</code></pre>
<pre class="r"><code>#---------------------------------------------------------

# Fit a linear regr&#39;n of price_twd_msq vs. house_age_years, no intercept
mdl_price_vs_age &lt;- lm(price_twd_msq ~ house_age_years + 0, data = taiwan_real_estate)

# See the result
mdl_price_vs_age</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ house_age_years + 0, data = taiwan_real_estate)
## 
## Coefficients:
##  house_age_years0 to 15  house_age_years15 to 30  house_age_years30 to 45  
##                  12.637                    9.877                   11.393</code></pre>
<pre class="r"><code>#---------------------------------------------------------

# Fit a linear regr&#39;n of price_twd_msq vs. n_convenience 
# plus house_age_years, no intercept
mdl_price_vs_both &lt;- lm(price_twd_msq ~ house_age_years + n_convenience+0, data = taiwan_real_estate)

# See the result
mdl_price_vs_both</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ house_age_years + n_convenience + 
##     0, data = taiwan_real_estate)
## 
## Coefficients:
##  house_age_years0 to 15  house_age_years15 to 30  house_age_years30 to 45  
##                  9.4133                   7.0852                   7.5110  
##           n_convenience  
##                  0.7915</code></pre>
</div>
<div id="section-1" class="section level1">
<h1>1-4</h1>
<pre class="r"><code># Using taiwan_real_estate, plot price_twd_msq vs. n_convenience
taiwan_real_estate %&gt;% ggplot(aes(n_convenience, price_twd_msq))+
  # Add a point layer
  geom_point() +
  # Add a smooth trend line using linear regr&#39;n, no ribbon
  geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>#---------------------------------------------------------


# Using taiwan_real_estate, plot price_twd_msq vs. house_age_years
taiwan_real_estate %&gt;% ggplot(aes(house_age_years, price_twd_msq))+
  # Add a box plot layer
  geom_boxplot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
</div>
<div id="section-2" class="section level1">
<h1>1-5</h1>
<p>When it comes to a linear regression model with a numeric and a categorical explanatory variable, ggplot2 doesn’t have an easy, “out of the box” way to show the predictions. Fortunately, the moderndive package includes an extra geom, geom_parallel_slopes() to make it simple.</p>
<p>taiwan_real_estate is available; ggplot2 and moderndive are loaded.</p>
<pre class="r"><code># Using taiwan_real_estate, plot price_twd_msq vs. n_convenience
# colored by house_age_years
taiwan_real_estate %&gt;% ggplot(aes(n_convenience, price_twd_msq, color=house_age_years))+
  # Add a point layer
  geom_point() +
  # Add parallel slopes, no ribbon
  geom_parallel_slopes(se=FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="section-3" class="section level1">
<h1>1-7</h1>
<pre class="r"><code># Make a grid of explanatory data
explanatory_data &lt;- expand_grid(
  # Set n_convenience to zero to ten
 n_convenience = seq(0, 10),
  # Set house_age_years to the unique values of that variable
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# See the result
explanatory_data</code></pre>
<pre><code>## # A tibble: 33 x 2
##    n_convenience house_age_years
##            &lt;int&gt; &lt;chr&gt;          
##  1             0 30 to 45       
##  2             0 15 to 30       
##  3             0 0 to 15        
##  4             1 30 to 45       
##  5             1 15 to 30       
##  6             1 0 to 15        
##  7             2 30 to 45       
##  8             2 15 to 30       
##  9             2 0 to 15        
## 10             3 30 to 45       
## # ... with 23 more rows</code></pre>
<pre class="r"><code># Add predictions to the data frame
prediction_data &lt;- explanatory_data %&gt;% 
  mutate(prediction_data=predict(
mdl_price_vs_both, explanatory_data
)
  
  )

# See the result
prediction_data</code></pre>
<pre><code>## # A tibble: 33 x 3
##    n_convenience house_age_years prediction_data
##            &lt;int&gt; &lt;chr&gt;                     &lt;dbl&gt;
##  1             0 30 to 45                   7.51
##  2             0 15 to 30                   7.09
##  3             0 0 to 15                    9.41
##  4             1 30 to 45                   8.30
##  5             1 15 to 30                   7.88
##  6             1 0 to 15                   10.2 
##  7             2 30 to 45                   9.09
##  8             2 15 to 30                   8.67
##  9             2 0 to 15                   11.0 
## 10             3 30 to 45                   9.89
## # ... with 23 more rows</code></pre>
<pre class="r"><code>#---------------------------------------------------------

taiwan_real_estate %&gt;% 
  ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE) #+</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>  # Add points using prediction_data, with size 5 and shape 15
  #geom_point(data = prediction_data,size = 5, shape = 15)</code></pre>
</div>
<div id="manually-calculating-predictions" class="section level1">
<h1>1-8 Manually calculating predictions</h1>
<pre class="r"><code># Get the coefficients from mdl_price_vs_both
coeffs &lt;- coefficients(mdl_price_vs_both)

# Extract the slope coefficient
slope &lt;- coeffs[1]

# Extract the intercept coefficient for 0 to 15
intercept_0_15 &lt;- coeffs[2]

# Extract the intercept coefficient for 15 to 30
intercept_15_30 &lt;- coeffs[3]

# Extract the intercept coefficient for 30 to 45
intercept_30_45 &lt;- coeffs[4]


#---------------------------------------------------------

prediction_data &lt;- explanatory_data %&gt;% 
  mutate(
    # Consider the 3 cases to choose the intercept
    intercept = case_when(
      house_age_years == &quot;0 to 15&quot; ~ intercept_0_15,
      house_age_years == &quot;15 to 30&quot; ~ intercept_15_30,
      house_age_years == &quot;30 to 45&quot; ~ intercept_30_45 
    ),
    # Manually calculate the predictions
    price_twd_msq = intercept + slope * n_convenience
  )

# See the results
prediction_data</code></pre>
<pre><code>## # A tibble: 33 x 4
##    n_convenience house_age_years intercept price_twd_msq
##            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;
##  1             0 30 to 45            0.791         0.791
##  2             0 15 to 30            7.51          7.51 
##  3             0 0 to 15             7.09          7.09 
##  4             1 30 to 45            0.791        10.2  
##  5             1 15 to 30            7.51         16.9  
##  6             1 0 to 15             7.09         16.5  
##  7             2 30 to 45            0.791        19.6  
##  8             2 15 to 30            7.51         26.3  
##  9             2 0 to 15             7.09         25.9  
## 10             3 30 to 45            0.791        29.0  
## # ... with 23 more rows</code></pre>
</div>
<div id="comparing-coefficients-of-determination" class="section level1">
<h1>1-10 Comparing coefficients of determination</h1>
<p>Recall that the coefficient of determination is a measure of how well the linear regression line fits the observed values. An important motivation for including several explanatory variables in a linear regression is that you can improve the fit compared to considering only a single explanatory variable.</p>
<p>Here you’ll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result.</p>
<pre class="r"><code>mdl_price_vs_conv %&gt;% 
  # Get the model-level coefficients
  glance() %&gt;% 
  # Select the coeffs of determination
  select(r.squared, adj.r.squared)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   r.squared adj.r.squared
##       &lt;dbl&gt;         &lt;dbl&gt;
## 1     0.326         0.324</code></pre>
<pre class="r"><code># Get the coeffs of determination for mdl_price_vs_age
mdl_price_vs_age %&gt;% 
  glance() %&gt;% 
  select(r.squared, adj.r.squared)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   r.squared adj.r.squared
##       &lt;dbl&gt;         &lt;dbl&gt;
## 1     0.896         0.895</code></pre>
<pre class="r"><code># Get the coeffs of determination for mdl_price_vs_both
mdl_price_vs_both %&gt;% 
  glance() %&gt;% 
  select(r.squared, adj.r.squared)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   r.squared adj.r.squared
##       &lt;dbl&gt;         &lt;dbl&gt;
## 1     0.931         0.931</code></pre>
</div>
<div id="comparing-residual-standard-error" class="section level1">
<h1>1-11 Comparing residual standard error</h1>
<p>The other common metric for assessing model fit is the residual standard error (RSE), which measures the typical size of the residuals.</p>
<p>In the last exercise you saw how including both explanatory variables into the model increased the coefficient of determination. How do you think using both explanatory variables will change the RSE?</p>
<pre class="r"><code>mdl_price_vs_conv %&gt;% 
  # Get the model-level coefficients
  glance() %&gt;% 
  # Pull out the RSE
  pull(sigma)</code></pre>
<pre><code>## [1] 3.383888</code></pre>
<pre class="r"><code># Get the RSE for mdl_price_vs_age
mdl_price_vs_age%&gt;% 
 glance() %&gt;% 
  pull(sigma)</code></pre>
<pre><code>## [1] 3.950184</code></pre>
<pre class="r"><code># Get the RSE for mdl_price_vs_both
mdl_price_vs_both%&gt;% 
 glance() %&gt;% 
  pull(sigma)</code></pre>
<pre><code>## [1] 3.21346</code></pre>
</div>
<div id="one-model-per-category" class="section level1">
<h1>2-2 One model per category</h1>
<p>The model you ran on the whole dataset fits some parts of the data better than others. It’s worth taking a look at what happens when you run a linear model on different parts of the dataset separately, to see if each model agrees or disagrees with the others.</p>
<pre class="r"><code># From previous step
taiwan_0_to_15 &lt;- taiwan_real_estate %&gt;%
  filter(house_age_years == &quot;0 to 15&quot;)
taiwan_15_to_30 &lt;- taiwan_real_estate %&gt;%
  filter(house_age_years == &quot;15 to 30&quot;)
taiwan_30_to_45 &lt;- taiwan_real_estate %&gt;%
  filter(house_age_years == &quot;30 to 45&quot;)

# Model price vs. no. convenience stores using 0 to 15 data
mdl_0_to_15 &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_0_to_15)

# Model price vs. no. convenience stores using 15 to 30 data
mdl_15_to_30 &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_15_to_30)

# Model price vs. no. convenience stores using 30 to 45 data
mdl_30_to_45 &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_30_to_45)

# See the results
mdl_0_to_15</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_0_to_15)
## 
## Coefficients:
##   (Intercept)  n_convenience  
##        9.2417         0.8336</code></pre>
<pre class="r"><code>mdl_15_to_30</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_15_to_30)
## 
## Coefficients:
##   (Intercept)  n_convenience  
##        6.8719         0.8519</code></pre>
<pre class="r"><code>mdl_30_to_45</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_30_to_45)
## 
## Coefficients:
##   (Intercept)  n_convenience  
##        8.1131         0.6687</code></pre>
</div>
<div id="predicting-multiple-models" class="section level1">
<h1>2-3 Predicting multiple models</h1>
<p>In order to see what each of the models for individual categories are doing, it’s helpful to make predictions from them. The flow is exactly the same as the flow for making predictions on the whole model, though remember that you only have a single explanatory variable in these models (so expand_grid() isn’t needed.)</p>
<pre class="r"><code># From previous step
explanatory_data &lt;- tibble(
  n_convenience = 0:10
)

# Add column of predictions using &quot;0 to 15&quot; model and explanatory data 
prediction_data_0_to_15 &lt;- explanatory_data %&gt;% 
  mutate(price_twd_msq=predict(mdl_0_to_15,explanatory_data))

# Same again, with &quot;15 to 30&quot;
prediction_data_15_to_30 &lt;- explanatory_data %&gt;% 
  mutate(price_twd_msq=predict(mdl_15_to_30,explanatory_data))

# Same again, with &quot;30 to 45&quot;
prediction_data_30_to_45 &lt;- explanatory_data %&gt;% 
  mutate(price_twd_msq=predict(mdl_30_to_45,explanatory_data))</code></pre>
</div>
<div id="section-4" class="section level1">
<h1>2-4</h1>
<pre class="r"><code># Using taiwan_real_estate, plot price vs. no. of conv. stores
# colored by house age
ggplot(taiwan_real_estate, aes(n_convenience,price_twd_msq, color=house_age_years)) +
  # Make it a scatter plot
  geom_point() +
  # Add smooth linear regression trend lines, no ribbon
  geom_smooth(method=lm, se=FALSE)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="assessing-model-performance" class="section level1">
<h1>2-5 Assessing model performance</h1>
<p>To test which approach is best – the whole dataset model or the models for each house age category – you need to calculate some metrics. Here’s, you’ll compare the coefficient of determination and the residual standard error for each model.</p>
<pre class="r"><code># Get the coeff. of determination for mdl_all_ages

#mdl_all_ages %&gt;% glance() %&gt;% pull(r.squared)

# Get the coeff. of determination for mdl_0_to_15
mdl_0_to_15 %&gt;%
glance() %&gt;%
pull(r.squared)</code></pre>
<pre><code>## [1] 0.3120536</code></pre>
<pre class="r"><code># Get the coeff. of determination for mdl_15_to_30
mdl_15_to_30 %&gt;%
glance() %&gt;%
pull(r.squared)</code></pre>
<pre><code>## [1] 0.4424605</code></pre>
<pre class="r"><code># Get the coeff. of determination for mdl_30_to_45
mdl_30_to_45 %&gt;%
glance() %&gt;%
pull(r.squared)</code></pre>
<pre><code>## [1] 0.3125713</code></pre>
<pre class="r"><code>#----------------------------------------------------


# Get the RSE for mdl_all
#mdl_all_ages %&gt;%glance() %&gt;%pull(sigma)

# Get the RSE for mdl_0_to_15
mdl_0_to_15 %&gt;%
glance() %&gt;%
pull(sigma)</code></pre>
<pre><code>## [1] 3.564127</code></pre>
<pre class="r"><code># Get the RSE for mdl_15_to_30
mdl_15_to_30 %&gt;%
glance() %&gt;%
pull(sigma)</code></pre>
<pre><code>## [1] 2.585273</code></pre>
<pre class="r"><code># Get the RSE for mdl_30_to_45
mdl_30_to_45 %&gt;%
glance() %&gt;%
pull(sigma)</code></pre>
<pre><code>## [1] 3.239037</code></pre>
</div>
<div id="specifying-an-interaction" class="section level1">
<h1>2-7 Specifying an interaction</h1>
<p>So far you used a single parallel slopes model, which gave an OK fit for the whole dataset, then three separate models for each house age category, which gave a better fit for each individual category, but was clunky because you had three separate models to work with and explain. Ideally, you’d have a single model that had all the predictive power of the individual models.</p>
<p>Defining this single model is achieved through adding interactions between explanatory variables. R’s formula syntax is flexible, and gives you a couple of options, depending on whether you prefer concise code that is quick to type and to read, or explicit code that describes what you are doing in detail.</p>
<pre class="r"><code># Model price vs both with an interaction using &quot;times&quot; syntax
lm(price_twd_msq ~ n_convenience * house_age_years, data = taiwan_real_estate)</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ n_convenience * house_age_years, 
##     data = taiwan_real_estate)
## 
## Coefficients:
##                           (Intercept)                          n_convenience  
##                               9.24170                                0.83359  
##               house_age_years15 to 30                house_age_years30 to 45  
##                              -2.36978                               -1.12858  
## n_convenience:house_age_years15 to 30  n_convenience:house_age_years30 to 45  
##                               0.01833                               -0.16489</code></pre>
<pre class="r"><code># Model price vs both with an interaction using &quot;colon&quot; syntax
lm(
  price_twd_msq ~ n_convenience + house_age_years + n_convenience:house_age_years, 
  data = taiwan_real_estate
)</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ n_convenience + house_age_years + 
##     n_convenience:house_age_years, data = taiwan_real_estate)
## 
## Coefficients:
##                           (Intercept)                          n_convenience  
##                               9.24170                                0.83359  
##               house_age_years15 to 30                house_age_years30 to 45  
##                              -2.36978                               -1.12858  
## n_convenience:house_age_years15 to 30  n_convenience:house_age_years30 to 45  
##                               0.01833                               -0.16489</code></pre>
</div>
<div id="interactions-with-understandable-coeffs" class="section level1">
<h1>2-8 Interactions with understandable coeffs</h1>
<p>The previous model with the interaction term returned coefficients that were a little tricky to interpret. In order clarify what the model is predicting, you can reformulate the model in a way that returns understandable coefficients. For further clarity, you can compare the results to the models on the separate house age categories (mdl_0_to_15, mdl_15_to_30, and mdl_30_to_45).</p>
<pre class="r"><code># Model price vs. house age plus an interaction, no intercept
mdl_readable_inter &lt;- lm(price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0, data = taiwan_real_estate)

# See the result
mdl_readable_inter</code></pre>
<pre><code>## 
## Call:
## lm(formula = price_twd_msq ~ house_age_years + n_convenience:house_age_years + 
##     0, data = taiwan_real_estate)
## 
## Coefficients:
##                house_age_years0 to 15                house_age_years15 to 30  
##                                9.2417                                 6.8719  
##               house_age_years30 to 45   house_age_years0 to 15:n_convenience  
##                                8.1131                                 0.8336  
## house_age_years15 to 30:n_convenience  house_age_years30 to 45:n_convenience  
##                                0.8519                                 0.6687</code></pre>
<pre class="r"><code># Get coefficients
coefficients(mdl_readable_inter)</code></pre>
<pre><code>##                house_age_years0 to 15               house_age_years15 to 30 
##                             9.2417022                             6.8719186 
##               house_age_years30 to 45  house_age_years0 to 15:n_convenience 
##                             8.1131235                             0.8335867 
## house_age_years15 to 30:n_convenience house_age_years30 to 45:n_convenience 
##                             0.8519172                             0.6686981</code></pre>
</div>
<div id="interactions-with-understandable-coeffs-1" class="section level1">
<h1>2-10 Interactions with understandable coeffs</h1>
<p>As with every other regression model you’ve created, the fun part is making predictions. Fortunately, the code flow for this case is the same as the one without interactions – R can handle calculating the interactions without any extra prompting from you. The only thing you need to remember is the trick for getting combinations of explanatory variables.</p>
<pre class="r"><code># Make a grid of explanatory data
explanatory_data &lt;- expand_grid(
  # Set n_convenience to zero to ten
  n_convenience= seq(0, 10),
  # Set house_age_years to the unique values of that variable
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# See the result
explanatory_data</code></pre>
<pre><code>## # A tibble: 33 x 2
##    n_convenience house_age_years
##            &lt;int&gt; &lt;chr&gt;          
##  1             0 30 to 45       
##  2             0 15 to 30       
##  3             0 0 to 15        
##  4             1 30 to 45       
##  5             1 15 to 30       
##  6             1 0 to 15        
##  7             2 30 to 45       
##  8             2 15 to 30       
##  9             2 0 to 15        
## 10             3 30 to 45       
## # ... with 23 more rows</code></pre>
<pre class="r"><code># From previous step
explanatory_data &lt;- expand_grid(
  n_convenience = 0:10,
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# Add predictions to the data frame
#prediction_data &lt;- explanatory_data %&gt;% 
#  mutate(
 #   price_twd_msq = predict(mdl_price_vs_both_inter, explanatory_data) )

# See the result
prediction_data</code></pre>
<pre><code>## # A tibble: 33 x 4
##    n_convenience house_age_years intercept price_twd_msq
##            &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;
##  1             0 30 to 45            0.791         0.791
##  2             0 15 to 30            7.51          7.51 
##  3             0 0 to 15             7.09          7.09 
##  4             1 30 to 45            0.791        10.2  
##  5             1 15 to 30            7.51         16.9  
##  6             1 0 to 15             7.09         16.5  
##  7             2 30 to 45            0.791        19.6  
##  8             2 15 to 30            7.51         26.3  
##  9             2 0 to 15             7.09         25.9  
## 10             3 30 to 45            0.791        29.0  
## # ... with 23 more rows</code></pre>
<pre class="r"><code>explanatory_data &lt;- expand_grid(
  n_convenience = 0:10,
  house_age_years = unique(taiwan_real_estate$house_age_years)
)
#prediction_data &lt;- explanatory_data %&gt;%  mutate(price_twd_msq =predict(mdl_price_vs_both_inter, explanatory_data)  )

# 
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +
geom_point() +
geom_smooth(method = &quot;lm&quot;, se = FALSE) +
geom_point(data = prediction_data, size = 5, shape = 15)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="visualizing-multiple-explanatory-variables" class="section level1">
<h1>4-2 Visualizing multiple explanatory variables</h1>
<p>Logistic regression also supports multiple explanatory variables. Plotting has similar issues as the linear regression case: it quickly becomes difficult to include more numeric variables in the plot. Here we’ll look at the case of two numeric explanatory variables, and the solution is basically the same as before: use color to denote the response.</p>
<p>Here there are only two possible values of response (zero and one), and later when we add predicted responses, the values all lie between zero and one. Once you include predicted responses, the most important thing to determine from the plot is whether the predictions are close to zero, or close to one. That means that a 2-color gradient split at 0.5 is really useful: responses above 0.5 are one color, and responses below 0.5 are another color.</p>
<pre class="r"><code>names(churn)</code></pre>
<pre><code>## [1] &quot;X1&quot;                        &quot;has_churned&quot;              
## [3] &quot;time_since_first_purchase&quot; &quot;time_since_last_purchase&quot;</code></pre>
<pre class="r"><code># Using churn, plot recency vs. length of relationship,
# colored by churn status
ggplot(churn, aes(time_since_first_purchase, time_since_last_purchase, color= has_churned)) +
  # Make it a scatter plot, with transparency 0.5
  geom_point(alpha=0.5) +
  # Use a 2-color gradient split at 0.5
  scale_color_gradient2(midpoint = 0.5)+
  # Use the black and white theme
  theme_bw()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="logistic-regression-with-2-explanatory-variables" class="section level1">
<h1>4-3 Logistic regression with 2 explanatory variables</h1>
<p>To include multiple explanatory variables in logistic regression models, the syntax is the same as for linear regressions. The only change is the same as in the simple case: you run a generalized linear model with a binomial error family.</p>
<p>Here you’ll fit a model of churn status with both of the explanatory variables from the dataset: the length of customer relationship and the recency of purchase.</p>
<p>Logistic regression also supports multiple explanatory variables. Plotting has similar issues as the linear regression case: it quickly becomes difficult to include more numeric variables in the plot. Here we’ll look at the case of two numeric explanatory variables, and the solution is basically the same as before: use color to denote the response.</p>
<p>Here there are only two possible values of response (zero and one), and later when we add predicted responses, the values all lie between zero and one. Once you include predicted responses, the most important thing to determine from the plot is whether the predictions are close to zero, or close to one. That means that a 2-color gradient split at 0.5 is really useful: responses above 0.5 are one color, and responses below 0.5 are another color.</p>
<pre class="r"><code># Fit a logistic regression of churn status vs. length of
# relationship, recency, and an interaction
mdl_churn_vs_both_inter &lt;- glm(has_churned ~ time_since_first_purchase * time_since_last_purchase, data=churn, family = binomial)


# See the result
mdl_churn_vs_both_inter</code></pre>
<pre><code>## 
## Call:  glm(formula = has_churned ~ time_since_first_purchase * time_since_last_purchase, 
##     family = binomial, data = churn)
## 
## Coefficients:
##                                        (Intercept)  
##                                            -0.1505  
##                          time_since_first_purchase  
##                                            -0.6376  
##                           time_since_last_purchase  
##                                             0.4233  
## time_since_first_purchase:time_since_last_purchase  
##                                             0.1123  
## 
## Degrees of Freedom: 399 Total (i.e. Null);  396 Residual
## Null Deviance:       554.5 
## Residual Deviance: 519.8     AIC: 527.8</code></pre>
</div>
<div id="logistic-regression-prediction" class="section level1">
<h1>4-4 Logistic regression prediction</h1>
<p>As with linear regression, the joy of logistic regression is that you can make predictions. Let’s step through the prediction flow one more time!</p>
<pre class="r"><code># Make a grid of explanatory data
explanatory_data &lt;- expand_grid(
  # Set len. relationship to seq from -2 to 4 in steps of 0.1
  time_since_first_purchase = seq(-2,4,0.1),
  # Set recency to seq from -1 to 6 in steps of 0.1
  time_since_last_purchase = seq(-1,6,0.1)
)

# See the result
explanatory_data</code></pre>
<pre><code>## # A tibble: 4,331 x 2
##    time_since_first_purchase time_since_last_purchase
##                        &lt;dbl&gt;                    &lt;dbl&gt;
##  1                        -2                     -1  
##  2                        -2                     -0.9
##  3                        -2                     -0.8
##  4                        -2                     -0.7
##  5                        -2                     -0.6
##  6                        -2                     -0.5
##  7                        -2                     -0.4
##  8                        -2                     -0.3
##  9                        -2                     -0.2
## 10                        -2                     -0.1
## # ... with 4,321 more rows</code></pre>
<pre class="r"><code>#--------------------------

# From previous steps
explanatory_data &lt;- expand_grid(
  time_since_first_purchase = seq(-2, 4, 0.1),
  time_since_last_purchase = seq(-1, 6, 0.1)
)

# Add a column of predictions using mdl_churn_vs_both_inter
# and explanatory_data with type response
prediction_data &lt;- explanatory_data %&gt;%
mutate(has_churned= predict(mdl_churn_vs_both_inter, explanatory_data, type = &quot;response&quot;))


# See the result
prediction_data</code></pre>
<pre><code>## # A tibble: 4,331 x 3
##    time_since_first_purchase time_since_last_purchase has_churned
##                        &lt;dbl&gt;                    &lt;dbl&gt;       &lt;dbl&gt;
##  1                        -2                     -1         0.716
##  2                        -2                     -0.9       0.720
##  3                        -2                     -0.8       0.724
##  4                        -2                     -0.7       0.728
##  5                        -2                     -0.6       0.732
##  6                        -2                     -0.5       0.736
##  7                        -2                     -0.4       0.740
##  8                        -2                     -0.3       0.744
##  9                        -2                     -0.2       0.747
## 10                        -2                     -0.1       0.751
## # ... with 4,321 more rows</code></pre>
<pre class="r"><code>#--------------------------



# From previous steps
explanatory_data &lt;- expand_grid(
  time_since_first_purchase = seq(-2, 4, 0.1),
  time_since_last_purchase = seq(-1, 6, 0.1)
)
prediction_data &lt;- explanatory_data %&gt;% 
  mutate(
    has_churned = predict(mdl_churn_vs_both_inter, explanatory_data, type = &quot;response&quot;)
  )

# Extend the plot
ggplot(
  churn, 
  aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)
) +
  geom_point(alpha = 0.5) +
  scale_color_gradient2(midpoint = 0.5) +
  theme_bw() +
  # Add points from prediction_data with size 3 and shape 15
  geom_point(data=prediction_data, size= 3,  shape= 15)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="confusion-matrix" class="section level1">
<h1>4-4 Confusion matrix</h1>
<p>When the response variable has just two outcomes, like the case of churn, the measures of success for the model are “how many cases where the customer churned did the model correctly predict?” and “how many cases where the customer didn’t churn did the model correctly predict?”. These can be found by generating a confusion matrix and calculating summary metrics on it. A mosaic plot is the natural way to visualize the results.</p>
<pre class="r"><code># Get the actual responses from churn
actual_response &lt;- churn$has_churned

# Get the predicted responses from the model
predicted_response &lt;- round(fitted(mdl_churn_vs_both_inter))

# Get a table of these values
outcomes &lt;- table(predicted_response, actual_response)

# Convert the table to a conf_mat object
confusion &lt;- conf_mat(outcomes)

# See the result
confusion</code></pre>
<pre><code>##                   actual_response
## predicted_response   0   1
##                  0 102  53
##                  1  98 147</code></pre>
<pre class="r"><code>#------------------


# From previous step
actual_response &lt;- churn$has_churned
predicted_response &lt;- round(fitted(mdl_churn_vs_both_inter))
outcomes &lt;- table(predicted_response, actual_response)
confusion &lt;- conf_mat(outcomes)

# &quot;Automatically&quot; plot the confusion matrix
autoplot(confusion)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code># Get summary metrics
summary(confusion, event_level = &quot;second&quot;)</code></pre>
<pre><code>## # A tibble: 13 x 3
##    .metric              .estimator .estimate
##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
##  1 accuracy             binary         0.622
##  2 kap                  binary         0.245
##  3 sens                 binary         0.735
##  4 spec                 binary         0.51 
##  5 ppv                  binary         0.6  
##  6 npv                  binary         0.658
##  7 mcc                  binary         0.251
##  8 j_index              binary         0.245
##  9 bal_accuracy         binary         0.622
## 10 detection_prevalence binary         0.612
## 11 precision            binary         0.6  
## 12 recall               binary         0.735
## 13 f_meas               binary         0.661</code></pre>
</div>
