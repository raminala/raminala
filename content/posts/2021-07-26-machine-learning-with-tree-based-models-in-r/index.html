---
title: Machine Learning with Tree-Based Models in R
author: ''
date: '2021-07-26'
slug: []
categories: []
tags:
  - Tree-Based Models
  - Machine Learning
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidymodels)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;tune&#39;:
##   method                   from   
##   required_pkgs.model_spec parsnip</code></pre>
<pre><code>## -- Attaching packages -------------------------------------- tidymodels 0.1.3 --</code></pre>
<pre><code>## v broom        0.7.8      v recipes      0.1.16
## v dials        0.0.9      v rsample      0.1.0 
## v dplyr        1.0.6      v tibble       3.1.2 
## v ggplot2      3.3.3      v tidyr        1.1.3 
## v infer        0.5.4      v tune         0.1.5 
## v modeldata    0.1.0      v workflows    0.2.2 
## v parsnip      0.1.6      v workflowsets 0.0.2 
## v purrr        0.3.4      v yardstick    0.0.8</code></pre>
<pre><code>## -- Conflicts ----------------------------------------- tidymodels_conflicts() --
## x purrr::discard() masks scales::discard()
## x dplyr::filter()  masks stats::filter()
## x dplyr::lag()     masks stats::lag()
## x recipes::step()  masks stats::step()
## * Use tidymodels_prefer() to resolve common conflicts.</code></pre>
<pre class="r"><code># Load the package
library(tidymodels)
library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v readr   1.4.0     v forcats 0.5.1
## v stringr 1.4.0</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x readr::col_factor() masks scales::col_factor()
## x purrr::discard()    masks scales::discard()
## x dplyr::filter()     masks stats::filter()
## x stringr::fixed()    masks recipes::fixed()
## x dplyr::lag()        masks stats::lag()
## x readr::spec()       masks yardstick::spec()</code></pre>
<pre class="r"><code>diabetes &lt;- read_csv(&quot;diabetes.csv&quot;)</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   X1 = col_double(),
##   outcome = col_character(),
##   pregnancies = col_double(),
##   glucose = col_double(),
##   blood_pressure = col_double(),
##   skin_thickness = col_double(),
##   insulin = col_double(),
##   bmi = col_double(),
##   diabetes_pedigree_function = col_double(),
##   age = col_double()
## )</code></pre>
<div id="specify-that-tree" class="section level2">
<h2>1-3 Specify that tree</h2>
<p>In order to build models and use them to solve real-world problems, you first need to lay the foundations of your model by creating a model specification. This is the very first step in every machine learning pipeline that you will ever build.</p>
<p>You are going to load the relevant packages and design the specification for your classification tree in just a few steps.</p>
<pre class="r"><code># Pick a model class
tree_spec &lt;- decision_tree() %&gt;% 
  # Set the engine
  set_engine(&quot;rpart&quot;) %&gt;% 
  # Set the mode
  set_mode(&quot;classification&quot;)

# Print the result
tree_spec</code></pre>
<pre><code>## Decision Tree Model Specification (classification)
## 
## Computational engine: rpart</code></pre>
</div>
<div id="train-that-model" class="section level2">
<h2>1-4 Train that model</h2>
<p>A model specification is a good start, just like the canvas for a painter. But just as a painter needs color, the specification needs data. Only the final model is able to make predictions:</p>
<p>Model specification + data = model</p>
<p>In this exercise, you will train a decision tree that models the risk of diabetes using health variables as predictors. The response variable, outcome, indicates whether the patient has diabetes or not, which means this is a binary classification problem (there are just two classes). The dataset also contains health variables of patients like blood_pressure, age, and bmi.</p>
<pre class="r"><code># Train the model
tree_model_bmi &lt;- tree_spec %&gt;% 
  fit(formula = factor(outcome) ~ bmi,
data = diabetes)

# Print the model
tree_model_bmi</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  0ms 
## n= 768 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 768 268 no (0.6510417 0.3489583)  
##   2) bmi&lt; 29.85 291  47 no (0.8384880 0.1615120) *
##   3) bmi&gt;=29.85 477 221 no (0.5366876 0.4633124)  
##     6) bmi&lt; 40.85 392 168 no (0.5714286 0.4285714) *
##     7) bmi&gt;=40.85 85  32 yes (0.3764706 0.6235294) *</code></pre>
</div>
<div id="traintest-split" class="section level2">
<h2>1-6 Train/test split</h2>
<p>In order to test your models, you need to build and test the model on two different parts of the data - otherwise, it’s like cheating on an exam (as you already know the answers).</p>
<p>The data split is an integral part of the modeling process. You will dive into this by splitting the diabetes data and confirming the split proportions.</p>
<pre class="r"><code># Create the split
diabetes_split &lt;- initial_split(diabetes, prop = 0.8)

# Extract the training and test set
diabetes_train &lt;- training(diabetes_split)
diabetes_test  &lt;- testing(diabetes_split)

# Verify the proportions of both sets
round(nrow(diabetes_train) / nrow(diabetes), 2) == 0.80</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>round(nrow(diabetes_test) / nrow(diabetes), 2) == 0.20</code></pre>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="avoiding-class-imbalances" class="section level2">
<h2>1-7 Avoiding class imbalances</h2>
<p>Some data contains very imbalanced outcomes - like a rare disease dataset. When splitting randomly, you might end up with a very unfortunate split. Imagine all the rare observations are in the test and none in the training set. That would ruin your whole training process!</p>
<p>Fortunately, the initial_split() function provides a remedy. You are going to observe and solve these so-called class imbalances in this exercise.</p>
<p>There’s already a split object diabetes_split available with a 75% training and 25% test split.</p>
<pre class="r"><code># Proportion of &#39;yes&#39; outcomes in the training data
counts_train &lt;- table(training(diabetes_split)$outcome)
prop_yes_train &lt;- counts_train[&quot;yes&quot;] / sum(counts_train)

# Proportion of &#39;yes&#39; outcomes in the test data
counts_test &lt;- table(testing(diabetes_split)$outcome)
prop_yes_test &lt;- counts_test[&quot;yes&quot;] / sum(counts_test)

paste(&quot;Proportion of positive outcomes in training set:&quot;, round(prop_yes_train, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in training set: 0.34&quot;</code></pre>
<pre class="r"><code>paste(&quot;Proportion of positive outcomes in test set:&quot;, round(prop_yes_test, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in test set: 0.37&quot;</code></pre>
<pre class="r"><code># Create a split with a constant outcome distribution
diabetes_split &lt;- initial_split(diabetes,
prop = 0.75,
strata = outcome)

# Proportion of &#39;yes&#39; outcomes in the training data
counts_train &lt;- table(training(diabetes_split)$outcome)
prop_yes_train &lt;- counts_train[&quot;yes&quot;] / sum(counts_train)

# Proportion of &#39;yes&#39; outcomes in the test data
counts_test &lt;- table(testing(diabetes_split)$outcome)
prop_yes_test &lt;- counts_test[&quot;yes&quot;] / sum(counts_test)

paste(&quot;Proportion of positive outcomes in training set:&quot;, round(prop_yes_train, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in training set: 0.35&quot;</code></pre>
<pre class="r"><code>paste(&quot;Proportion of positive outcomes in test set:&quot;, round(prop_yes_test, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in test set: 0.35&quot;</code></pre>
</div>
<div id="from-zero-to-hero" class="section level2">
<h2>1-8 From zero to hero</h2>
<p>You mastered the skills of creating a model specification and splitting the data into training and test sets. You also know how to avoid class imbalances in the split. It’s now time to combine what you learned in the preceding lesson and build your model using only the training set!</p>
<p>You are going to build a proper machine learning pipeline. This is comprised of creating a model specification, splitting your data into training and test sets, and last but not least, fitting the training data to a model. Enjoy!</p>
<pre class="r"><code># Create the balanced data split
diabetes_split &lt;- initial_split(diabetes,
prop = 0.75,
strata = outcome)

# Build the specification of the model
tree_spec &lt;- decision_tree() %&gt;% 
  set_engine(&quot;rpart&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)

# Train the model
model_trained &lt;- tree_spec %&gt;% 
  fit(formula = factor(outcome) ~ bmi+skin_thickness, 
      data = training(diabetes_split))

model_trained</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  0ms 
## n= 576 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##    1) root 576 201 no (0.65104167 0.34895833)  
##      2) bmi&lt; 27.35 155  15 no (0.90322581 0.09677419) *
##      3) bmi&gt;=27.35 421 186 no (0.55819477 0.44180523)  
##        6) bmi&lt; 40.85 361 150 no (0.58448753 0.41551247)  
##         12) bmi&gt;=40.05 8   0 no (1.00000000 0.00000000) *
##         13) bmi&lt; 40.05 353 150 no (0.57507082 0.42492918)  
##           26) bmi&lt; 29.85 72  22 no (0.69444444 0.30555556) *
##           27) bmi&gt;=29.85 281 128 no (0.54448399 0.45551601)  
##             54) bmi&gt;=36.7 68  25 no (0.63235294 0.36764706) *
##             55) bmi&lt; 36.7 213 103 no (0.51643192 0.48356808)  
##              110) skin_thickness&gt;=42.5 12   2 no (0.83333333 0.16666667) *
##              111) skin_thickness&lt; 42.5 201 100 yes (0.49751244 0.50248756)  
##                222) bmi&lt; 35.85 184  87 no (0.52717391 0.47282609)  
##                  444) skin_thickness&lt; 27.5 103  44 no (0.57281553 0.42718447)  
##                    888) bmi&gt;=33 41  11 no (0.73170732 0.26829268) *
##                    889) bmi&lt; 33 62  29 yes (0.46774194 0.53225806)  
##                     1778) skin_thickness&gt;=6.5 34  14 no (0.58823529 0.41176471) *
##                     1779) skin_thickness&lt; 6.5 28   9 yes (0.32142857 0.67857143) *
##                  445) skin_thickness&gt;=27.5 81  38 yes (0.46913580 0.53086420)  
##                    890) bmi&lt; 32.2 22   6 no (0.72727273 0.27272727) *
##                    891) bmi&gt;=32.2 59  22 yes (0.37288136 0.62711864)  
##                     1782) bmi&gt;=35.15 13   5 no (0.61538462 0.38461538) *
##                     1783) bmi&lt; 35.15 46  14 yes (0.30434783 0.69565217) *
##                223) bmi&gt;=35.85 17   3 yes (0.17647059 0.82352941) *
##        7) bmi&gt;=40.85 60  24 yes (0.40000000 0.60000000)  
##         14) bmi&lt; 48.1 49  22 yes (0.44897959 0.55102041)  
##           28) bmi&gt;=44.75 14   4 no (0.71428571 0.28571429) *
##           29) bmi&lt; 44.75 35  12 yes (0.34285714 0.65714286) *
##         15) bmi&gt;=48.1 11   2 yes (0.18181818 0.81818182) *</code></pre>
</div>
<div id="make-predictions" class="section level2">
<h2>1-10 Make predictions</h2>
<p>Making predictions with data is one of the fundamental goals of machine learning. Now that you know how to split the data and fit a model, it’s time to make predictions about unseen samples with your models.</p>
<p>You are going to make predictions about your test set using a model obtained by fitting the training data to a tree specification.</p>
<pre class="r"><code>model &lt;- tree_spec %&gt;% 
  fit(formula = factor(outcome) ~ ., 
      data = diabetes_train)

# Generate predictions
predictions &lt;- predict(model,
                   diabetes_test, type = &quot;class&quot;)

# Add the true outcomes
predictions_combined &lt;- predictions %&gt;% 
  mutate(true_class = diabetes_test$outcome)


# Print the first lines of the result
head(predictions_combined)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   .pred_class true_class
##   &lt;fct&gt;       &lt;chr&gt;     
## 1 yes         yes       
## 2 no          no        
## 3 no          yes       
## 4 no          yes       
## 5 yes         no        
## 6 yes         yes</code></pre>
</div>
<div id="crack-the-matrix" class="section level2">
<h2>1-11 Crack the matrix</h2>
<p>Visual representations are a great and intuitive way to assess results. One way to visualize and assess the performance of your model is by using a confusion matrix. In this exercise, you will create the confusion matrix of your predicted values to see in which cases it performs well and in which cases it doesn’t.</p>
<pre class="r"><code># The confusion matrix
diabetes_matrix &lt;- conf_mat(data = predictions_combined,
estimate = .pred_class,
truth = true_class)</code></pre>
<pre><code>## Warning in vec2table(truth = truth, estimate = estimate, dnn = dnn, ...): `truth`
## was converted to a factor</code></pre>
<pre class="r"><code># Print the matrix
print(diabetes_matrix)</code></pre>
<pre><code>##           Truth
## Prediction no yes
##        no  78  29
##        yes 19  28</code></pre>
</div>
<div id="are-you-predicting-correctly" class="section level2">
<h2>1-12 Are you predicting correctly?</h2>
<p>Your model should be as good as possible, right? One way you can assess this is by counting how often it predicted the correct classes compared to the total number of predictions it made. As discussed in the video, we call this performance measure accuracy. You can either calculate this manually or by using a handy shortcut. Both obtain the same result.</p>
<pre class="r"><code># The accuracy calculated by a function
acc_auto &lt;- accuracy(predictions_combined, estimate = .pred_class, truth = factor(true_class))

acc_auto$estimate</code></pre>
<pre><code>## Warning: Unknown or uninitialised column: `estimate`.</code></pre>
<pre><code>## NULL</code></pre>
</div>
<div id="train-a-regression-tree" class="section level2">
<h2>2-2 Train a regression tree</h2>
<p>As you know already, decision trees are a useful tool for classification problems. Moreover, you can also use them to model regression problems. The structural difference is that there will be numeric values (instead of classes) on the leaf nodes.</p>
<p>In this exercise, you will use the chocolate dataset to fit a regression tree. This is very similar to what you already did in Chapter 1 with the diabetes dataset.</p>
<pre class="r"><code>chocolate_train &lt;- read_csv(&quot;chocolate_train.csv&quot;)</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   X1 = col_double(),
##   final_grade = col_double(),
##   review_date = col_double(),
##   cocoa_percent = col_double(),
##   company_location = col_character(),
##   bean_type = col_character(),
##   broad_bean_origin = col_character()
## )</code></pre>
<pre class="r"><code># Build the specification
model_spec &lt;- decision_tree() %&gt;%
  set_mode(&quot;regression&quot;) %&gt;%
  set_engine(&quot;rpart&quot;)

# Fit to the data
model_fit &lt;- model_spec %&gt;%
  fit(formula = final_grade ~ cocoa_percent+review_date,
      data = chocolate_train)

model_fit</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  0ms 
## n= 1437 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 1437 341.707000 3.187543  
##    2) cocoa_percent&gt;=0.905 24  10.364580 2.229167  
##      4) review_date&lt; 2012.5 13   4.076923 1.865385 *
##      5) review_date&gt;=2012.5 11   2.534091 2.659091 *
##    3) cocoa_percent&lt; 0.905 1413 308.924400 3.203822  
##      6) cocoa_percent&gt;=0.755 190  37.447700 3.035526  
##       12) review_date&lt; 2012.5 77  17.698050 2.866883 *
##       13) review_date&gt;=2012.5 113  16.067480 3.150442 *
##      7) cocoa_percent&lt; 0.755 1223 265.259200 3.229967  
##       14) cocoa_percent&lt; 0.625 78  13.042470 2.983974 *
##       15) cocoa_percent&gt;=0.625 1145 247.175200 3.246725 *</code></pre>
</div>
<div id="predict-new-values" class="section level2">
<h2>2-3 Predict new values</h2>
<p>A predictive model is one that predicts the outcomes of new, unseen data. Besides the numeric predictors, there are other useful columns in the dataset. The goal of this exercise is to predict the final rating grades of a chocolate tasting based on all other predictor variables that are available.</p>
<pre class="r"><code>chocolate_test &lt;- read_csv(&quot;chocolate_test.csv&quot;)</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   X1 = col_double(),
##   final_grade = col_double(),
##   review_date = col_double(),
##   cocoa_percent = col_double(),
##   company_location = col_character(),
##   bean_type = col_character(),
##   broad_bean_origin = col_character()
## )</code></pre>
<pre class="r"><code># Train the model
chocolate_model &lt;- model_spec %&gt;%
  fit(formula = final_grade ~ . , 
      data = chocolate_train)

# Predict new data
#predictions &lt;- predict(chocolate_model,new_data = chocolate_test) %&gt;%
  # Add the test set
 # bind_cols(chocolate_test)

#predictions</code></pre>
</div>
<div id="in-sample-performance" class="section level2">
<h2>2-6 In-sample performance</h2>
<p>It’s very important to know whether your regression model is useful or not. A useful model can be one that captures the structure of your training set well. One way to assess this in-sample performance is to predict on training data and calculate the mean absolute error of all predicted data points.</p>
<p>In this exercise, you will evaluate your in-sample predictions using MAE (mean absolute error). MAE tells you approximately how far away the predictions are from the true values.</p>
</div>
<div id="out-of-sample-performance" class="section level2">
<h2>2-7 Out-of-sample performance</h2>
<p>In-sample performance provides insights about how well a model captures the data it is modeling. For predictive models, it’s also important to check model performance on new, unseen data, the out-of-sample performance.</p>
<p>In this exercise, you will check the test set predictions of your model using MAE (mean absolute error).</p>
</div>
<div id="create-the-folds" class="section level2">
<h2>2-10 Create the folds</h2>
<p>Splitting data only once into training and test sets has statistical insecurities - there is a small chance that your test set contains only high-rated beans, while all the low-rated beans are in your training set. It also means that you can only measure the performance of your model once.</p>
<p>Cross-validation gives you a more robust estimate of your out-of-sample performance without the statistical pitfalls - it assesses your model more profoundly.</p>
<pre class="r"><code># Set seed for reproducibility
set.seed(20)

# Build 10 folds
chocolate_folds &lt;- vfold_cv(chocolate_train, v = 10)

chocolate_folds</code></pre>
<pre><code>## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits             id    
##    &lt;list&gt;             &lt;chr&gt; 
##  1 &lt;split [1293/144]&gt; Fold01
##  2 &lt;split [1293/144]&gt; Fold02
##  3 &lt;split [1293/144]&gt; Fold03
##  4 &lt;split [1293/144]&gt; Fold04
##  5 &lt;split [1293/144]&gt; Fold05
##  6 &lt;split [1293/144]&gt; Fold06
##  7 &lt;split [1293/144]&gt; Fold07
##  8 &lt;split [1294/143]&gt; Fold08
##  9 &lt;split [1294/143]&gt; Fold09
## 10 &lt;split [1294/143]&gt; Fold10</code></pre>
</div>
<div id="fit-the-folds" class="section level2">
<h2>2-11 Fit the folds</h2>
<p>Now that you split your data into folds, it’s time to use them for model training and calculating the out-of-sample error of every single model. This way, you gain a balanced estimation of the performance of your model specification because you evaluated it out-of-sample several times.</p>
<pre class="r"><code># Create a specification
tree_spec &lt;- decision_tree() %&gt;%
    set_engine(&quot;rpart&quot;) %&gt;%
    set_mode(&quot;regression&quot;)

# Fit all folds to the specification
fits_cv &lt;- fit_resamples(tree_spec,
               final_grade ~ .,
               resamples = chocolate_folds,
               metrics = metric_set(mae, rmse))</code></pre>
<pre><code>## x Fold01: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold02: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold03: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold04: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold05: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold06: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold07: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold08: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold09: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold10: preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## Warning: All models failed. See the `.notes` column.</code></pre>
<pre class="r"><code>fits_cv</code></pre>
<pre><code>## Warning: This tuning result has notes. Example notes on model fitting include:
## preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = attr(object, : factor company_location has new levels Martinique
## preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = attr(object, : factor bean_type has new levels Amazon mix, Blend-Forastero,Criollo
## preprocessor 1/1, model 1/1 (predictions): Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = attr(object, : factor company_location has new levels India, Singapore</code></pre>
<pre><code>## # Resampling results
## # 10-fold cross-validation 
## # A tibble: 10 x 4
##    splits             id     .metrics .notes          
##    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;   &lt;list&gt;          
##  1 &lt;split [1293/144]&gt; Fold01 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  2 &lt;split [1293/144]&gt; Fold02 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  3 &lt;split [1293/144]&gt; Fold03 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  4 &lt;split [1293/144]&gt; Fold04 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  5 &lt;split [1293/144]&gt; Fold05 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  6 &lt;split [1293/144]&gt; Fold06 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  7 &lt;split [1293/144]&gt; Fold07 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  8 &lt;split [1294/143]&gt; Fold08 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
##  9 &lt;split [1294/143]&gt; Fold09 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;
## 10 &lt;split [1294/143]&gt; Fold10 &lt;NULL&gt;   &lt;tibble [1 x 1]&gt;</code></pre>
</div>
<div id="evaluate-the-folds" class="section level2">
<h2>2-12 Evaluate the folds</h2>
<p>Now that you fit 10 models using all 10 of your folds and calculated the MAE and RMSE of each of these models, it’s time to visualize how large the errors are. This way, you build an intuition of the out-of-sample error distribution, which is helpful in assessing your model quality.</p>
<p>You will plot all these errors as a histogram and display the summary statistics across all folds.</p>
<pre class="r"><code>library(ggplot2)

# Collect the errors
#all_errors &lt;- collect_metrics(fits_cv, summarize = FALSE)
#all_errors
# Plot an error histogram
#ggplot(all_errors, aes(.estimate, fill=.metric)) +
 #       geom_histogram()

# Collect and print error statistics
#collect_metrics(fits_cv, summarize = TRUE)</code></pre>
</div>
<div id="adjust-model-complexity" class="section level2">
<h2>2-15 Adjust model complexity</h2>
<p>To make good predictions, you need to adjust the complexity of your model. Simple models can only represent simple data structures, while complex models can represent fine-grained data structures.</p>
<p>In this exercise, you are going to create trees of different complexities by altering the hyperparameters of a regression tree.</p>
<pre class="r"><code># Create a model having only one split
chocolate_model &lt;- decision_tree(tree_depth = 1) %&gt;% 
        set_mode(&quot;regression&quot;) %&gt;%
        set_engine(&quot;rpart&quot;) %&gt;% 
        fit(final_grade ~ ., data = chocolate_train)
chocolate_model</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  10ms 
## n= 1437 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 1437 341.70700 3.187543  
##   2) cocoa_percent&gt;=0.905 24  10.36458 2.229167 *
##   3) cocoa_percent&lt; 0.905 1413 308.92440 3.203822 *</code></pre>
<pre class="r"><code># Create a model with high cost for complexity
chocolate_model &lt;- decision_tree(cost_complexity = 0.1) %&gt;% 
        set_mode(&quot;regression&quot;) %&gt;%
        set_engine(&quot;rpart&quot;) %&gt;% 
        fit(final_grade ~ ., data = chocolate_train)
chocolate_model</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  0ms 
## n= 1437 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 1437 341.707 3.187543 *</code></pre>
</div>
<div id="n-sample-and-out-of-sample-performance" class="section level2">
<h2>2-16 n-sample and out-of-sample performance</h2>
<p>Does a more sophisticated model always perform better? As we discussed in the video, that’s only half the truth.</p>
<p>Overfitted models understand the structure of their training set perfectly but cannot generalize to new data. That’s a bummer! At the end of the day, the main purpose of a predictive model is to perform well on new data, right? Go investigate!</p>
<pre class="r"><code># Predict on and combine with training data and calculate the error
#predict(complex_model, new_data = chocolate_train) %&gt;%
#   bind_cols(chocolate_train) %&gt;% 
#   mae(estimate = .pred,
#truth = final_grade)</code></pre>
</div>
<div id="generate-a-tuning-grid" class="section level2">
<h2>3-2 Generate a tuning grid</h2>
<p>The standard hyperparameters of most models provide a good fit for most datasets. Yet, they need optimization for the best performance. Otherwise, it’s like driving a car with an activated hand brake. Release the brake and tune your models!</p>
<p>In this exercise, you’ll create two objects that serve as a starting point: a tuning grid (a set of hyperparameter combinations) and a model specification that you later train with every value of the grid.</p>
<pre class="r"><code># Create a specification with tuning placeholders
tune_spec &lt;- decision_tree(tree_depth = tune(),
                           cost_complexity = tune()) %&gt;% 
  # Specify mode
  set_mode(&quot;classification&quot;) %&gt;%
  # Specify engine
  set_engine(&quot;rpart&quot;) 

tune_spec</code></pre>
<pre><code>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = tune()
##   tree_depth = tune()
## 
## Computational engine: rpart</code></pre>
<pre class="r"><code># Create a regular grid
tree_grid &lt;- grid_regular(parameters(tune_spec),
                 levels = 2)

tree_grid</code></pre>
<pre><code>## # A tibble: 4 x 2
##   cost_complexity tree_depth
##             &lt;dbl&gt;      &lt;int&gt;
## 1    0.0000000001          1
## 2    0.1                   1
## 3    0.0000000001         15
## 4    0.1                  15</code></pre>
</div>
<div id="tune-along-the-grid" class="section level2">
<h2>3-3 Tune along the grid</h2>
<p>After creating the tuning grid and a dummy specification, you need to fit a model on every grid point and evaluate the resulting model. This is very easy in the tidymodels framework using the tune_grid() function, as introduced in the slides.</p>
<p>In the remaining exercises, you will use the credit card customers dataset, which has the following columns:</p>
<p>still_customer: flag (yes or no) that indicates if a customer is still an active customer</p>
<p>total_trans_amt: total sum of transactions in USD</p>
<p>customer_age: age of the customer</p>
<p>income_category: labels like $60K - $80K or Less than $40K to indicate the category of yearly income
… and 16 more columns.</p>
<pre class="r"><code>set.seed(275)

# Create CV folds of the customers tibble
folds &lt;- vfold_cv(customers, v = 3)

# Tune along the grid
tune_results &lt;- tune_grid(tune_spec, 
                          factor(still_customer) ~ .,
                          resamples = folds,
                          grid = tree_grid,
                          metrics = metric_set(accuracy))</code></pre>
<pre><code>## x Fold1: preprocessor 1/1, model 1/4 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold1: preprocessor 1/1, model 2/4 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold1: preprocessor 1/1, model 3/4 (predictions): Error in model.frame.default(...</code></pre>
<pre><code>## x Fold1: preprocessor 1/1, model 4/4 (predictions): Error in model.frame.default(...</code></pre>
<pre class="r"><code># Plot the tuning results
autoplot(tune_results)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
<div id="pick-the-winner" class="section level2">
<h2>3-4 Pick the winner</h2>
<p>Once tuning has been performed, it’s time to pick the optimal hyperparameters from the results and build the final model. Two helpers from tidymodels come in handy:</p>
<p>The function select_best() extracts the optimal hyperparameters from a tuning results tibble, and finalize_model() plugs these results into the specification, replacing the placeholders.</p>
<pre class="r"><code># Select the parameters that perform best
final_params &lt;- select_best(tune_results)

final_params</code></pre>
<pre><code>## # A tibble: 1 x 3
##   cost_complexity tree_depth .config             
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;               
## 1    0.0000000001         15 Preprocessor1_Model3</code></pre>
<pre class="r"><code># Finalize the specification
best_spec &lt;- finalize_model(tune_spec, final_params)

best_spec</code></pre>
<pre><code>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = 1e-10
##   tree_depth = 15
## 
## Computational engine: rpart</code></pre>
<pre class="r"><code># Build the final model
final_model &lt;- fit(best_spec,
                   factor(still_customer) ~ .,
                   customers)

final_model</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  41ms 
## n= 800 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 800 128 no (0.84000000 0.16000000)  
##     2) total_ct_chng_q4_q1&gt;=0.4605 723  81 no (0.88796680 0.11203320)  
##       4) total_revolving_bal&gt;=628.5 550  28 no (0.94909091 0.05090909)  
##         8) total_trans_ct&gt;=64.5 334   4 no (0.98802395 0.01197605) *
##         9) total_trans_ct&lt; 64.5 216  24 no (0.88888889 0.11111111)  
##          18) total_trans_amt&lt; 5942 209  17 no (0.91866029 0.08133971)  
##            36) credit_limit&gt;=1794.5 194  11 no (0.94329897 0.05670103)  
##              72) total_amt_chng_q4_q1&gt;=0.7375 104   0 no (1.00000000 0.00000000) *
##              73) total_amt_chng_q4_q1&lt; 0.7375 90  11 no (0.87777778 0.12222222)  
##               146) total_trans_amt&lt; 2016.5 54   2 no (0.96296296 0.03703704) *
##               147) total_trans_amt&gt;=2016.5 36   9 no (0.75000000 0.25000000)  
##                 294) total_trans_ct&gt;=50.5 24   1 no (0.95833333 0.04166667) *
##                 295) total_trans_ct&lt; 50.5 12   4 yes (0.33333333 0.66666667) *
##            37) credit_limit&lt; 1794.5 15   6 no (0.60000000 0.40000000) *
##          19) total_trans_amt&gt;=5942 7   0 yes (0.00000000 1.00000000) *
##       5) total_revolving_bal&lt; 628.5 173  53 no (0.69364162 0.30635838)  
##        10) total_trans_ct&gt;=58.5 119  22 no (0.81512605 0.18487395)  
##          20) total_trans_amt&lt; 5297.5 82   2 no (0.97560976 0.02439024) *
##          21) total_trans_amt&gt;=5297.5 37  17 yes (0.45945946 0.54054054)  
##            42) total_trans_ct&gt;=92 13   0 no (1.00000000 0.00000000) *
##            43) total_trans_ct&lt; 92 24   4 yes (0.16666667 0.83333333)  
##              86) total_trans_amt&lt; 7729 7   3 no (0.57142857 0.42857143) *
##              87) total_trans_amt&gt;=7729 17   0 yes (0.00000000 1.00000000) *
##        11) total_trans_ct&lt; 58.5 54  23 yes (0.42592593 0.57407407)  
##          22) total_ct_chng_q4_q1&gt;=1.028 8   0 no (1.00000000 0.00000000) *
##          23) total_ct_chng_q4_q1&lt; 1.028 46  15 yes (0.32608696 0.67391304)  
##            46) total_trans_amt&lt; 1770 16   5 no (0.68750000 0.31250000) *
##            47) total_trans_amt&gt;=1770 30   4 yes (0.13333333 0.86666667) *
##     3) total_ct_chng_q4_q1&lt; 0.4605 77  30 yes (0.38961039 0.61038961)  
##       6) total_trans_amt&lt; 1873.5 36  13 no (0.63888889 0.36111111)  
##        12) total_trans_amt&gt;=1052 29   6 no (0.79310345 0.20689655)  
##          24) avg_utilization_ratio&gt;=0.046 18   0 no (1.00000000 0.00000000) *
##          25) avg_utilization_ratio&lt; 0.046 11   5 yes (0.45454545 0.54545455) *
##        13) total_trans_amt&lt; 1052 7   0 yes (0.00000000 1.00000000) *
##       7) total_trans_amt&gt;=1873.5 41   7 yes (0.17073171 0.82926829)  
##        14) total_trans_ct&gt;=55 8   2 no (0.75000000 0.25000000) *
##        15) total_trans_ct&lt; 55 33   1 yes (0.03030303 0.96969697) *</code></pre>
</div>
<div id="calculate-specificity" class="section level2">
<h2>3-6 Calculate specificity</h2>
<p>Using different measures for model performance allows you to more accurately assess it. There are several metrics for different use cases. Specificity measures the proportion of true negative outcomes correctly identified:</p>
<p>This formula implies that with specificity approaching 100%, the number of false positives (FP) approaches 0.</p>
<p>In this exercise, you are going to investigate the out-of-sample specificity of your model with cross-validation.</p>
</div>
<div id="draw-the-roc-curve" class="section level2">
<h2>3-7 Draw the ROC curve</h2>
<p>Visualizing model performance with a ROC curve allows you to gather the performance across all possible thresholds into a single plot. It shows the sensitivity and specificity for every threshold. The more “up and left” a ROC curve is, the better the model.</p>
<p>You are going to predict class probabilities of credit card customers having churned and plot the results as a ROC curve.</p>
</div>
<div id="extract-the-training-and-test-set" class="section level1">
<h1>Extract the training and test set</h1>
<p>customers_test &lt;- testing(customers_split)</p>
</div>
<div id="predict-probabilities-on-test-set" class="section level1">
<h1>Predict probabilities on test set</h1>
<p>predictions &lt;- predict(model, customers_test, type = “prob”) %&gt;%
# Add test set
bind_cols(customers_test)</p>
</div>
<div id="calculate-the-roc-curve-for-all-thresholds" class="section level1">
<h1>Calculate the ROC curve for all thresholds</h1>
<p>roc &lt;- roc_curve(predictions,
estimate = .pred_yes,
truth = still_customer)</p>
</div>
<div id="plot-the-roc-curve" class="section level1">
<h1>Plot the ROC curve</h1>
<p>autoplot(roc)</p>
<div id="area-under-the-roc-curve" class="section level2">
<h2>3-8 Area under the ROC curve</h2>
<p>The area under the ROC curve boils down many other performance estimates to one single number and allows you to assess a model’s performance very quickly. For this reason, it is a very common performance measure for classification models.</p>
</div>
</div>
<div id="calculate-area-under-the-curve" class="section level1">
<h1>Calculate area under the curve</h1>
<p>auc_result &lt;- roc_auc(predictions,
estimate = .pred_yes,
truth = still_customer)</p>
<p>print(paste(“The area under the ROC curve is”, round(auc_result$.estimate, 3)))</p>
</div>
<div id="create-bagged-trees" class="section level1">
<h1>3-10 Create bagged trees</h1>
<p>Ensemble models like bagged trees are more powerful than single decision trees. Each tree in the ensemble gives a vote, and the average or majority vote is your prediction. This ensures you use swarm intelligence instead of relying on a single tree. For bagged trees, the bootstrap method ensures that in every ensemble tree, only a bootstrapped sample (sampled with replacement) of the original dataset is used to train the tree and create the prediction.</p>
</div>
<div id="create-the-specification" class="section level1">
<h1>Create the specification</h1>
<p>spec_bagged &lt;- baguette::bag_tree() %&gt;%
set_mode(“classification”) %&gt;%
set_engine(“rpart”, times = 20)</p>
</div>
<div id="fit-to-the-training-data" class="section level1">
<h1>Fit to the training data</h1>
<p>model_bagged &lt;- fit(spec_bagged,
formula = still_customer ~ total_trans_amt+customer_age+education_level,
data = customers_train)</p>
</div>
<div id="print-the-model" class="section level1">
<h1>Print the model</h1>
<p>model_bagged</p>
</div>
<div id="section" class="section level1">
<h1>3-11</h1>
</div>
<div id="check-for-overfitting" class="section level1">
<h1>3-12 Check for overfitting</h1>
<p>A very high in-sample AUC like can be an indicator of overfitting. It is also possible that your dataset is just very well structured, or your model might just be terrific!</p>
<p>To check which of these is true, you need to produce out-of-sample estimates of your AUC, and because you don’t want to touch your test set yet, you can produce these using cross-validation on your training set.</p>
</div>
<div id="n-sample-and-out-of-sample-performance-1" class="section level1">
<h1>3-15 n-sample and out-of-sample performance</h1>
<p>Does a</p>
</div>
