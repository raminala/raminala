---
title: Machine Learning with Tree-Based Models in R
author: ''
date: '2021-07-26'
slug: []
categories: []
tags:
  - Tree-Based Models
  - Machine Learning
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidymodels)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;tune&#39;:
##   method                   from   
##   required_pkgs.model_spec parsnip</code></pre>
<pre><code>## -- Attaching packages -------------------------------------- tidymodels 0.1.3 --</code></pre>
<pre><code>## v broom        0.7.8      v recipes      0.1.16
## v dials        0.0.9      v rsample      0.1.0 
## v dplyr        1.0.6      v tibble       3.1.2 
## v ggplot2      3.3.3      v tidyr        1.1.3 
## v infer        0.5.4      v tune         0.1.5 
## v modeldata    0.1.0      v workflows    0.2.2 
## v parsnip      0.1.6      v workflowsets 0.0.2 
## v purrr        0.3.4      v yardstick    0.0.8</code></pre>
<pre><code>## -- Conflicts ----------------------------------------- tidymodels_conflicts() --
## x purrr::discard() masks scales::discard()
## x dplyr::filter()  masks stats::filter()
## x dplyr::lag()     masks stats::lag()
## x recipes::step()  masks stats::step()
## * Use tidymodels_prefer() to resolve common conflicts.</code></pre>
<pre class="r"><code># Load the package
library(tidymodels)
library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v readr   1.4.0     v forcats 0.5.1
## v stringr 1.4.0</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x readr::col_factor() masks scales::col_factor()
## x purrr::discard()    masks scales::discard()
## x dplyr::filter()     masks stats::filter()
## x stringr::fixed()    masks recipes::fixed()
## x dplyr::lag()        masks stats::lag()
## x readr::spec()       masks yardstick::spec()</code></pre>
<pre class="r"><code>diabetes &lt;- read_csv(&quot;diabetes.csv&quot;)</code></pre>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   X1 = col_double(),
##   outcome = col_character(),
##   pregnancies = col_double(),
##   glucose = col_double(),
##   blood_pressure = col_double(),
##   skin_thickness = col_double(),
##   insulin = col_double(),
##   bmi = col_double(),
##   diabetes_pedigree_function = col_double(),
##   age = col_double()
## )</code></pre>
<div id="specify-that-tree" class="section level2">
<h2>1-3 Specify that tree</h2>
<p>In order to build models and use them to solve real-world problems, you first need to lay the foundations of your model by creating a model specification. This is the very first step in every machine learning pipeline that you will ever build.</p>
<p>You are going to load the relevant packages and design the specification for your classification tree in just a few steps.</p>
<pre class="r"><code># Pick a model class
tree_spec &lt;- decision_tree() %&gt;% 
  # Set the engine
  set_engine(&quot;rpart&quot;) %&gt;% 
  # Set the mode
  set_mode(&quot;classification&quot;)

# Print the result
tree_spec</code></pre>
<pre><code>## Decision Tree Model Specification (classification)
## 
## Computational engine: rpart</code></pre>
</div>
<div id="train-that-model" class="section level2">
<h2>1-4 Train that model</h2>
<p>A model specification is a good start, just like the canvas for a painter. But just as a painter needs color, the specification needs data. Only the final model is able to make predictions:</p>
<p>Model specification + data = model</p>
<p>In this exercise, you will train a decision tree that models the risk of diabetes using health variables as predictors. The response variable, outcome, indicates whether the patient has diabetes or not, which means this is a binary classification problem (there are just two classes). The dataset also contains health variables of patients like blood_pressure, age, and bmi.</p>
<pre class="r"><code># Train the model
tree_model_bmi &lt;- tree_spec %&gt;% 
  fit(formula = factor(outcome) ~ bmi,
data = diabetes)

# Print the model
tree_model_bmi</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  10ms 
## n= 768 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 768 268 no (0.6510417 0.3489583)  
##   2) bmi&lt; 29.85 291  47 no (0.8384880 0.1615120) *
##   3) bmi&gt;=29.85 477 221 no (0.5366876 0.4633124)  
##     6) bmi&lt; 40.85 392 168 no (0.5714286 0.4285714) *
##     7) bmi&gt;=40.85 85  32 yes (0.3764706 0.6235294) *</code></pre>
</div>
<div id="traintest-split" class="section level2">
<h2>1-6 Train/test split</h2>
<p>In order to test your models, you need to build and test the model on two different parts of the data - otherwise, it’s like cheating on an exam (as you already know the answers).</p>
<p>The data split is an integral part of the modeling process. You will dive into this by splitting the diabetes data and confirming the split proportions.</p>
<pre class="r"><code># Create the split
diabetes_split &lt;- initial_split(diabetes, prop = 0.8)

# Extract the training and test set
diabetes_train &lt;- training(diabetes_split)
diabetes_test  &lt;- testing(diabetes_split)

# Verify the proportions of both sets
round(nrow(diabetes_train) / nrow(diabetes), 2) == 0.80</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>round(nrow(diabetes_test) / nrow(diabetes), 2) == 0.20</code></pre>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="avoiding-class-imbalances" class="section level2">
<h2>1-7 Avoiding class imbalances</h2>
<p>Some data contains very imbalanced outcomes - like a rare disease dataset. When splitting randomly, you might end up with a very unfortunate split. Imagine all the rare observations are in the test and none in the training set. That would ruin your whole training process!</p>
<p>Fortunately, the initial_split() function provides a remedy. You are going to observe and solve these so-called class imbalances in this exercise.</p>
<p>There’s already a split object diabetes_split available with a 75% training and 25% test split.</p>
<pre class="r"><code># Proportion of &#39;yes&#39; outcomes in the training data
counts_train &lt;- table(training(diabetes_split)$outcome)
prop_yes_train &lt;- counts_train[&quot;yes&quot;] / sum(counts_train)

# Proportion of &#39;yes&#39; outcomes in the test data
counts_test &lt;- table(testing(diabetes_split)$outcome)
prop_yes_test &lt;- counts_test[&quot;yes&quot;] / sum(counts_test)

paste(&quot;Proportion of positive outcomes in training set:&quot;, round(prop_yes_train, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in training set: 0.37&quot;</code></pre>
<pre class="r"><code>paste(&quot;Proportion of positive outcomes in test set:&quot;, round(prop_yes_test, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in test set: 0.27&quot;</code></pre>
<pre class="r"><code># Create a split with a constant outcome distribution
diabetes_split &lt;- initial_split(diabetes,
prop = 0.75,
strata = outcome)

# Proportion of &#39;yes&#39; outcomes in the training data
counts_train &lt;- table(training(diabetes_split)$outcome)
prop_yes_train &lt;- counts_train[&quot;yes&quot;] / sum(counts_train)

# Proportion of &#39;yes&#39; outcomes in the test data
counts_test &lt;- table(testing(diabetes_split)$outcome)
prop_yes_test &lt;- counts_test[&quot;yes&quot;] / sum(counts_test)

paste(&quot;Proportion of positive outcomes in training set:&quot;, round(prop_yes_train, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in training set: 0.35&quot;</code></pre>
<pre class="r"><code>paste(&quot;Proportion of positive outcomes in test set:&quot;, round(prop_yes_test, 2))</code></pre>
<pre><code>## [1] &quot;Proportion of positive outcomes in test set: 0.35&quot;</code></pre>
</div>
<div id="from-zero-to-hero" class="section level2">
<h2>1-8 From zero to hero</h2>
<p>You mastered the skills of creating a model specification and splitting the data into training and test sets. You also know how to avoid class imbalances in the split. It’s now time to combine what you learned in the preceding lesson and build your model using only the training set!</p>
<p>You are going to build a proper machine learning pipeline. This is comprised of creating a model specification, splitting your data into training and test sets, and last but not least, fitting the training data to a model. Enjoy!</p>
<pre class="r"><code># Create the balanced data split
diabetes_split &lt;- initial_split(diabetes,
prop = 0.75,
strata = outcome)

# Build the specification of the model
tree_spec &lt;- decision_tree() %&gt;% 
  set_engine(&quot;rpart&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)

# Train the model
model_trained &lt;- tree_spec %&gt;% 
  fit(formula = factor(outcome) ~ bmi+skin_thickness, 
      data = training(diabetes_split))

model_trained</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  0ms 
## n= 576 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 576 201 no (0.65104167 0.34895833)  
##     2) bmi&lt; 29.65 205  33 no (0.83902439 0.16097561) *
##     3) bmi&gt;=29.65 371 168 no (0.54716981 0.45283019)  
##       6) bmi&lt; 48.1 359 157 no (0.56267409 0.43732591)  
##        12) skin_thickness&gt;=14.5 265 108 no (0.59245283 0.40754717)  
##          24) skin_thickness&lt; 17.5 10   0 no (1.00000000 0.00000000) *
##          25) skin_thickness&gt;=17.5 255 108 no (0.57647059 0.42352941)  
##            50) bmi&gt;=45.95 7   1 no (0.85714286 0.14285714) *
##            51) bmi&lt; 45.95 248 107 no (0.56854839 0.43145161)  
##             102) bmi&lt; 40.8 219  89 no (0.59360731 0.40639269) *
##             103) bmi&gt;=40.8 29  11 yes (0.37931034 0.62068966) *
##        13) skin_thickness&lt; 14.5 94  45 yes (0.47872340 0.52127660)  
##          26) bmi&gt;=33.5 51  24 no (0.52941176 0.47058824)  
##            52) bmi&lt; 37.65 26   9 no (0.65384615 0.34615385) *
##            53) bmi&gt;=37.65 25  10 yes (0.40000000 0.60000000) *
##          27) bmi&lt; 33.5 43  18 yes (0.41860465 0.58139535) *
##       7) bmi&gt;=48.1 12   1 yes (0.08333333 0.91666667) *</code></pre>
</div>
<div id="make-predictions" class="section level2">
<h2>Make predictions</h2>
<p>Making predictions with data is one of the fundamental goals of machine learning. Now that you know how to split the data and fit a model, it’s time to make predictions about unseen samples with your models.</p>
<p>You are going to make predictions about your test set using a model obtained by fitting the training data to a tree specification.</p>
<pre class="r"><code>model &lt;- tree_spec %&gt;% 
  fit(formula = factor(outcome) ~ ., 
      data = diabetes_train)

# Generate predictions
predictions &lt;- predict(model,
                   diabetes_test, type = &quot;class&quot;)

# Add the true outcomes
predictions_combined &lt;- predictions %&gt;% 
  mutate(true_class = diabetes_test$outcome)


# Print the first lines of the result
head(predictions_combined)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   .pred_class true_class
##   &lt;fct&gt;       &lt;chr&gt;     
## 1 no          no        
## 2 yes         yes       
## 3 yes         yes       
## 4 yes         yes       
## 5 yes         no        
## 6 no          no</code></pre>
</div>
<div id="crack-the-matrix" class="section level2">
<h2>1-11 Crack the matrix</h2>
<p>Visual representations are a great and intuitive way to assess results. One way to visualize and assess the performance of your model is by using a confusion matrix. In this exercise, you will create the confusion matrix of your predicted values to see in which cases it performs well and in which cases it doesn’t.</p>
<pre class="r"><code># The confusion matrix
diabetes_matrix &lt;- conf_mat(data = predictions_combined,
estimate = .pred_class,
truth = true_class)</code></pre>
<pre><code>## Warning in vec2table(truth = truth, estimate = estimate, dnn = dnn, ...): `truth`
## was converted to a factor</code></pre>
<pre class="r"><code># Print the matrix
print(diabetes_matrix)</code></pre>
<pre><code>##           Truth
## Prediction no yes
##        no  87  10
##        yes 26  31</code></pre>
</div>
<div id="are-you-predicting-correctly" class="section level2">
<h2>Are you predicting correctly?</h2>
<p>Your model should be as good as possible, right? One way you can assess this is by counting how often it predicted the correct classes compared to the total number of predictions it made. As discussed in the video, we call this performance measure accuracy. You can either calculate this manually or by using a handy shortcut. Both obtain the same result.</p>
<pre class="r"><code># The accuracy calculated by a function
acc_auto &lt;- accuracy(predictions_combined, estimate = .pred_class, truth = factor(true_class))

acc_auto$estimate</code></pre>
<pre><code>## Warning: Unknown or uninitialised column: `estimate`.</code></pre>
<pre><code>## NULL</code></pre>
</div>
