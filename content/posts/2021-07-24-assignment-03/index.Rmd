---
title: assignment-03--
author: ''
date: '2021-07-24'
slug: []
categories: []
tags: []
---

---
title: "Assignment-03"
author: ''
date: '2021-07-22'
slug: []
categories: []
tags: []
---

## Resampling



```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(knitr)
library(markdown)
bank <- read_csv("bank.csv") %>%
  select(c(age, job, marital, education, default, balance, housing, loan, y)) %>%
  mutate(subscription_status=ifelse(y == 'yes', 1, 0))
 

```



```{r, echo=FALSE, message=FALSE, warning=FALSE}
mdl_bank_S <- glm(subscription_status ~ balance , data=bank, family = binomial)


```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mdl_bank_M <- glm(subscription_status ~ age+job+marital+education+default+balance+housing+loan , data=bank, family = binomial)

```



```{r, warning=FALSE, message=FALSE}
library(caret)

library(yardstick)

library(tictoc)
```


## Resampling methods

## ***Cross-Validation methods***

## LOOCV (Leave one out cross validation) 

For this dataset, the LOOCV method takes long time to process. Because it consists building the model for (4521-1) times, evaluating the remaining sample with the model and averaging error.

We are using tictoc library to measure the running time for each code chunk to evaluate performance of these methods in terms of running time.

---


```{r, warning=FALSE}

start_time_LOOCV <- Sys.time()


# Define training control
train.control_LOOCV <- trainControl(method = "LOOCV")

# Train the model
model_LOOCV <- train(subscription_status ~ ., data = bank, method = "lm",
               trControl = train.control_LOOCV)
# Summarize the results
print(model_LOOCV)

end_time_LOOCV <- Sys.time()

LOOCV_run_time= end_time_LOOCV - start_time_LOOCV

#Print run time
print(LOOCV_run_time)

```

Calculated errors shows there is negligible error in predicting outputs. we will see this in final section called confusion matrix. Besides, running of this method lasted "1.24 min".

## K-fold cross-validation

Here k selected as 10 (by setting number argument to 10 in trainControl function). This number selected because there are 4521 observation in dataset.

```{r, warning=FALSE}
# Define training control
set.seed(120) 

start_time_K_fold <- Sys.time()

train.control_K_fold <- trainControl(method = "repeatedcv", number = 10, repeats = 4)

# Train the model
model_K_fold <- train(subscription_status ~., data = bank, method = "lm",
               trControl = train.control_K_fold)
# Summarize the results
print(model_K_fold)

end_time_K_fold <- Sys.time()

K_fold_run_time= end_time_K_fold - start_time_K_fold

#Print run time
print(K_fold_run_time)
```

Again error is nearly zero but code run in just 1.4 seconds that is a great improvement in caparison with LOOCV method.

## Bootstrap method 


```{r, warning=FALSE}
# Define training control

start_time_bootstrap <- Sys.time()

train.control_Bootstrap <- trainControl(method = "boot", number = 100)

# Train the model
model_Bootstrap <- train(subscription_status ~., data = bank, method = "lm",
               trControl = train.control_Bootstrap)
# Summarize the results
print(model_Bootstrap)

end_time_bootstrap <- Sys.time()

bootstrap_run_time= end_time_bootstrap - start_time_bootstrap

#Print run time
print(bootstrap_run_time)
```

Error is also negligible here, and code run in just 3.6 seconds that is a great improvement in caparison with LOOCV method but slower than K-fold. Obviously, number of iteration affects timing.

## Evaluating performance with confusion matrix

Calculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset.

```{r}

status_predictions_SLR <- predict(mdl_bank_S, new_data = bank, type = "response")

status_predictions_MLR <- predict(mdl_bank_M, new_data = bank, type = "response")

status_predictions_model_LOOCV <- predict(model_LOOCV$finalModel, new_data = bank)
status_predictions_model_K_fold <- predict(model_K_fold$finalModel, new_data = bank)
status_predictions_model_Bootstrap <- predict(model_Bootstrap$finalModel, new_data = bank)

test_results_SLR <- bank %>%
  bind_cols(status_pred=status_predictions_SLR)

test_results_MLR <- bank %>%
  bind_cols(status_pred=status_predictions_MLR)

test_results_model_LOOCV <- bank %>%
  bind_cols(status_pred=status_predictions_model_LOOCV)


test_results_model_K_fold <- bank %>%
  bind_cols(status_pred=status_predictions_model_K_fold)

test_results_model_Bootstrap <- bank %>%
  bind_cols(status_pred=status_predictions_model_Bootstrap)


```


```{r}
# LOOCV

p <- test_results_model_LOOCV %>% select(subscription_status,status_pred) %>% 
   mutate(truth=ifelse(subscription_status == '1' , 'yes', 'no')) %>% 
   mutate(estimate=ifelse(status_pred == '1' , 'yes', 'no'))

# Calculate the confusion matrix
conf_mat(p, truth = truth,
    estimate = estimate)%>% 
  # Create a heat map
  autoplot(type = 'heatmap')
```


```{r}
# K_fold

p2 <- test_results_model_K_fold %>% select(subscription_status,status_pred) %>% 
   mutate(truth=ifelse(subscription_status == '1' , 'yes', 'no')) %>% 
   mutate(estimate=ifelse(status_pred == '1' , 'yes', 'no'))

# Calculate the confusion matrix
conf_mat(p2, truth = truth,
    estimate = estimate)%>% 
  # Create a heat map
  autoplot(type = 'heatmap')
```


```{r}
# Bootstrap

p3 <- test_results_model_Bootstrap %>% select(subscription_status,status_pred) %>% 
   mutate(truth=ifelse(subscription_status == '1' , 'yes', 'no')) %>% 
   mutate(estimate=ifelse(status_pred == '1' , 'yes', 'no'))

# Calculate the confusion matrix
conf_mat(p3, truth = truth,
    estimate = estimate)%>% 
  # Create a heat map
  autoplot(type = 'heatmap')

```


```{r}
# SLR

p4 <- test_results_SLR %>% select(subscription_status,status_pred) %>% 
   mutate(truth=ifelse(subscription_status == '1' , 'yes', 'no')) %>% 
   mutate(estimate=ifelse(status_pred > 0.13 , 'yes', 'no'))

# Calculate the confusion matrix
conf_mat(p4, truth = truth,
    estimate = estimate)%>% 
  # Create a heat map
  autoplot(type = 'heatmap')

```




```{r}
#  MLR

p4 <- test_results_MLR %>% select(subscription_status,status_pred) %>% 
   mutate(truth=ifelse(subscription_status == '1' , 'yes', 'no')) %>% 
   mutate(estimate=ifelse(status_pred > 0.3 , 'yes', 'no'))

# Calculate the confusion matrix
conf_mat(p4, truth = truth,
    estimate = estimate)%>% 
  # Create a heat map
  autoplot(type = 'heatmap')

```

Confusion matrices and heatmap plots shows there are no error in predictions.

Explanations about predictions of SLR (Single linear regression) and MLR (Multiple linear regression) is necessary. In these cases, we defined a threshold of probabilities that separates "yes" and "no" outcomes. These thresholds are selected so that number of errors (numbers in sub diagonal of matrix) become minimum.
 