---
title: Regression Model Validation- Cross-Validation
author: ''
date: '2021-07-21'
slug: []
categories: []
tags:
  - Regression
  - Classification
---

```{r}
library(tidyverse)
library(caret)
```

```{r}
# Load the data
data("swiss")
# Inspect the data
sample_n(swiss, 3)
```

## Spliting dataset
 
 splits the swiss data set so that 80% is used for training a linear regression model and 20% is used to evaluate the model performance.
 
```{r}
# Split the data into training and test set

set.seed(10242)

training.samples <- swiss$Fertility %>%
  createDataPartition(p = 0.8, list = FALSE)

train.data  <- swiss[training.samples, ]

test.data <- swiss[-training.samples, ]

# Build the model
model <- lm(Fertility ~., data = train.data)

# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(test.data)

data.frame( R2 = R2(predictions, test.data$Fertility),
            RMSE = RMSE(predictions, test.data$Fertility),
            MAE = MAE(predictions, test.data$Fertility))
```
 
 When comparing two models, the one that produces the *lowest test sample RMSE* is the *preferred model*.
 
##  RMSE

 the RMSE and the MAE are measured in the same scale as the outcome variable. Dividing the RMSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible.
 
```{r}
RMSE(predictions, test.data$Fertility)/mean(test.data$Fertility)
```
 
## Leave one out cross validation - LOOCV

```{r}
# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
               trControl = train.control)
# Summarize the results
summary(model)

#print(model)
```



