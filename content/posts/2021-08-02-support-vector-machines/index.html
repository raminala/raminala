---
title: 'Support Vector Machines '
author: ''
date: '2021-08-02'
slug: []
categories: []
tags:
  - Support Vector Machines
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.2     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(e1071)</code></pre>
<div id="generate-a-2d-uniformly-distributed-dataset." class="section level2">
<h2>1-1- Generate a 2d uniformly distributed dataset.</h2>
<p>The aim of this lesson is to create a dataset that will be used to illustrate the basic principles of support vector machines. In this exercise we will do the first step, which is to create a 2 dimensional uniformly distributed dataset containing 600 datapoints.</p>
<pre class="r"><code>#set seed
set.seed(42)

#set number of data points. 
n &lt;- 600

#Generate data frame with two uniformly distributed predictors lying between 0 and 1.
df &lt;- data.frame(x1 = runif(n), 
                 x2 = runif(n))</code></pre>
</div>
<div id="create-a-decision-boundary" class="section level2">
<h2>1-2- Create a decision boundary</h2>
<p>The dataset you created in the previous exercise is available to you in the dataframe df (recall that it consists of two uniformly distributed variables x1 and x2, lying between 0 and 1). In this exercise you will add a class variable to that dataset. You will do this by creating a variable y whose value is -1 or +1 depending on whether the point (x1, x2) lies below or above the straight line that passes through the origin and has slope 1.4.</p>
<p>Create a new column y in the dataframe df with the following specs:
y = -1 if x2 &lt; 1.4<em>x1
y = 1 if x2 &gt; 1.4</em>x1</p>
<pre class="r"><code>#classify data points depending on location
df$y &lt;- factor(ifelse(df$x2 - 1.4*df$x1 &lt; 0, -1, 1), 
    levels = c(-1, 1))</code></pre>
</div>
<div id="introduce-a-margin-in-the-dataset" class="section level2">
<h2>1-2- Introduce a margin in the dataset</h2>
<p>Create a margin in the dataset that you generated in the previous exercise and then display the margin in a plot. Recall that the slope of the linear decision boundary you created in the previous exercise is 1.4.</p>
<pre class="r"><code>#set margin
delta &lt;- 0.07

# retain only those points that lie outside the margin
df1 &lt;- df[abs(1.4*df$x1 - df$x2) &gt; delta, ]

#build plot
plot_margins &lt;- ggplot(data = df1, aes(x = x1, y = x2, color = y)) + geom_point() + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;)) + 
    geom_abline(slope = 1.4, intercept = 0)+
    geom_abline(slope = 1.4, intercept = delta, linetype = &quot;dashed&quot;) +
    geom_abline(slope = 1.4, intercept = -delta, linetype = &quot;dashed&quot;)
 
#display plot 
plot_margins</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="creating-training-and-test-datasets" class="section level2">
<h2>2-2-Creating training and test datasets</h2>
<p>Splitting a dataset into training and test sets is an important step in building and testing a classification model. The training set is used to build the model and the test set to evaluate its predictive accuracy.</p>
<p>In this exercise, you will split the dataset you created in the previous chapter into training and test sets. The dataset has been loaded in the dataframe df and a seed has already been set to ensure reproducibility.</p>
<pre class="r"><code>#split train and test data in an 80/20 proportion
df[, &quot;train&quot;] &lt;- ifelse(runif(nrow(df))&lt;0.8, 1, 0)

#assign training rows to data frame trainset
trainset &lt;- df[df$train == 1, ]
#assign test rows to data frame testset
testset &lt;- df[df$train == 0, ]

#find index of &quot;train&quot; column
trainColNum &lt;- grep(&quot;train&quot;, names(df))

#remove &quot;train&quot; column from train and test dataset
trainset &lt;- trainset[, -trainColNum]
testset &lt;- testset[, -trainColNum]</code></pre>
</div>
<div id="building-a-linear-svm-classifier" class="section level2">
<h2>2-3 Building a linear SVM classifier</h2>
<p>In this exercise, you will use the svm() function from the e1071 library to build a linear SVM classifier using training dataset you created in the previous exercise. The training dataset has been loaded for you in the dataframe trainset</p>
<pre class="r"><code>#build svm model, setting required parameters
svm_model&lt;- svm(y ~ ., 
                data = trainset, 
                type = &quot;C-classification&quot;, 
                kernel = &quot;linear&quot;, 
                scale = FALSE)</code></pre>
</div>
<div id="exploring-the-model-and-calculating-accuracy" class="section level2">
<h2>2-4 Exploring the model and calculating accuracy</h2>
<p>In this exercise you will explore the contents of the model and calculate its training and test accuracies.</p>
<pre><code>##  [1] &quot;call&quot;            &quot;type&quot;            &quot;kernel&quot;          &quot;cost&quot;           
##  [5] &quot;degree&quot;          &quot;gamma&quot;           &quot;coef0&quot;           &quot;nu&quot;             
##  [9] &quot;epsilon&quot;         &quot;sparse&quot;          &quot;scaled&quot;          &quot;x.scale&quot;        
## [13] &quot;y.scale&quot;         &quot;nclasses&quot;        &quot;levels&quot;          &quot;tot.nSV&quot;        
## [17] &quot;nSV&quot;             &quot;labels&quot;          &quot;SV&quot;              &quot;index&quot;          
## [21] &quot;rho&quot;             &quot;compprob&quot;        &quot;probA&quot;           &quot;probB&quot;          
## [25] &quot;sigma&quot;           &quot;coefs&quot;           &quot;na.action&quot;       &quot;fitted&quot;         
## [29] &quot;decision.values&quot; &quot;terms&quot;</code></pre>
<pre><code>##               x1          x2
## 7   0.7365883146 0.768852225
## 8   0.1346665972 0.163928856
## 14  0.2554288243 0.351792022
## 19  0.4749970816 0.486642912
## 40  0.6117786434 0.714631942
## 45  0.4317512489 0.520339758
## 58  0.1712643304 0.100229354
## 61  0.6756072745 0.772399305
## 70  0.2405447396 0.287320588
## 88  0.0899805163 0.124696854
## 99  0.7439746463 0.912029979
## 103 0.2165673110 0.202548483
## 116 0.5373766953 0.724161461
## 118 0.3556659538 0.298152283
## 140 0.6236134572 0.859278115
## 143 0.4640695513 0.535269056
## 144 0.7793681615 0.941694443
## 145 0.7335279596 0.892355152
## 160 0.6801641781 0.944522879
## 168 0.3170533518 0.408421214
## 173 0.4140496817 0.380267640
## 177 0.8246794064 0.918857760
## 183 0.3169752378 0.381375618
## 194 0.1290892835 0.021196302
## 198 0.1123082417 0.145815430
## 199 0.7431877197 0.824081728
## 204 0.4427962683 0.532290264
## 209 0.2524584394 0.281511990
## 245 0.5496204135 0.539190846
## 253 0.2697161783 0.288755647
## 261 0.7148536935 0.978302628
## 263 0.3110496188 0.298268895
## 268 0.2050496121 0.182046106
## 272 0.7853494422 0.870432480
## 278 0.4037828147 0.476424339
## 286 0.1709963905 0.164468810
## 294 0.3864540118 0.370921416
## 295 0.3324459905 0.382318948
## 307 0.3921784570 0.343302177
## 325 0.5648222226 0.618285144
## 335 0.6753195773 0.773493237
## 338 0.3169501573 0.333509587
## 344 0.3597852497 0.345139100
## 356 0.4757110127 0.619424278
## 359 0.4974644876 0.654067937
## 383 0.3650141200 0.454168669
## 393 0.6568108753 0.815567016
## 400 0.0755990995 0.007417523
## 403 0.6307261516 0.819845088
## 404 0.4187716243 0.539360294
## 406 0.1079870730 0.022227321
## 415 0.1459286907 0.166783036
## 427 0.4664852461 0.464965629
## 443 0.3626018071 0.369346223
## 450 0.0619409799 0.011438249
## 464 0.0728918249 0.051444089
## 466 0.6399842701 0.695480783
## 468 0.1887495262 0.244276195
## 469 0.3938296412 0.391966397
## 479 0.1730011790 0.136427131
## 490 0.0486410747 0.045257503
## 501 0.1365052082 0.059230526
## 503 0.5195604505 0.627322678
## 525 0.6494539515 0.833293378
## 526 0.6903516576 0.790328991
## 549 0.3458497624 0.413091426
## 569 0.5174370031 0.657742033
## 590 0.7148487861 0.902375512
## 595 0.8058112133 0.937903824
## 596 0.4862421674 0.671019064
## 600 0.4587231132 0.446819442
## 18  0.1174873617 0.206528918
## 37  0.0073341469 0.108096598
## 50  0.6188382073 0.922261254
## 59  0.2610879638 0.472588875
## 64  0.5664884241 0.823944246
## 77  0.0078847387 0.221126178
## 84  0.6456318784 0.955186023
## 86  0.5636468416 0.824162796
## 90  0.3052183695 0.548420829
## 92  0.0002388966 0.122946701
## 97  0.3330719834 0.534791055
## 104 0.3889450287 0.717138722
## 111 0.6089374525 0.882491525
## 114 0.4527315726 0.737772155
## 115 0.5357899938 0.796319364
## 122 0.4106351258 0.625107899
## 132 0.6034740848 0.958318281
## 150 0.1490720524 0.377477208
## 158 0.0290858189 0.148069276
## 175 0.4274944656 0.725024226
## 178 0.5923042425 0.900228734
## 189 0.1333296183 0.390023998
## 196 0.0531294835 0.276241161
## 210 0.2596899802 0.503687580
## 212 0.6498758376 0.920630739
## 213 0.3364191323 0.499029460
## 215 0.4513108502 0.743930877
## 217 0.5746373343 0.930141046
## 229 0.0483467767 0.218475638
## 237 0.0865806018 0.263718613
## 239 0.5545858634 0.935806216
## 242 0.5614379565 0.806933147
## 247 0.1594698546 0.276332649
## 249 0.4992728804 0.812805236
## 260 0.6189515470 0.936232617
## 280 0.3679018202 0.574839342
## 293 0.3152607968 0.625878707
## 308 0.1588467748 0.244498280
## 309 0.3199476011 0.541676977
## 311 0.1078112544 0.374908455
## 353 0.0736677952 0.311655199
## 360 0.4705006266 0.679453118
## 366 0.5825784358 0.900965292
## 377 0.2014912262 0.296121662
## 378 0.1084886545 0.376079086
## 389 0.1687364434 0.269311399
## 409 0.0842775232 0.235715229
## 429 0.6899186051 0.987516199
## 434 0.0838848404 0.142832364
## 441 0.6501286714 0.944913187
## 449 0.6658076318 0.999379032
## 465 0.1147626776 0.363946523
## 471 0.3479114065 0.564496977
## 472 0.0014338985 0.041401571
## 476 0.4636517153 0.678141702
## 481 0.2329343846 0.409654615
## 488 0.2485451805 0.533491509
## 507 0.5753528811 0.809969487
## 508 0.1465723943 0.391752972
## 524 0.1270027745 0.348539336
## 530 0.2665205784 0.458110426
## 548 0.2248729232 0.328771049
## 553 0.6809630166 0.969763529
## 558 0.2770604359 0.510976796
## 562 0.2056735931 0.433566746
## 568 0.2516635812 0.363130948
## 581 0.2458533479 0.494881822
## 586 0.4806177358 0.786027395
## 591 0.3165616125 0.563688410</code></pre>
<pre><code>##   [1]   5   6  10  14  30  33  45  47  54  72  83  85  95  97 113 116 117 118
##  [19] 132 138 142 146 150 160 164 165 170 175 202 207 214 216 221 225 229 235
##  [37] 242 243 253 269 277 279 284 293 296 316 325 332 335 336 338 345 352 366
##  [55] 372 383 385 387 388 397 406 414 416 434 435 453 468 485 489 490 492  13
##  [73]  27  37  46  50  61  68  70  74  76  81  86  90  93  94 101 107 122 130
##  [91] 144 147 156 162 176 178 179 181 183 192 196 197 200 204 205 213 231 241
## [109] 254 255 257 290 297 302 311 312 321 340 354 359 364 371 384 390 391 395
## [127] 398 404 420 421 433 438 452 457 460 464 467 477 481 486</code></pre>
<pre><code>## [1] -0.2796884</code></pre>
<pre><code>## [1] 0.9776423</code></pre>
<pre><code>## [1] 0.9814815</code></pre>
</div>
<div id="visualizing-support-vectors-using-ggplot" class="section level2">
<h2>2-5 Visualizing support vectors using ggplot</h2>
<p>In this exercise you will plot the training dataset you used to build a linear SVM and mark out the support vectors. The training dataset has been preloaded for you in the dataframe trainset and the SVM model is stored in the variable svm_model.</p>
<pre class="r"><code>#build scatter plot of training dataset
scatter_plot &lt;- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;))
 scatter_plot</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>#add plot layer marking out the support vectors 
layered_plot &lt;- 
    scatter_plot + geom_point(data = trainset[svm_model$index, ], aes(x = x1, y = x2), color = &quot;purple&quot;, size = 4, alpha = 0.5)

#display plot
layered_plot</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
</div>
<div id="visualizing-decision-margin-bounds-using-ggplot2" class="section level2">
<h2>2-6 Visualizing decision &amp; margin bounds using ggplot2</h2>
<p>In this exercise, you will add the decision and margin boundaries to the support vector scatter plot created in the previous exercise. The SVM model is available in the variable svm_model and the weight vector has been precalculated for you and is available in the variable w.</p>
<pre class="r"><code>w=data.frame(x1=6.55241 , x2=-4.73278)

#calculate slope and intercept of decision boundary from weight vector and svm model
slope_1 &lt;- -w[1]/w[2]
intercept_1 &lt;- svm_model$rho/w[2]

#build scatter plot of training dataset
scatter_plot &lt;- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;))
#add decision boundary
plot_decision &lt;- scatter_plot + geom_abline(slope = slope_1, intercept = intercept_1) 
#add margin boundaries
plot_margins &lt;- plot_decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = &quot;dashed&quot;)+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = &quot;dashed&quot;)
#display plot
#plot_margins</code></pre>
</div>
<div id="visualizing-decision-margin-bounds-using-plot" class="section level2">
<h2>2-7 Visualizing decision &amp; margin bounds using <code>plot()</code></h2>
<p>In this exercise, you will rebuild the SVM model (as a refresher) and use the built in SVM plot() function to visualize the decision regions and support vectors. The training data is available in the dataframe trainset.</p>
<pre class="r"><code>#build svm model
svm_model&lt;- 
    svm(y ~ ., data = trainset, type = &quot;C-classification&quot;, 
        kernel = &quot;linear&quot;, scale = FALSE)

#plot decision boundaries and support vectors for the training data
plot(x = svm_model, data = trainset)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="tuning-a-linear-svm" class="section level2">
<h2>2-8 Tuning a linear SVM</h2>
<p>In this exercise you will study the influence of varying cost on the number of support vectors for linear SVMs. To do this, you will build two SVMs, one with cost = 1 and the other with cost = 100 and find the number of support vectors. A model training dataset is available in the dataframe trainset.</p>
<p>.</p>
<pre class="r"><code>#build svm model, cost = 1
svm_model_1 &lt;- svm(y ~ .,
                   data = trainset,
                   type = &quot;C-classification&quot;,
                   cost = 1,
                   kernel = &quot;linear&quot;,
                   scale = FALSE)

#print model details
svm_model_1</code></pre>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = trainset, type = &quot;C-classification&quot;, 
##     cost = 1, kernel = &quot;linear&quot;, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  140</code></pre>
<pre class="r"><code>#build svm model, cost = 100
svm_model_100 &lt;- svm(y ~ .,
                   data = trainset,
                   type = &quot;C-classification&quot;,
                   cost = 100,
                   kernel = &quot;linear&quot;,
                   scale = FALSE)

#print model details
svm_model_100</code></pre>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = trainset, type = &quot;C-classification&quot;, 
##     cost = 100, kernel = &quot;linear&quot;, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  100 
## 
## Number of Support Vectors:  32</code></pre>
</div>
<div id="visualizing-decision-boundaries-and-margins" class="section level2">
<h2>2-9 Visualizing decision boundaries and margins</h2>
<p>In the previous exercise you built two linear classifiers for a linearly separable dataset, one with cost = 1 and the other cost = 100. In this exercise you will visualize the margins for the two classifiers on a single plot. The following objects are available for use:</p>
<p>The training dataset: trainset.
The cost = 1 and cost = 100 classifiers in svm_model_1 and svm_model_100, respectively.
The slope and intercept for the cost = 1 classifier is stored in slope_1 and intercept_1.
The slope and intercept for the cost = 100 classifier is stored in slope_100 and intercept_100.
Weight vectors for the two costs are stored in w_1 and w_100, respectively
A basic scatter plot of the training data is stored in train_plot</p>
<pre class="r"><code>#add decision boundary and margins for cost = 1 to training data scatter plot</code></pre>
</div>
<div id="a-multiclass-classification-problem" class="section level2">
<h2>2-10 A multiclass classification problem</h2>
<p>In this exercise, you will use the svm() function from the e1071 library to build a linear multiclass SVM classifier for a dataset that is known to be perfectly linearly separable. Calculate the training and test accuracies, and plot the model using the training data. The training and test datasets are available in the dataframes trainset and testset. Use the default setting for the cost parameter.</p>
<pre class="r"><code>svm_model&lt;- 
    svm(y ~ ., data = trainset, type = &quot;C-classification&quot;, 
        kernel = &quot;linear&quot;, scale = FALSE)

#compute training accuracy
pred_train &lt;- predict(svm_model, trainset)
mean(pred_train == trainset$y)</code></pre>
<pre><code>## [1] 0.9776423</code></pre>
<pre class="r"><code>#compute test accuracy
pred_test &lt;- predict(svm_model, testset)
mean(pred_test == testset$y)</code></pre>
<pre><code>## [1] 0.9814815</code></pre>
<pre class="r"><code>#plot
plot(svm_model, trainset)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="generating-a-2d-radially-separable-dataset" class="section level2">
<h2>3-2-Generating a 2d radially separable dataset</h2>
<p>In this exercise you will create a 2d radially separable dataset containing 400 uniformly distributed data points.</p>
<pre class="r"><code>#set number of variables and seed
n &lt;- 400
set.seed(1)

#Generate data frame with two uniformly distributed predictors, x1 and x2
df &lt;- data.frame(x1 = runif(n, min = -1, max = 1), 
                 x2 = runif(n, min = -1, max = 1))

#We want a circular boundary. Set boundary radius 
radius &lt;- 0.8
radius_squared &lt;- radius^2

#create dependent categorical variable, y, with value -1 or 1 depending on whether point lies
#within or outside the circle.
df$y &lt;- factor(ifelse((df$x1)^2 + (df$x2)^2 &lt; radius_squared, -1, 1), levels = c(-1, 1))</code></pre>
</div>
<div id="visualizing-the-dataset" class="section level2">
<h2>3-3-Visualizing the dataset</h2>
<p>In this exercise you will use ggplot() to visualize the dataset you created in the previous exercise. The dataset is available in the dataframe df. Use color to distinguish between the two classes.</p>
<pre class="r"><code>#build scatter plot, distinguish class by color
scatter_plot &lt;- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() +
    scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;))

#display plot
scatter_plot</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="linear-svm-for-a-radially-separable-dataset" class="section level2">
<h2>3-5- Linear SVM for a radially separable dataset</h2>
<p>In this exercise you will build two linear SVMs, one for cost = 1 (default) and the other for cost = 100, for the radially separable dataset you created in the first lesson of this chapter. You will also calculate the training and test accuracies for both costs.</p>
<pre class="r"><code>#split train and test data in an 80/20 proportion
df[, &quot;train&quot;] &lt;- ifelse(runif(nrow(df))&lt;0.8, 1, 0)

#assign training rows to data frame trainset
trainset &lt;- df[df$train == 1, ]
testset&lt;- df[df$train == 0, ]</code></pre>
<pre class="r"><code>#default cost mode;
svm_model_1 &lt;- svm(y ~ ., data = trainset, type = &quot;C-classification&quot;, cost = 1, kernel = &quot;linear&quot;)</code></pre>
<pre><code>## Warning in svm.default(x, y, scale = scale, ..., na.action = na.action):
## Variable(s) &#39;train&#39; constant. Cannot scale data.</code></pre>
<pre class="r"><code>#training accuracy
pred_train &lt;- predict(svm_model_1, trainset)
mean(pred_train == trainset$y)</code></pre>
<pre><code>## [1] 0.5673981</code></pre>
<pre class="r"><code>#test accuracy
pred_test &lt;- predict(svm_model_1, testset)
mean(pred_test == testset$y)</code></pre>
<pre><code>## [1] 0.4938272</code></pre>
</div>
<div id="average-accuracy-for-linear-svm" class="section level2">
<h2>3-6- Average accuracy for linear SVM</h2>
<p>In this exercise you will calculate the average accuracy for a default cost linear SVM using 100 different training/test partitions of the dataset you generated in the first lesson of this chapter. The e1071 library has been preloaded and the dataset is available in the dataframe df. Use random 80/20 splits of the data in df when creating training and test datasets for each iteration.</p>
<pre class="r"><code># Print average accuracy and standard deviation
accuracy &lt;- rep(NA, 100)
set.seed(2)

# Calculate accuracies for 100 training/test partitions
for (i in 1:100){
    df[, &quot;train&quot;] &lt;- ifelse(runif(nrow(df)) &lt; 0.8, 1, 0)
    trainset &lt;- df[df$train == 1, ]
    testset &lt;- df[df$train == 0, ]
    trainColNum &lt;- grep(&quot;train&quot;, names(trainset))
    trainset &lt;- trainset[, -trainColNum]
    testset &lt;- testset[, -trainColNum]
    svm_model &lt;- svm(y ~ ., data = trainset, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;)
    pred_test &lt;- predict(svm_model, testset)
    accuracy[i] &lt;- mean(pred_test == testset$y)
}

# Print average accuracy and standard deviation
mean(accuracy)</code></pre>
<pre><code>## [1] 0.5554571</code></pre>
<pre class="r"><code>sd(accuracy)</code></pre>
<pre><code>## [1] 0.04243524</code></pre>
</div>
<div id="visualizing-transformed-radially-separable-data" class="section level2">
<h2>3-8-Visualizing transformed radially separable data</h2>
<p>In this exercise you will transform the radially separable dataset you created earlier in this chapter and visualize it in the x1<sup>2-x2</sup>2 plane. As a reminder, the separation boundary for the data is the circle x1^2 + x2^2 = 0.64(radius = 0.8 units). The dataset has been loaded for you in the dataframe df.</p>
<pre class="r"><code>#transform data
df1 &lt;- data.frame(x1sq = df$x1^2, x2sq = df$x2^2, y = df$y)

#plot data points in the transformed space
plot_transformed &lt;- ggplot(data = df1, aes(x = x1sq, y = x2sq, color = y)) + 
    geom_point()+ guides(color = FALSE) + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;))

#add decision boundary and visualize
plot_decision &lt;- plot_transformed + geom_abline(slope = -1, intercept = 0.64)
plot_decision</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="svm-with-polynomial-kernel" class="section level2">
<h2>3-9-SVM with polynomial kernel</h2>
<p>In this exercise you will build a SVM with a quadratic kernel (polynomial of degree 2) for the radially separable dataset you created earlier in this chapter. You will then calculate the training and test accuracies and create a plot of the model using the built in plot() function. The training and test datasets are available in the dataframes trainset and testset, and the e1071 library has been preloaded.</p>
<pre class="r"><code>svm_model&lt;- 
    svm(y ~ ., data = trainset, type = &quot;C-classification&quot;, 
        kernel = &quot;polynomial&quot;, degree = 2)

#measure training and test accuracy
pred_train &lt;- predict(svm_model, trainset)
mean(pred_train == trainset$y)</code></pre>
<pre><code>## [1] 0.9692308</code></pre>
<pre class="r"><code>pred_test &lt;- predict(svm_model, testset)
mean(pred_test == testset$y)</code></pre>
<pre><code>## [1] 0.9466667</code></pre>
<pre class="r"><code>#plot
plot(svm_model, trainset)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div id="using-tune.svm" class="section level2">
<h2>3-11- Using <code>tune.svm()</code></h2>
<p>This exercise will give you hands-on practice with using the tune.svm() function. You will use it to obtain the optimal values for the cost, gamma, and coef0 parameters for an SVM model based on the radially separable dataset you created earlier in this chapter. The training data is available in the dataframe trainset, the test data in testset, and the e1071 library has been preloaded for you. Remember that the class variable y is stored in the third column of the trainset and testset.</p>
<p>Also recall that in the video, Kailash used cost=10^(1:3) to get a range of the cost parameter from 10=10^1 to 1000=10^3 in multiples of 10.</p>
<pre class="r"><code>#tune model
tune_out &lt;- 
    tune.svm(x = trainset[, -3], y = trainset[, 3], 
             type = &quot;C-classification&quot;, 
             kernel = &quot;polynomial&quot;, degree = 2, cost = 10^(-1:2), 
             gamma = c(0.1, 1, 10), coef0 = c(0.1, 1, 10))

#list optimal values
tune_out$best.parameters$cost</code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="r"><code>tune_out$best.parameters$gamma</code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="r"><code>tune_out$best.parameters$coef0</code></pre>
<pre><code>## [1] 1</code></pre>
</div>
<div id="building-and-visualizing-the-tuned-model" class="section level2">
<h2>3-12- Building and visualizing the tuned model</h2>
<p>In the final exercise of this chapter, you will build a polynomial SVM using the optimal values of the parameters that you obtained from tune.svm() in the previous exercise. You will then calculate the training and test accuracies and visualize the model using svm.plot(). The e1071 library has been preloaded and the test and training datasets are available in the dataframes trainset and testset. The output of tune.svm() is available in the variable tune_out.</p>
<pre class="r"><code>#Build tuned model
svm_model &lt;- svm(y~ ., data = trainset, type = &quot;C-classification&quot;, 
                 kernel = &quot;polynomial&quot;, degree = 2, 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma, 
                 coef0 = tune_out$best.parameters$coef0)

#Calculate training and test accuracies
pred_train &lt;- predict(svm_model, trainset)
mean(pred_train == trainset$y)</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>pred_test &lt;- predict(svm_model, testset)
mean(pred_test == testset$y)</code></pre>
<pre><code>## [1] 0.9866667</code></pre>
<pre class="r"><code>#plot model
plot(svm_model, trainset)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="672" />
## 4-2- Generating a complex dataset - part 1</p>
<p>In this exercise you will create a dataset that has two attributes x1 and x2, with x1 normally distributed (mean = -0.5, sd = 1) and x2 uniformly distributed in (-1, 1).</p>
<pre class="r"><code>#number of data points
#number of data points
n &lt;- 1000

#set seed
set.seed(1)

#create dataframe
df &lt;- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), 
                 x2 = runif(n, min = -1, max = 1))</code></pre>
</div>
<div id="generating-a-complex-dataset---part-2" class="section level2">
<h2>4-3- Generating a complex dataset - part 2</h2>
<p>In this exercise, you will create a decision boundary for the dataset you created in the previous exercise. The boundary consists of two circles of radius 0.8 units with centers at x1 = -0.8, x2 = 0) and (x1 = 0.8, x2 = 0) that just touch each other at the origin. Define a binary classification variable y such that points that lie within either of the circles have y = -1 and those that lie outside both circle have y = 1.</p>
<p>The dataset created in the previous exercise is available in the dataframe df.</p>
<pre class="r"><code>#set radius and centers
radius &lt;- 0.8
center_1 &lt;- c(-0.8, 0)
center_2 &lt;- c(0.8, 0)
radius_squared &lt;- radius^2

#create binary classification variable
df$y &lt;- factor(ifelse((df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 &lt; radius_squared|
                      (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 &lt; radius_squared, -1, 1),
                      levels = c(-1, 1))</code></pre>
</div>
<div id="visualizing-the-dataset-1" class="section level2">
<h2>4-4- Visualizing the dataset</h2>
<p>In this exercise you will use ggplot() to visualise the complex dataset you created in the previous exercises. The dataset is available in the dataframe df. You are not required to visualize the decision boundary.</p>
<p>Here you will use coord_equal() to give the x and y axes the same physical representation on the plot, making the circles appear as circles rather than ellipses.</p>
<pre class="r"><code># Plot x2 vs. x1, colored by y
scatter_plot&lt;- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    # Add a point layer
    geom_point() + 
    scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;)) +
    # Specify equal coordinates
    coord_equal()
 
scatter_plot </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>v&lt;-rnorm(500)
#v
mean(v)</code></pre>
<pre><code>## [1] -0.02953537</code></pre>
<pre class="r"><code>rep(c(-1, 1),c(5,3))</code></pre>
<pre><code>## [1] -1 -1 -1 -1 -1  1  1  1</code></pre>
<pre class="r"><code>set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code>library(e1071)</code></pre>
<pre class="r"><code>dat = data.frame(x, y = as.factor(y))

dat</code></pre>
<pre><code>##            X1          X2  y
## 1   0.4992207  0.83459057 -1
## 2  -0.5095988  0.86535664 -1
## 3  -0.3130123 -0.54000741 -1
## 4   0.7136758  0.09454017 -1
## 5  -1.0506291 -0.49390571 -1
## 6  -0.7504743  0.19165412 -1
## 7  -0.3502830 -0.16827662 -1
## 8   0.6202636 -0.03877393 -1
## 9  -0.2169360  0.03409674 -1
## 10 -0.2485535  0.68234694 -1
## 11  1.3264276  1.40061717  1
## 12  0.6886685  1.38550956  1
## 13  2.7057305  1.52631182  1
## 14  2.3354574 -0.49779748  1
## 15 -1.3406379  2.20544301  1
## 16  1.2445945 -0.18418390  1
## 17  0.7721881  1.53916368  1
## 18 -0.3640324  2.32334502  1
## 19  1.6404738  1.17708567  1
## 20 -1.0440209  1.95016425  1</code></pre>
<pre class="r"><code>svmfit = svm(y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10, scale = FALSE)
print(svmfit)</code></pre>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  10 
## 
## Number of Support Vectors:  6</code></pre>
<pre class="r"><code>plot(svmfit, dat)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
</div>
<div id="support-vector-machine" class="section level2">
<h2>Support Vector Machine</h2>
<p>Support Vector Machine works by identifying the optimal <em>decision boundary</em> that separates data points from different groups (or classes), and then predicts the class of new observations based on this separation boundary.</p>
<p>Support vector machine methods can handle both linear and non-linear class boundaries. It can be used for multi-class classification problems.</p>
</div>
<div id="libraries" class="section level2">
<h2>Libraries</h2>
<pre class="r"><code>library(tidyverse)
library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(kernlab)</code></pre>
<pre><code>## 
## Attaching package: &#39;kernlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     alpha</code></pre>
</div>
<div id="loading-data" class="section level2">
<h2>Loading data</h2>
<pre class="r"><code># Load the data
data(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;)
pima.data &lt;- na.omit(PimaIndiansDiabetes2)

# Split the data into training and test set
set.seed(123)

training.samples &lt;- pima.data$diabetes %&gt;%
  createDataPartition(p = 0.8, list = FALSE)

train.data  &lt;- pima.data[training.samples, ]
test.data &lt;- pima.data[-training.samples, ]</code></pre>
</div>
<div id="svm-linear-classifier" class="section level2">
<h2>SVM linear classifier</h2>
<pre class="r"><code># Fit the model on the training set
set.seed(123)
model &lt;- train(
  diabetes ~., data = train.data, method = &quot;svmLinear&quot;,
  trControl = trainControl(&quot;cv&quot;, number = 10),
  preProcess = c(&quot;center&quot;,&quot;scale&quot;)
  )
# Make predictions on the test data
predicted.classes &lt;- model %&gt;% predict(test.data)
head(predicted.classes)</code></pre>
<pre><code>## [1] neg neg pos pos neg neg
## Levels: neg pos</code></pre>
<pre class="r"><code># Compute model accuracy rate
mean(predicted.classes == test.data$diabetes)</code></pre>
<pre><code>## [1] 0.7692308</code></pre>
<pre class="r"><code># Fit the model on the training set
set.seed(123)
model &lt;- train(
  diabetes ~., data = train.data, method = &quot;svmLinear&quot;,
  trControl = trainControl(&quot;cv&quot;, number = 10),
  tuneGrid = expand.grid(C = seq(0, 2, length = 20)),
  preProcess = c(&quot;center&quot;,&quot;scale&quot;)
  )</code></pre>
<pre><code>## Warning: model fit failed for Fold01: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold02: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold03: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold04: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold05: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold06: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold07: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold08: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold09: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning: model fit failed for Fold10: C=0.0000 Error in .local(x, ...) : 
##   No Support Vectors found. You may want to change your parameters</code></pre>
<pre><code>## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): missing values found in
## aggregated results</code></pre>
<pre class="r"><code># Plot model accuracy vs different values of Cost
plot(model)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre class="r"><code># Print the best tuning parameter C that
# maximizes model accuracy
model$bestTune</code></pre>
<pre><code>##           C
## 2 0.1052632</code></pre>
</div>
