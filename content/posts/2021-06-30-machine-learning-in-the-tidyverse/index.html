---
title: Machine Learning in the Tidyverse
author: ''
date: '2021-06-30'
slug: []
categories: []
tags:
  - data_wrangling
  - Machine Learning
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="nesting-your-data" class="section level2">
<h2>1-2 Nesting your data</h2>
<p>In this course, you will work with a collection of economic and social indicators for 77 countries over a period of 52 years. This data is stored in the gapminder dataframe.</p>
<p>In this exercise, you will transform your gapminder data into a nested dataframe by using the first tool needed to build the foundation of tidy machine learning skills: nest().</p>
<p>Note: This is a more granular version than the dataset available from the gapminder package. This version is available in the dslabs package.</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.2     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>gapminder &lt;- read_csv(&quot;gapminder.csv&quot;)</code></pre>
<pre><code>## 
## -- Column specification --------------------------------------------------------
## cols(
##   country = col_character(),
##   year = col_double(),
##   infant_mortality = col_double(),
##   life_expectancy = col_double(),
##   fertility = col_double(),
##   population = col_double(),
##   gdpPercap = col_double()
## )</code></pre>
<pre class="r"><code># Explore gapminder
head(gapminder)</code></pre>
<pre><code>## # A tibble: 6 x 7
##   country  year infant_mortality life_expectancy fertility population gdpPercap
##   &lt;chr&gt;   &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;
## 1 Algeria  1960             148.            47.5      7.65   11124892      1242
## 2 Algeria  1961             148.            48.0      7.65   11404859      1047
## 3 Algeria  1962             148.            48.6      7.65   11690152       820
## 4 Algeria  1963             148.            49.1      7.65   11985130      1075
## 5 Algeria  1964             149.            49.6      7.65   12295973      1109
## 6 Algeria  1965             149.            50.1      7.66   12626953      1147</code></pre>
<pre class="r"><code># Prepare the nested dataframe gap_nested

gap_nested &lt;- gapminder %&gt;% 
  group_by(country) %&gt;% 
  nest()

# Explore gap_nested
head(gap_nested)</code></pre>
<pre><code>## # A tibble: 6 x 2
## # Groups:   country [6]
##   country    data             
##   &lt;chr&gt;      &lt;list&gt;           
## 1 Algeria    &lt;tibble [52 x 6]&gt;
## 2 Argentina  &lt;tibble [52 x 6]&gt;
## 3 Australia  &lt;tibble [52 x 6]&gt;
## 4 Austria    &lt;tibble [52 x 6]&gt;
## 5 Bangladesh &lt;tibble [52 x 6]&gt;
## 6 Belgium    &lt;tibble [52 x 6]&gt;</code></pre>
</div>
<div id="unnesting-your-data" class="section level2">
<h2>1-3 Unnesting your data</h2>
<p>As you’ve seen in the previous exercise, a nested dataframe is simply a way to shape your data. Essentially taking the group_by() windows and packaging them in corresponding rows.</p>
<p>In the same way you can use the nest() function to break your data into nested chunks, you can use the unnest() function to expand the dataframes that are nested in these chunks.</p>
<pre class="r"><code># Create the unnested dataframe called gap_unnnested
gap_unnested &lt;- gap_nested %&gt;% 
  unnest(col=data)
  
# Confirm that your data was not modified  
identical(gapminder, gap_unnested)</code></pre>
<pre><code>## [1] FALSE</code></pre>
</div>
<div id="explore-a-nested-cell" class="section level2">
<h2>1-4 Explore a nested cell</h2>
<p>In the first exercise, you successfully created a nested dataframe gap_nested. The data column contains tibbles for each country. In this exercise, you will explore one of these nested chunks.</p>
<pre class="r"><code># Extract the data of Algeria
algeria_df &lt;- gap_nested$data[[1]]

# Calculate the minimum of the population vector
min(algeria_df$population)</code></pre>
<pre><code>## [1] 11124892</code></pre>
<pre class="r"><code># Calculate the maximum of the population vector
max(algeria_df$population)</code></pre>
<pre><code>## [1] 36717132</code></pre>
<pre class="r"><code># Calculate the mean of the population vector
mean(algeria_df$population)</code></pre>
<pre><code>## [1] 23129438</code></pre>
</div>
<div id="mapping-your-data" class="section level2">
<h2>1-6 Mapping your data</h2>
<p>In combination with mutate(), you can use map() to append the results of your calculation to a dataframe. Since the map() function always returns a vector of lists you must use unnest() to extract this information into a numeric vector.</p>
<p>Here you will explore this functionality by calculating the mean population of each country in the gapminder dataset.</p>
<pre class="r"><code># Calculate the mean population for each country
pop_nested &lt;- gap_nested %&gt;%
  mutate(mean_pop = map(.x=data, .f=~mean(.x$population)))

# Take a look at pop_nested
head(pop_nested)</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   country [6]
##   country    data              mean_pop 
##   &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;   
## 1 Algeria    &lt;tibble [52 x 6]&gt; &lt;dbl [1]&gt;
## 2 Argentina  &lt;tibble [52 x 6]&gt; &lt;dbl [1]&gt;
## 3 Australia  &lt;tibble [52 x 6]&gt; &lt;dbl [1]&gt;
## 4 Austria    &lt;tibble [52 x 6]&gt; &lt;dbl [1]&gt;
## 5 Bangladesh &lt;tibble [52 x 6]&gt; &lt;dbl [1]&gt;
## 6 Belgium    &lt;tibble [52 x 6]&gt; &lt;dbl [1]&gt;</code></pre>
<pre class="r"><code># Extract the mean_pop value by using unnest
pop_mean &lt;- pop_nested %&gt;% 
  unnest(mean_pop)

# Take a look at pop_mean
head(pop_mean)</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   country [6]
##   country    data               mean_pop
##   &lt;chr&gt;      &lt;list&gt;                &lt;dbl&gt;
## 1 Algeria    &lt;tibble [52 x 6]&gt; 23129438.
## 2 Argentina  &lt;tibble [52 x 6]&gt; 30783053.
## 3 Australia  &lt;tibble [52 x 6]&gt; 16074837.
## 4 Austria    &lt;tibble [52 x 6]&gt;  7746272.
## 5 Bangladesh &lt;tibble [52 x 6]&gt; 97649407.
## 6 Belgium    &lt;tibble [52 x 6]&gt;  9983596.</code></pre>
</div>
<div id="expecting-mapped-output" class="section level2">
<h2>1-7Expecting mapped output</h2>
<p>When you know that the output of your mapped function is an expected type (here it is a numeric vector) you can leverage the map_*() family of functions to explicitly try to return that object type instead of a list.</p>
<p>Here you will again calculate the mean population of each country, but instead, you will use map_dbl() to explicitly append the numeric vector returned by mean() to your dataframe.</p>
<pre class="r"><code># Calculate mean population and store result as a double
pop_mean &lt;- gap_nested %&gt;%
  mutate(mean_pop = map_dbl(data, ~mean(.x$population)))

# Take a look at pop_mean
head(pop_mean)</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   country [6]
##   country    data               mean_pop
##   &lt;chr&gt;      &lt;list&gt;                &lt;dbl&gt;
## 1 Algeria    &lt;tibble [52 x 6]&gt; 23129438.
## 2 Argentina  &lt;tibble [52 x 6]&gt; 30783053.
## 3 Australia  &lt;tibble [52 x 6]&gt; 16074837.
## 4 Austria    &lt;tibble [52 x 6]&gt;  7746272.
## 5 Bangladesh &lt;tibble [52 x 6]&gt; 97649407.
## 6 Belgium    &lt;tibble [52 x 6]&gt;  9983596.</code></pre>
</div>
<div id="mapping-many-models" class="section level2">
<h2>1-8 Mapping many models</h2>
<p>The gap_nested dataframe available in your workspace contains the gapminder dataset nested by country.</p>
<p>You will use this data to build a linear model for each country to predict life expectancy using the year feature.</p>
<p>Note: The term feature is synonymous with the terms variable or predictor. It refers to an attribute of your data that can be used to build a machine learning model.</p>
<pre class="r"><code># Build a linear model for each country
gap_models &lt;- gap_nested %&gt;%
    mutate(model = map(data, ~lm(formula = life_expectancy~year, data = .x)))
    
# Extract the model for Algeria    
algeria_model &lt;- gap_models$model[[1]]

# View the summary for the Algeria model
summary(algeria_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = life_expectancy ~ year, data = .x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.044 -1.577 -0.543  1.700  3.843 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.197e+03  3.994e+01  -29.96   &lt;2e-16 ***
## year         6.349e-01  2.011e-02   31.56   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.177 on 50 degrees of freedom
## Multiple R-squared:  0.9522, Adjusted R-squared:  0.9513 
## F-statistic: 996.2 on 1 and 50 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="extracting-model-statistics-tidily" class="section level2">
<h2>1-11 Extracting model statistics tidily</h2>
<p>In this exercise, you will use the tidy() and glance() functions to extract information from algeria_model in a tidy manner.</p>
<p>For a linear model, tidy() extracts the model coefficients while glance() returns the model statistics such as the R^2.</p>
<pre class="r"><code>library(broom)

# Extract the coefficients of the algeria_model as a dataframe
tidy(algeria_model)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term         estimate std.error statistic  p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept) -1197.      39.9        -30.0 1.32e-33
## 2 year            0.635    0.0201      31.6 1.11e-34</code></pre>
<pre class="r"><code># Extract the statistics of the algeria_model as a dataframe
glance(algeria_model)</code></pre>
<pre><code>## # A tibble: 1 x 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.952         0.951  2.18      996. 1.11e-34     1  -113.  232.  238.
## # ... with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
</div>
<div id="augmenting-your-data" class="section level2">
<h2>1-12 Augmenting your data</h2>
<p>From the results of glance(), you learned that using the available features the linear model fits well with an adjusted of 0.99. The augment() function can help you explore this fit by appending the predictions to the original data.</p>
<p>Here you will leverage this to compare the predicted values of life_expectancy with the original ones based on the year feature.</p>
<pre class="r"><code># Build the augmented dataframe
algeria_fitted &lt;- augment(algeria_model)

# Compare the predicted values with the actual values of life expectancy
algeria_fitted %&gt;% 
  ggplot(aes(x = year)) +
  geom_point(aes(y = life_expectancy)) + 
  geom_line(aes(y = .fitted), color = &quot;red&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="tidy-up-the-coefficients-of-your-models" class="section level2">
<h2>2-2 Tidy up the coefficients of your models</h2>
<p>In this exercise you will leverage the list column workflow along with the tidy() function from broom to extract and explore the coefficients for the 77 models you built.</p>
<p>Remember the gap_models dataframe contains a model predicting life expectancy by year for 77 countries.</p>
<pre class="r"><code># Extract the coefficient statistics of each model into nested dataframes
model_coef_nested &lt;- gap_models %&gt;% 
    mutate(coef = map(model, ~tidy(.x)))
    
# Simplify the coef dataframes for each model    
model_coef &lt;- model_coef_nested %&gt;%
    unnest(coef)

# Plot a histogram of the coefficient estimates for year         
model_coef %&gt;% 
  filter(term == &quot;year&quot;) %&gt;% 
  ggplot(aes(x = estimate)) +
  geom_histogram()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="glance-at-the-fit-of-your-models" class="section level2">
<h2>2-5 Glance at the fit of your models</h2>
<p>In this exercise you will use glance() to calculate how well the linear models fit the data for each country.</p>
<pre class="r"><code># Extract the fit statistics of each model into dataframes
model_perf_nested &lt;- gap_models %&gt;% 
    mutate(fit = map(model, ~glance(.x)))

# Simplify the fit dataframes for each model    
model_perf &lt;- model_perf_nested %&gt;% 
    unnest(col=fit)
    
# Look at the first six rows of model_perf
head(model_perf)</code></pre>
<pre><code>## # A tibble: 6 x 15
## # Groups:   country [6]
##   country  data     model r.squared adj.r.squared sigma statistic  p.value    df
##   &lt;chr&gt;    &lt;list&gt;   &lt;lis&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1 Algeria  &lt;tibble~ &lt;lm&gt;      0.952         0.951 2.18       996. 1.11e-34     1
## 2 Argenti~ &lt;tibble~ &lt;lm&gt;      0.984         0.984 0.431     3137. 8.78e-47     1
## 3 Austral~ &lt;tibble~ &lt;lm&gt;      0.983         0.983 0.511     2905. 5.83e-46     1
## 4 Austria  &lt;tibble~ &lt;lm&gt;      0.987         0.986 0.438     3702. 1.48e-48     1
## 5 Banglad~ &lt;tibble~ &lt;lm&gt;      0.949         0.947 1.83       921. 7.10e-34     1
## 6 Belgium  &lt;tibble~ &lt;lm&gt;      0.990         0.990 0.331     5094. 5.54e-52     1
## # ... with 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;,
## #   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
</div>
<div id="best-and-worst-fitting-models" class="section level2">
<h2>2-6 Best and worst fitting models</h2>
<p>In this exercise you will answer the following questions:</p>
<p>Overall, how well do your models fit your data?
Which are the best fitting models?
Which models do not fit the data well?</p>
<pre class="r"><code># Plot a histogram of rsquared for the 77 models    
model_perf %&gt;% 
  ggplot(aes(x = r.squared)) + 
  geom_histogram(bins = 30)  </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code># Extract the 4 best fitting models
best_fit &lt;- model_perf %&gt;% 
  top_n(n = 4, wt = r.squared)

# Extract the 4 models with the worst fit
worst_fit &lt;- model_perf %&gt;% 
  top_n(n = 4, wt = -r.squared)</code></pre>
</div>
<div id="augment-the-fitted-values-of-each-model" class="section level2">
<h2>2-8 Augment the fitted values of each model</h2>
<p>In this exercise you will prepare your four best and worst fitting models for further exploration by augmenting your model data with augment().</p>
<pre class="r"><code>best_augmented &lt;- best_fit %&gt;% 
  # Build the augmented dataframe for each country model
  mutate(augmented = map(model, ~augment(.x))) %&gt;% 
  # Expand the augmented dataframes
  unnest(augmented)

worst_augmented &lt;- worst_fit %&gt;% 
  # Build the augmented dataframe for each country model
  mutate(augmented = map(model, ~augment(.x))) %&gt;% 
  # Expand the augmented dataframes
  unnest(augmented)</code></pre>
</div>
<div id="explore-your-best-and-worst-fitting-models" class="section level2">
<h2>2-9 Explore your best and worst fitting models</h2>
<p>Let’s explore your four best and worst fitting models by comparing the fitted lines with the actual values.</p>
<pre class="r"><code># Compare the predicted values with the actual values of life expectancy 
# for the top 4 best fitting models
best_augmented %&gt;% 
  ggplot(aes(x = year)) +
  geom_point(aes(y = life_expectancy)) + 
  geom_line(aes(y = .fitted), color = &quot;red&quot;) +
  facet_wrap(~country, scales = &quot;free_y&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code># Compare the predicted values with the actual values of life expectancy 
# for the top 4 worst fitting models
worst_augmented %&gt;% 
  ggplot(aes(x = year)) +
  geom_point(aes(y = life_expectancy)) + 
  geom_line(aes(y = .fitted), color = &quot;red&quot;) +
  facet_wrap(~country, scales = &quot;free_y&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="build-better-models" class="section level2">
<h2>2-11 Build better models</h2>
<p>Earlier you built a collection of simple models to fit life expectancy using the year feature. Your previous analysis showed that some of these models didn’t fit very well.</p>
<p>In this exercise you will build multiple regression models for each country using all available features. You may be interested in comparing the performance of the four worst fitting models so their adjusted are provided below:</p>
<p>Country Adjusted
Botswana -0.0060772
Lesotho -0.0169851
Zambia 0.1668999
Zimbabwe 0.2083979</p>
<pre class="r"><code># Build a linear model for each country using all features
gap_fullmodel &lt;- gap_nested %&gt;% 
  mutate(model = map(data, ~lm(life_expectancy~., data = .x)))

fullmodel_perf &lt;- gap_fullmodel %&gt;% 
  # Extract the fit statistics of each model into dataframes
  mutate(fit = map(model, ~glance(.x))) %&gt;% 
  # Simplify the fit dataframes for each model
  unnest(fit)
  
# View the performance for the four countries with the worst fitting 
# four simple models you looked at before
fullmodel_perf %&gt;% 
  filter(country %in% worst_fit$country) %&gt;% 
  select(country, adj.r.squared)</code></pre>
<pre><code>## # A tibble: 77 x 2
## # Groups:   country [77]
##    country    adj.r.squared
##    &lt;chr&gt;              &lt;dbl&gt;
##  1 Algeria            0.999
##  2 Argentina          0.999
##  3 Australia          0.994
##  4 Austria            0.996
##  5 Bangladesh         0.981
##  6 Belgium            0.996
##  7 Benin              0.996
##  8 Bolivia            0.999
##  9 Botswana           0.844
## 10 Brazil             0.999
## # ... with 67 more rows</code></pre>
</div>
<div id="the-test-train-split" class="section level2">
<h2>3-2 The test-train split</h2>
<p>In a disciplined machine learning workflow it is crucial to withhold a portion of your data (testing data) from any decision-making process. This allows you to independently assess the performance of your model when it is finalized. The remaining data, the training data, is used to build and select the best model.</p>
<p>In this exercise, you will use the rsample package to split your data to perform the initial train-test split of your gapminder data.</p>
<p>Note: Since this is a random split of the data it is good practice to set a seed before splitting it.</p>
<pre class="r"><code>library(rsample)
set.seed(42)

# Prepare the initial split object
gap_split &lt;- initial_split(gapminder, prop = 0.75)

# Extract the training dataframe
training_data &lt;- training(gap_split)

# Extract the testing dataframe
testing_data &lt;- testing(gap_split)

# Calculate the dimensions of both training_data and testing_data
dim(training_data)</code></pre>
<pre><code>## [1] 3003    7</code></pre>
<pre class="r"><code>dim(testing_data)</code></pre>
<pre><code>## [1] 1001    7</code></pre>
</div>
<div id="cross-validation-dataframes" class="section level2">
<h2>3-3 Cross-validation dataframes</h2>
<p>Now that you have withheld a portion of your data as testing data, you can use the remaining portion to find the best performing model.</p>
<p>In this exercise, you will split the training data into a series of 5 train-validate sets using the vfold_cv() function from the rsample package.</p>
<pre class="r"><code>set.seed(42)

# Prepare the dataframe containing the cross validation partitions
cv_split &lt;- vfold_cv(training_data, v = 5)

cv_data &lt;- cv_split %&gt;% 
  mutate(
    # Extract the train dataframe for each split
    train = map(splits, ~training(.x)), 
    # Extract the validate dataframe for each split
    validate = map(splits, ~testing(.x))
  )

# Use head() to preview cv_data
head(cv_data)</code></pre>
<pre><code>## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits             id    train                validate          
##   &lt;list&gt;             &lt;chr&gt; &lt;list&gt;               &lt;list&gt;            
## 1 &lt;split [2402/601]&gt; Fold1 &lt;tibble [2,402 x 7]&gt; &lt;tibble [601 x 7]&gt;
## 2 &lt;split [2402/601]&gt; Fold2 &lt;tibble [2,402 x 7]&gt; &lt;tibble [601 x 7]&gt;
## 3 &lt;split [2402/601]&gt; Fold3 &lt;tibble [2,402 x 7]&gt; &lt;tibble [601 x 7]&gt;
## 4 &lt;split [2403/600]&gt; Fold4 &lt;tibble [2,403 x 7]&gt; &lt;tibble [600 x 7]&gt;
## 5 &lt;split [2403/600]&gt; Fold5 &lt;tibble [2,403 x 7]&gt; &lt;tibble [600 x 7]&gt;</code></pre>
</div>
<div id="build-cross-validated-models" class="section level2">
<h2>3-5 Build cross-validated models</h2>
<p>In this exercise, you will build a linear model predicting life_expectancy using all available features. You will do this for the train data of each cross-validation fold.</p>
<pre class="r"><code># Build a model using the train data for each fold of the cross validation
cv_models_lm &lt;- cv_data %&gt;% 
  mutate(model = map(train, ~lm(formula = life_expectancy~., data = .x)))</code></pre>
</div>
<div id="preparing-for-evaluation" class="section level2">
<h2>3-6 Preparing for evaluation</h2>
<p>In order to measure the validate performance of your models you need compare the predicted values of life_expectancy for the observations from validate set to the actual values recorded. Here you will prepare both of these vectors for each partition.</p>
<pre class="r"><code>cv_prep_lm &lt;- cv_models_lm %&gt;% 
  mutate(
    # Extract the recorded life expectancy for the records in the validate dataframes
    validate_actual = map(validate, ~.x$life_expectancy),
    # Predict life expectancy for each validate set using its corresponding model
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))
  )</code></pre>
</div>
<div id="evaluate-model-performance" class="section level2">
<h2>3-7 Evaluate model performance</h2>
<p>Now that you have both the actual and predicted values of each fold you can compare them to measure performance.</p>
<p>For this regression model, you will measure the Mean Absolute Error (MAE) between these two vectors. This value tells you the average difference between the actual and predicted values.</p>
<pre class="r"><code>library(Metrics)
# Calculate the mean absolute error for each validate fold       
cv_eval_lm &lt;- cv_prep_lm %&gt;% 
  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))

# Print the validate_mae column
cv_eval_lm$validate_mae</code></pre>
<pre><code>## [1] 1.564897 1.481837 1.395386 1.589818 1.460490</code></pre>
<pre class="r"><code># Calculate the mean of validate_mae column
mean(cv_eval_lm$validate_mae)</code></pre>
<pre><code>## [1] 1.498485</code></pre>
</div>
<div id="build-a-random-forest-model" class="section level2">
<h2>3-9 Build a random forest model</h2>
<p>Here you will use the same cross-validation data to build (using train) and evaluate (using validate) random forests for each partition. Since you are using the same cross-validation partitions as your regression models, you are able to directly compare the performance of the two models.</p>
<p>Note: We will limit our random forests to contain 100 trees to ensure they finish fitting in a reasonable time. The default number of trees for ranger() is 500.</p>
<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>library(ranger)</td>
</tr>
<tr class="even">
<td># Build a random forest model for each fold
cv_models_rf &lt;- cv_data %&gt;%
mutate(model = map(train, ~ranger(formula = life_expectancy~., data = .x,
num.trees = 10, seed = 42)))</td>
</tr>
<tr class="odd">
<td># Generate predictions using the random forest model
cv_prep_rf &lt;- cv_models_rf %&gt;%
mutate(validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions))</td>
</tr>
</tbody>
</table>
</div>
<div id="evaluate-a-random-forest-model" class="section level2">
<h2>3-10 Evaluate a random forest model</h2>
<p>Similar to the linear regression model, you will use the MAE metric to evaluate the performance of the random forest model.</p>
<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>library(ranger)</td>
</tr>
<tr class="even">
<td># Calculate validate MAE for each fold
cv_eval_rf &lt;- cv_prep_rf %&gt;%
mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))</td>
</tr>
<tr class="odd">
<td># Print the validate_mae column
cv_eval_rf$validate_mae</td>
</tr>
<tr class="even">
<td># Calculate the mean of validate_mae column
mean(cv_eval_rf$validate_mae)</td>
</tr>
</tbody>
</table>
</div>
<div id="fine-tune-your-model" class="section level2">
<h2>3-11 Fine tune your model</h2>
<p>Wow! That was a significant improvement over a regression model. Now let’s see if you can further improve this performance by fine tuning your random forest models. To do this you will vary the mtry parameter when building your random forest models on your train data.</p>
<p>The default value of mtry for ranger is the rounded down square root of the total number of features (6). This results in a value of 2.</p>
<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td># Prepare for tuning your cross validation folds by varying mtry
cv_tune &lt;- cv_data %&gt;%
crossing(mtry = 2:5)</td>
</tr>
<tr class="even">
<td># Build a model for each fold &amp; mtry combination
cv_model_tunerf &lt;- cv_tune %&gt;%
mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = life_expectancy~.,
data = .x, mtry = .y,
num.trees = 100, seed = 42)))</td>
</tr>
</tbody>
</table>
</div>
<div id="the-best-performing-parameter" class="section level2">
<h2>3-12 The best performing parameter</h2>
<p>You’ve now built models where you’ve varied the random forest-specific hyperparameter mtry in the hopes of improving your model further. Now you will measure the performance of each mtry value across the 5 cross validation partitions to see if you can improve the model.</p>
<p>Remember that the validate MAE you calculated two exercises ago of 0.795 was for the default mtry value of 2.</p>
<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td># Generate validate predictions for each model
cv_prep_tunerf &lt;- cv_model_tunerf %&gt;%
mutate(validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions))</td>
</tr>
<tr class="even">
<td># Calculate validate MAE for each fold and mtry combination
cv_eval_tunerf &lt;- cv_prep_tunerf %&gt;%
mutate(validate_mae = map2_dbl(.x = validate_actual, .y = validate_predicted, ~mae(actual = .x, predicted = .y)))</td>
</tr>
<tr class="odd">
<td># Calculate the mean validate_mae for each mtry used
cv_eval_tunerf %&gt;%
group_by(mtry) %&gt;%
summarise(mean_mae = mean(validate_mae))</td>
</tr>
</tbody>
</table>
</div>
<div id="build-evaluate-the-best-model" class="section level2">
<h2>3-14 Build &amp; evaluate the best model</h2>
<p>Using cross-validation you were able to identify the best model for predicting life_expectancy using all the features in gapminder. Now that you’ve selected your model, you can use the independent set of data (testing_data) that you’ve held out to estimate the performance of this model on new data.</p>
<p>You will build this model using all training_data and evaluate using testing_data.</p>
<hr />
</div>
<div id="build-the-model-using-all-training-data-and-the-best-performing-parameter" class="section level1">
<h1>Build the model using all training data and the best performing parameter</h1>
<p>best_model &lt;- ranger(formula = life_expectancy~., data = training_data,
mtry = 4, num.trees = 100, seed = 42)</p>
</div>
<div id="prepare-the-test_actual-vector" class="section level1">
<h1>Prepare the test_actual vector</h1>
<p>test_actual &lt;- testing_data$life_expectancy</p>
</div>
<div id="predict-life_expectancy-for-the-testing_data" class="section level1">
<h1>Predict life_expectancy for the testing_data</h1>
<p>test_predicted &lt;- predict(best_model, testing_data)$predictions</p>
</div>
<div id="calculate-the-test-mae" class="section level1">
<h1>Calculate the test MAE</h1>
<p>mae(test_actual, test_predicted)</p>
<hr />
<div id="prepare-train-test-validate-parts" class="section level2">
<h2>4-2 Prepare train-test-validate parts</h2>
<p>In this exercise, you will leverage the tools that you have learned thus far to build a classification model to predict employee attrition.</p>
<p>You will work with the attrition dataset, which contains 30 features about employees which you will use to predict if they have left the company.</p>
<p>You will first prepare the training &amp; testing data sets, then you will further split the training data using cross-validation so that you can search for the best performing model for this task.</p>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre class="r"><code>set.seed(42)

# Prepare the initial split object
data_split &lt;- initial_split(attrition, prop = 0.75)

# Extract the training dataframe
training_data &lt;- training(data_split)

# Extract the testing dataframe
testing_data &lt;- testing(data_split)</code></pre>
<pre class="r"><code>set.seed(42)
cv_split &lt;- vfold_cv(training_data, v = 5)

cv_data &lt;- cv_split %&gt;% 
  mutate(
    # Extract the train dataframe for each split
    train = map(splits, ~training(.x)),
    # Extract the validate dataframe for each split
    validate = map(splits, ~testing(.x))
  )</code></pre>
</div>
<div id="build-cross-validated-models-1" class="section level2">
<h2>4-3 Build cross-validated models</h2>
<p>In this exercise, you will build logistic regression models for each fold in your cross-validation.</p>
<p>You will build this using the glm() function and by setting the family argument to “binomial”.</p>
<pre class="r"><code># Build a model using the train data for each fold of the cross validation
cv_models_lr &lt;- cv_data %&gt;% 
  mutate(model = map(train, ~glm(formula = factor(Attrition)~., 
                               data = .x, family = &quot;binomial&quot;)))</code></pre>
</div>
<div id="predictions-of-a-single-model" class="section level2">
<h2>4-5 Predictions of a single model</h2>
<p>To calculate the performance of a classification model you need to compare the actual values of Attrition to those predicted by the model. When calculating metrics for binary classification tasks (such as precision and recall), the actual and predicted vectors must be converted to binary values.</p>
<p>In this exercise, you will learn how to prepare these vectors using the model and validate dataframes from the first cross-validation fold as an example.</p>
<pre class="r"><code># Extract the first model and validate 
model &lt;- cv_models_lr$model[[1]]
validate &lt;- cv_models_lr$validate[[1]]

# Prepare binary vector of actual Attrition values in validate
validate_actual &lt;- validate$Attrition == &quot;Yes&quot;

# Predict the probabilities for the observations in validate
validate_prob &lt;- predict(model, validate, type = &quot;response&quot;)

# Prepare binary vector of predicted Attrition values for validate
validate_predicted &lt;- validate_prob &gt; 0.5</code></pre>
</div>
<div id="performance-of-a-single-model" class="section level2">
<h2>4-6 Performance of a single model</h2>
<p>Now that you have the binary vectors for the actual and predicted values of the model, you can calculate many commonly used binary classification metrics. In this exercise you will focus on:</p>
<p>accuracy: rate of correctly predicted values relative to all predictions.
precision: portion of predictions that the model correctly predicted as TRUE.
recall: portion of actual TRUE values that the model correctly recovered.</p>
<pre class="r"><code>library(Metrics)

# Compare the actual &amp; predicted performance visually using a table
table(validate_actual, validate_predicted)</code></pre>
<pre><code>##                validate_predicted
## validate_actual FALSE TRUE
##           FALSE   184    8
##           TRUE     14   15</code></pre>
<pre class="r"><code># Calculate the accuracy
accuracy(validate_actual, validate_predicted)</code></pre>
<pre><code>## [1] 0.9004525</code></pre>
<pre class="r"><code># Calculate the precision
precision(validate_actual, validate_predicted)</code></pre>
<pre><code>## [1] 0.6521739</code></pre>
<pre class="r"><code># Calculate the recall
recall(validate_actual, validate_predicted)</code></pre>
<pre><code>## [1] 0.5172414</code></pre>
</div>
<div id="prepare-for-cross-validated-performance" class="section level2">
<h2>4-7 Prepare for cross-validated performance</h2>
<p>Now that you know how to calculate the performance metrics for a single model, you are now ready to expand this for all the folds in the cross-validation dataframe.</p>
<pre class="r"><code>cv_prep_lr &lt;- cv_models_lr %&gt;% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(validate, ~.x$Attrition == &quot;Yes&quot;),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = &quot;response&quot;) &gt; 0.5)
  )</code></pre>
</div>
<div id="calculate-cross-validated-performance" class="section level2">
<h2>4-8 Calculate cross-validated performance</h2>
<p>It is crucial to optimize models using a carefully selected metric aimed at achieving the goal of the model.</p>
<p>Imagine that in this case you want to use this model to identify employees that are predicted to leave the company. Ideally, you want a model that can capture as many of the ready-to-leave employees as possible so that you can intervene. The corresponding metric that captures this is the recall metric. As such, you will exclusively use recall to optimize and select your models.</p>
<pre class="r"><code># Calculate the validate recall for each cross validation fold
cv_perf_recall &lt;- cv_prep_lr %&gt;% 
  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, 
                                    ~recall(actual = .x, predicted = .y)))

# Print the validate_recall column
cv_perf_recall$validate_recall</code></pre>
<pre><code>## [1] 0.5172414 0.3823529 0.4545455 0.4444444 0.4222222</code></pre>
<pre class="r"><code># Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)</code></pre>
<pre><code>## [1] 0.4441613</code></pre>
</div>
<div id="tune-random-forest-models" class="section level2">
<h2>4-10 Tune random forest models</h2>
<p>Now that you have a working logistic regression model you will prepare a random forest model to compare it with.</p>
<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>library(ranger)</td>
</tr>
<tr class="even">
<td># Prepare for tuning your cross validation folds by varying mtry
cv_tune &lt;- cv_data %&gt;%
crossing(mtry = c(2, 4, 8, 16))</td>
</tr>
<tr class="odd">
<td># Build a cross validation model for each fold &amp; mtry combination
cv_models_rf &lt;- cv_tune %&gt;%
mutate(model = map2(train, mtry, ~ranger(formula = factor(Attrition)~.,
data = .x, mtry = .y,
num.trees = 100, seed = 42)))</td>
</tr>
</tbody>
</table>
</div>
<div id="random-forest-performance" class="section level2">
<h2>4-11 Random forest performance</h2>
<p>It is now time to see whether the random forests models you built in the previous exercise are able to outperform the logistic regression model.</p>
<p>Remember that the validate recall for the logistic regression model was 0.43.</p>
<table style="width:6%;">
<colgroup>
<col width="5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>cv_prep_rf &lt;- cv_models_rf %&gt;%
mutate(
# Prepare binary vector of actual Attrition values in validate
validate_actual = map(validate, ~.x<span class="math inline">\(Attrition == &quot;Yes&quot;), # Prepare binary vector of predicted Attrition values for validate validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = &quot;response&quot;)\)</span>predictions == “Yes”)
)</td>
</tr>
<tr class="even">
<td># Calculate the validate recall for each cross validation fold
cv_perf_recall &lt;- cv_prep_rf %&gt;%
mutate(recall = map2_dbl(.x = validate_actual, .y = validate_predicted, ~recall(actual = .x, predicted = .y)))</td>
</tr>
<tr class="odd">
<td># Calculate the mean recall for each mtry used
cv_perf_recall %&gt;%
group_by(mtry) %&gt;%
summarise(mean_recall = mean(recall))</td>
</tr>
</tbody>
</table>
</div>
<div id="build-final-classification-model" class="section level2">
<h2>4-12 Build final classification model</h2>
<p>Comparing the recall performance between the logistic regression model (0.4) and the best performing random forest model (0.2), you’ve learned that the model with the best performance is the logistic regression model. In this exercise, you will build the logistic regression model using all of the train data and you will prepare the necessary vectors for evaluating this model’s test performance.</p>
<pre class="r"><code># Build the logistic regression model using all training data
best_model &lt;- glm(formula = factor(Attrition)~., 
                  data = training_data, family = &quot;binomial&quot;)


# Prepare binary vector of actual Attrition values for testing_data
test_actual &lt;- testing_data$Attrition == &quot;Yes&quot;

# Prepare binary vector of predicted Attrition values for testing_data
test_predicted &lt;- predict(best_model, testing_data, type = &quot;response&quot;) &gt; 0.5</code></pre>
</div>
<div id="measure-final-model-performance" class="section level2">
<h2>4-13 Measure final model performance</h2>
<p>Now its time to calculate the test performance of your final model (logistic regression). Here you will use the held out testing data to characterize the performance you would expect from this model when it is applied to new data.</p>
<pre class="r"><code>library(Metrics)

# Compare the actual &amp; predicted performance visually using a table
table(test_actual , test_predicted)</code></pre>
<pre><code>##            test_predicted
## test_actual FALSE TRUE
##       FALSE   301    7
##       TRUE     33   27</code></pre>
<pre class="r"><code># Calculate the test accuracy
accuracy(test_actual, test_predicted)</code></pre>
<pre><code>## [1] 0.8913043</code></pre>
<pre class="r"><code># Calculate the test precision
precision(test_actual, test_predicted)</code></pre>
<pre><code>## [1] 0.7941176</code></pre>
<pre class="r"><code># Calculate the test recall
recall(test_actual, test_predicted)</code></pre>
<pre><code>## [1] 0.45</code></pre>
</div>
</div>
