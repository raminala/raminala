---
title: 'Assignment A04: Tree based Method'
author: Ramin Ala
date: '2021-07-29'
slug: []
categories: []
tags:
  - Assignment
  - Tree-Based Models
---

***Ramin Ala***

## Introduction

Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification (tree "vote"), for that class. The forest chooses the classification having the most votes (over all the trees in the forest). Random decision forests correct for decision trees' habit of overfitting to their training set.

the random forest provides a strong improvement, which consists of applying bagging to the data and bootstrap sampling to the predictor variables at each split. This means that at each splitting step of the tree algorithm, a random sample of n predictors is chosen as split candidates from the full set of the predictors.

In this exercise, a random forest classification applies on the bank dataset to predict whether an unknown new customer will subscribe to an account or not. We are using "caret" package for modeling random forest. Dataset divided to 80-20 percents to train with 80% and test the acquired model with remaining 20% test data.

Variable importance analysis is also applied on this dataset to determine importance of variables and sort them in terms of importance.

This dataset consists of an imbalanced number of outputs, so that 90 percent of outcomes are "no" and 10 percent are "yes". This leads to severe bias of generated model towards "no". In fact, all inputs of test set resulted in "no". Although model's accuracy is good, it has a terrible specificity. To tackle this problem, a balanced model is built for this dataset  using "ROSE" package. This results in slighltly decrease in accuracy but big improvement is terms of specificity.

## libraries

```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(randomForest)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

bank <- read_csv("bank.csv") %>%
  select(c(age, job, marital, education, default, balance, housing, loan, y)) #%>%
  #mutate(subscription_status=ifelse(y == 'yes', 1, 0)) #%>%
  #subset(select=-y)
 
#bank %>% group_by(y) %>%  count()


```

## Splitting data


```{r}
set.seed(20)

# Split 80% of data as training set
training_bank <- bank$y %>% 
  createDataPartition(p = 0.8, list = FALSE)

bank_train  <- bank[training_bank, ]
bank_test <- bank[-training_bank, ]

#train.data %>% group_by(y) %>% count()
#test.data %>% group_by(y) %>% count()
```

## Computing random forest classifier


```{r}
set.seed(20)

# Fit the model on the training set
model <- train(
  y ~., data = bank_train, method = "rf",
  trControl = trainControl("cv", number = 3),
  importance = TRUE
  )

# Best tuning parameter
model$bestTune
```

```{r}
# Final model
model$finalModel
```

## Make predictions on the test data


```{r}
predicted.classes <- model %>% predict(bank_test)

```


## binding predictions and truth 

```{r, warning=FALSE}


bank_bind <- bank_test %>%
   bind_cols(.pred=predicted.classes)


test_pred_mat <- bank_bind %>% conf_mat(truth = y, estimate = .pred)
summary(test_pred_mat)
autoplot(test_pred_mat, type = 'heatmap')
```


## Variable importance

```{r}
importance(model$finalModel)
```

MeanDecreaseAccuracy: A measure of the extent to which a variable improves the accuracy of the forest in predicting the classification. Higher values mean that the variable improves prediction.

(MeanDecreaseGini): Provides a more nuanced measure of importance, which factors in both the contribution that variable makes to accuracy, and the degree of misclassification (e.g., if a variable improves the probability of an observation being classified to a segment from 55% to 90%, this will show up in the Importance (MeanDecreaseGini), but not in MeanDecreaseAccuracy). As with MeanDecreaseAccuracy, high numbers indicate that a variable is more important as a predictor.

```{r}
# Plot MeanDecreaseAccuracy
varImpPlot(model$finalModel, type = 1)
# Plot MeanDecreaseGini
varImpPlot(model$finalModel, type = 2)
```

The results show that across all of the trees considered in the random forest, the glucose and age variables are the two most important variables.

```{r, message=FALSE}
# the importance of variables in percentage

varImp(model)
```

## imbalanced dataset to balanced one

 
```{r, message=FALSE}
library(ROSE)

bank_train_balanced <- ovun.sample(y ~ ., data = bank_train, method = "over",N = 6500)$data

bank_train_balanced %>%
  group_by(y) %>%
  count()
```


## Computing random forest classifier for balanced dataset


```{r}
set.seed(20)

# Fit the model on the training set
model2 <- train(
  y ~., data = bank_train_balanced, method = "rf",
  trControl = trainControl("cv", number = 3),
  importance = TRUE
  )

# Best tuning parameter
model2$bestTune
```

## Make predictions on the test data using new model 


```{r}
predicted.classes2 <- model2 %>% predict(bank_test)

```



## binding predictions and truth for new data

```{r, warning=FALSE}


bank_bind2 <- bank_test %>%
   bind_cols(.pred=predicted.classes2)


test_pred_mat2 <- bank_bind2 %>% conf_mat(truth = y, estimate = .pred)
summary(test_pred_mat2)
autoplot(test_pred_mat2, type = 'heatmap')
```