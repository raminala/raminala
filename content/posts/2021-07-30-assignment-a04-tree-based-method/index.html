---
title: 'Assignment A04: Tree based Method'
author: Ramin Ala
date: '2021-07-29'
slug: []
categories: []
tags:
  - Assignment
  - Tree-Based Models
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><strong><em>Ramin Ala</em></strong></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Random decision forests correct for decision treesâ€™ habit of overfitting to their training set.</p>
<p>Random Forest algorithm is a special type of bagging applied to decision trees.</p>
<p>the random forest provides a strong improvement, which consists of applying bagging to the data and bootstrap sampling to the predictor variables at each split. This means that at each splitting step of the tree algorithm, a random sample of n predictors is chosen as split candidates from the full set of the predictors.</p>
</div>
<div id="libraries" class="section level2">
<h2>libraries</h2>
<pre class="r"><code>library(tidyverse)
library(tidymodels)
library(caret)
library(randomForest)</code></pre>
</div>
<div id="splitting-data" class="section level2">
<h2>Splitting data</h2>
<pre class="r"><code>set.seed(20)

# Split 80% of data as training set
training_bank &lt;- bank$y %&gt;% 
  createDataPartition(p = 0.8, list = FALSE)

bank_train  &lt;- bank[training_bank, ]
bank_test &lt;- bank[-training_bank, ]

#train.data %&gt;% group_by(y) %&gt;% count()
#test.data %&gt;% group_by(y) %&gt;% count()</code></pre>
</div>
<div id="computing-random-forest-classifier" class="section level2">
<h2>Computing random forest classifier</h2>
<pre class="r"><code>set.seed(20)

# Fit the model on the training set
model &lt;- train(
  y ~., data = bank_train, method = &quot;rf&quot;,
  trControl = trainControl(&quot;cv&quot;, number = 3),
  importance = TRUE
  )

# Best tuning parameter
model$bestTune</code></pre>
<pre><code>##   mtry
## 1    2</code></pre>
<pre class="r"><code># Final model
model$finalModel</code></pre>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, mtry = min(param$mtry, ncol(x)), importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 11.53%
## Confusion matrix:
##       no yes class.error
## no  3200   0           0
## yes  417   0           1</code></pre>
</div>
<div id="make-predictions-on-the-test-data" class="section level2">
<h2>Make predictions on the test data</h2>
<pre class="r"><code>predicted.classes &lt;- model %&gt;% predict(bank_test)</code></pre>
</div>
<div id="binding-predictions-and-truth" class="section level2">
<h2>binding predictions and truth</h2>
<pre class="r"><code>bank_bind &lt;- bank_test %&gt;%
   bind_cols(.pred=predicted.classes)


test_pred_mat &lt;- bank_bind %&gt;% conf_mat(truth = y, estimate = .pred)
summary(test_pred_mat)</code></pre>
<pre><code>## # A tibble: 13 x 3
##    .metric              .estimator .estimate
##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
##  1 accuracy             binary         0.885
##  2 kap                  binary         0    
##  3 sens                 binary         1    
##  4 spec                 binary         0    
##  5 ppv                  binary         0.885
##  6 npv                  binary       NaN    
##  7 mcc                  binary        NA    
##  8 j_index              binary         0    
##  9 bal_accuracy         binary         0.5  
## 10 detection_prevalence binary         1    
## 11 precision            binary         0.885
## 12 recall               binary         1    
## 13 f_meas               binary         0.939</code></pre>
<pre class="r"><code>autoplot(test_pred_mat, type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="variable-importance" class="section level2">
<h2>Variable importance</h2>
<pre class="r"><code>importance(model$finalModel)</code></pre>
<pre><code>##                            no        yes MeanDecreaseAccuracy MeanDecreaseGini
## age                 8.4153642  7.4180405          11.69687461       22.8356872
## jobblue-collar      2.8847131  2.4836213           3.87898042        2.8803708
## jobentrepreneur     2.6304393 -2.2411580           1.54544883        1.1872712
## jobhousemaid       -0.8212681 -0.7072955          -1.06143558        1.3717751
## jobmanagement       4.0224764 -1.0486284           4.19691298        1.5571955
## jobretired          2.6225767  0.9829453           4.39976552        3.6838437
## jobself-employed   -2.5383420 -1.8945317          -3.16846014        1.1897875
## jobservices         3.4068470 -2.7661859           2.65044742        1.2058057
## jobstudent         -4.2486142  2.1823473          -3.89460469        1.1854107
## jobtechnician       2.8719788 -0.9910160           2.39469234        1.6720720
## jobunemployed      -0.7229227  1.8852340          -0.09628516        0.8025047
## jobunknown          4.4756569 -1.7232319           4.22397658        1.4089918
## maritalmarried      8.4673906 -2.5447151           7.87233098        3.1610415
## maritalsingle       7.7193098 -4.7866469           7.32128381        2.2396345
## educationsecondary  5.9846482 -3.3835465           5.66087363        2.0483593
## educationtertiary   7.1697719 -6.8501425           6.14873540        2.4072047
## educationunknown    4.3862899 -1.9276462           3.62608026        1.2457435
## defaultyes          4.6802991 -3.0952156           4.03327125        1.4271119
## balance             3.2825289  4.3461612           4.77923212       21.3537650
## housingyes          1.6872296  0.1202470           1.83391730        5.6255633
## loanyes            -3.3086774  3.2340984          -2.09696698        3.5048515</code></pre>
<p>MeanDecreaseAccuracy: A measure of the extent to which a variable improves the accuracy of the forest in predicting the classification. Higher values mean that the variable improves prediction.</p>
<p>(MeanDecreaseGini): Provides a more nuanced measure of importance, which factors in both the contribution that variable makes to accuracy, and the degree of misclassification (e.g., if a variable improves the probability of an observation being classified to a segment from 55% to 90%, this will show up in the Importance (MeanDecreaseGini), but not in MeanDecreaseAccuracy). As with MeanDecreaseAccuracy, high numbers indicate that a variable is more important as a predictor.</p>
<pre class="r"><code># Plot MeanDecreaseAccuracy
varImpPlot(model$finalModel, type = 1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code># Plot MeanDecreaseGini
varImpPlot(model$finalModel, type = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<p>The results show that across all of the trees considered in the random forest, the glucose and age variables are the two most important variables.</p>
<pre class="r"><code># the importance of variables in percentage

varImp(model)</code></pre>
<pre><code>## rf variable importance
## 
##   only 20 most important variables shown (out of 21)
## 
##                    Importance
## age                    100.00
## balance                 59.52
## maritalmarried          51.10
## jobblue-collar          48.36
## jobretired              39.66
## jobmanagement           36.55
## maritalsingle           36.34
## jobunknown              35.45
## educationsecondary      34.71
## educationunknown        34.00
## jobtechnician           31.15
## housingyes              30.79
## defaultyes              29.69
## jobunemployed           27.61
## jobservices             25.03
## jobentrepreneur         23.79
## educationtertiary       23.45
## loanyes                 21.51
## jobhousemaid            14.33
## jobstudent              11.68</code></pre>
</div>
<div id="imbalanced-dataset-to-balanced-one" class="section level2">
<h2>imbalanced dataset to balanced one</h2>
<pre class="r"><code>library(ROSE)

bank_train_balanced &lt;- ovun.sample(y ~ ., data = bank_train, method = &quot;over&quot;,N = 6500)$data

bank_train_balanced %&gt;%
  group_by(y) %&gt;%
  count()</code></pre>
<pre><code>## # A tibble: 2 x 2
## # Groups:   y [2]
##   y         n
##   &lt;chr&gt; &lt;int&gt;
## 1 no     3200
## 2 yes    3300</code></pre>
</div>
<div id="computing-random-forest-classifier-for-balanced-dataset" class="section level2">
<h2>Computing random forest classifier for balanced dataset</h2>
<pre class="r"><code>set.seed(20)

# Fit the model on the training set
model2 &lt;- train(
  y ~., data = bank_train_balanced, method = &quot;rf&quot;,
  trControl = trainControl(&quot;cv&quot;, number = 3),
  importance = TRUE
  )

# Best tuning parameter
model2$bestTune</code></pre>
<pre><code>##   mtry
## 2   11</code></pre>
</div>
<div id="make-predictions-on-the-test-data-using-new-model" class="section level2">
<h2>Make predictions on the test data using new model</h2>
<pre class="r"><code>predicted.classes2 &lt;- model2 %&gt;% predict(bank_test)</code></pre>
</div>
<div id="binding-predictions-and-truth-for-new-data" class="section level2">
<h2>binding predictions and truth for new data</h2>
<pre class="r"><code>bank_bind2 &lt;- bank_test %&gt;%
   bind_cols(.pred=predicted.classes2)


test_pred_mat2 &lt;- bank_bind2 %&gt;% conf_mat(truth = y, estimate = .pred)
summary(test_pred_mat2)</code></pre>
<pre><code>## # A tibble: 13 x 3
##    .metric              .estimator .estimate
##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
##  1 accuracy             binary         0.840
##  2 kap                  binary         0.128
##  3 sens                 binary         0.924
##  4 spec                 binary         0.192
##  5 ppv                  binary         0.898
##  6 npv                  binary         0.247
##  7 mcc                  binary         0.130
##  8 j_index              binary         0.116
##  9 bal_accuracy         binary         0.558
## 10 detection_prevalence binary         0.910
## 11 precision            binary         0.898
## 12 recall               binary         0.924
## 13 f_meas               binary         0.911</code></pre>
<pre class="r"><code>autoplot(test_pred_mat2, type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
