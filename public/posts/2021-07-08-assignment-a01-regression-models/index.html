<!doctype html>
<html lang="en-us">
  <head>
    <title>Assignment A01: Regression Models // Ramin Ala</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.83.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="John Doe" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://ramin1ala.netlify.app/css/main.min.9dbf51e978afe3857b6cf4f89e41949fcd0410a29b2c2a5da404b58dfd6fc228.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Assignment A01: Regression Models"/>
<meta name="twitter:description" content="Ramin Ala
Executive SummaryThis assignment is about deciding, or in a better term predicting, the price of a diamond based on available data. Since prediction would be done on a quantitative variable (price), it considers as a regression problem. Here, we will solve the problem using linear fitting to avoid over fitting.
There are a dizzying array of diamond characteristics, configurations, and pricing such as carat, color, Cut, clarity, Polish, Symmetry, and certification, and data collected from three different wholesaler websites."/>

    <meta property="og:title" content="Assignment A01: Regression Models" />
<meta property="og:description" content="Ramin Ala
Executive SummaryThis assignment is about deciding, or in a better term predicting, the price of a diamond based on available data. Since prediction would be done on a quantitative variable (price), it considers as a regression problem. Here, we will solve the problem using linear fitting to avoid over fitting.
There are a dizzying array of diamond characteristics, configurations, and pricing such as carat, color, Cut, clarity, Polish, Symmetry, and certification, and data collected from three different wholesaler websites." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-08T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-07-08T00:00:00&#43;00:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://ramin1ala.netlify.app/"><img class="app-header-avatar" src="/avatar.jpg" alt="John Doe" /></a>
      <h1>Ramin Ala</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
            ~
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
            ~
          
          <a class="app-header-menu-item" href="/about/">About</a>
            ~
          
          <a class="app-header-menu-item" href="/publications/">Publications</a>
      </nav>
      <p>An Electrical engineer with experience in RF and interest in data science.</p>
      <div class="app-header-social">
        
          <a href="https://github.com/raminala" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="https://www.linkedin.com/in/ramin-ala-6645b3188/" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>Linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg>
          </a>
        
          <a href="https://twitter.com/ala_ramin" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>Twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Assignment A01: Regression Models</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jul 8, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          13 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://ramin1ala.netlify.app/tags/regression/">Regression</a>
              <a class="tag" href="https://ramin1ala.netlify.app/tags/resampling/">Resampling</a>
              <a class="tag" href="https://ramin1ala.netlify.app/tags/assignment/">Assignment</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      
<script src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/header-attrs/header-attrs.js"></script>


<p><strong><em>Ramin Ala</em></strong></p>
<hr />
<div id="executive-summary" class="section level2">
<h2>Executive Summary</h2>
<p>This assignment is about deciding, or in a better term predicting, the price of a diamond based on available data. Since prediction would be done on a quantitative variable (price), it considers as a <em>regression problem</em>. Here, we will solve the problem using <em>linear fitting</em> to avoid over fitting.</p>
<p>There are a dizzying array of diamond characteristics, configurations, and pricing such as carat, color, Cut, clarity, Polish, Symmetry, and certification, and data collected from three different wholesaler websites. The first step would be understanding data via visualization and then going to making a linear model and prediction based on that.</p>
</div>
<div id="univariate-analysis" class="section level2">
<h2>Univariate Analysis</h2>
<p>Univariate analysis is perhaps the simplest form of statistical analysis. The key fact is that only one variable is involved. Like other forms of statistics, it can be inferential or descriptive.</p>
<p>Although univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate, it provides some insights.</p>
<p><strong>Univariate Analysis on price</strong></p>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Apparently, there are two distinct price ranges. below $500 and above $1800. With univariate analysis, we could not understand the reasons and decisive parameters for this gap in the price. However, one might decide to model these two sections separately for more exact model. This might be the most important finding by this analysis.</p>
<p>Now it worth to take a more statistical look at price variable.</p>
<pre><code>## # A tibble: 1 x 6
##   minimum    q1 median  mean    q3 maximum
##     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1     160   520   2169 1717. 3012.    3145</code></pre>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>First point that a reader could get from these data is that maximum price ($3145) is very close to the offered price ($3100). It means that the professor might be offered the best diamond in the market. On the other hand, mean of price ($1716) is far from offered price (again, he might expect an exceptional item!).</p>
<p>The box plot summarize these parameters in a visualization friendly manner, one look provides
minimum, mode, median, max, first and third quarterlies. Note that the red dotted line is the offered price that laied at the very end of box plot.</p>
</div>
<div id="bivariate-analysis" class="section level2">
<h2>Bivariate Analysis</h2>
<p>Bivariate analysis is a simple forms of quantitative (statistical) analysis, however more sophesticated than the univariable analysis which done in the previous part. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them.</p>
<p><strong>Metric Data: Price, Carat</strong></p>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Carat is the most important parameter in this dataset and it is a determining factor for the price. This claim can be proven from its coefficient in the multiple linear regression model. This figure shows price versus carat along with a regression line. This analysis adds this knowledge to our understanding that price, possibly, has a positive relation with the carat. The word “possibly” added intentionally here because in a multi-variable dataset with existence of correlation between variables, sometime another variable is the reason for increase in target variable. It will be more obvious when all variables considered together.</p>
</div>
<div id="non-metric-data-color-cut-certification-polish-symmetry-clarity" class="section level2">
<h2>Non-Metric Data: Color, Cut, Certification, Polish, Symmetry, Clarity</h2>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>This plot shows the mean of price for nine categories of colors. Although these prices might be the indirect cause of other parameters but gives some clues. It seems that color has not a profound impact on price. Two colour catagories could be recognized from this plot, one with average price of $2500 (D, G, K, and L) and another with the average price of $1300 (E, F, H, J). Professor should be more careful on this as his diamond looks in the lower price category in terms of color.</p>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Importance of clarity might be guesses from this plot. In contrast to color, there is big change in price average with selecting different clarity. I1, I2, Si1, and SI3 are expensive categories and professor selected his diamond from these categories.</p>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>This plot mainly dirsct buyer to select from F category if he is looking for a perfect cut and from V if don’t want to break the bank.</p>
<p>Bar plot is a easy way to visualize categorical variable. The mean of price versus colour, clarity and cut is depicted here.</p>
</div>
<div id="analysis-of-three-variables-price-considering-both-carat-and-clarity" class="section level2">
<h2>Analysis of three variables: price considering both carat and clarity</h2>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Clarity is the second important parameter. To get the impression about effect of these two parameters on price, price versus carat with the additional layer of color for clarity is useful. Apparently, there is a big gap versus high and low carat items and diamond with both every color is available at both ends of price (below $500 and more than $2000).</p>
</div>
<div id="four-variaables-analysis-price-considering-carat-colour-and-clarity" class="section level2">
<h2>four variaables Analysis: price considering carat, colour, and clarity</h2>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Zoom in higher price section helps to add more layers without loosing readability. This plot shows price versus carat, colour, and clarity. Again, it emphasize the importance of clarity even for lower carat diamonds.</p>
</div>
<div id="correlation-covariance" class="section level2">
<h2>Correlation, Covariance</h2>
<p>These are measures of association between variables.</p>
<pre><code>## [1] &quot;Correlation between price-carat:  0.925436039946847&quot;</code></pre>
<p>This shows the expected outcome, higher carat mean higher price (, same trand for increase-decrease).</p>
<pre><code>## [1] &quot;Covariance between price-carat:  413.23180808656&quot;</code></pre>
<p>As expected!</p>
<hr />
</div>
<div id="regression-model" class="section level2">
<h2>Regression Model</h2>
</div>
<div id="simple-linear-regression" class="section level2">
<h2>Simple Linear Regression</h2>
<p>Here, a linear model is generated using <strong>lm()</strong> function to attribute price with carat. Coefficients are as below table.</p>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    -200.      43.1     -4.65 4.40e-  6
## 2 carat          2865.      56.0     51.1  1.02e-186</code></pre>
<pre><code>## 
## Call:
## lm(formula = price ~ carat, data = diamonds)
## 
## Coefficients:
## (Intercept)        carat  
##      -200.5       2864.7</code></pre>
</div>
<div id="simple-linear-regression--prediction-carat-0.9" class="section level2">
<h2>Simple Linear Regression- prediction, carat= 0.9</h2>
<p>The goal is predicting the price based only on carat. To this end, generated model in the previous section is employed to predict price using <strong>predict()</strong> function. Expected price (with only consideration of carat=0.9) is as below:</p>
<pre><code>##        1 
## 2377.776</code></pre>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple Linear Regression</h2>
<p>A linear model is generated using <strong>lm()</strong> function. Seven variables passes to the model and intercept defined as zero (for simplicity). As result, coefficients calculated as below table.</p>
<pre><code>## # A tibble: 34 x 5
##    term    estimate std.error statistic   p.value
##    &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 carat      4205.      51.3     82.0  1.02e-254
##  2 colourD   -1433.     147.      -9.77 2.25e- 20
##  3 colourE   -1624.     144.     -11.3  6.40e- 26
##  4 colourF   -1744.     140.     -12.5  2.13e- 30
##  5 colourG   -1734.     143.     -12.1  5.96e- 29
##  6 colourH   -1863.     141.     -13.2  2.26e- 33
##  7 colourI   -1934.     142.     -13.6  6.09e- 35
##  8 colourJ   -2068.     146.     -14.2  2.44e- 37
##  9 colourK   -2419.     151.     -16.1  2.52e- 45
## 10 colourL   -2606.     156.     -16.7  5.37e- 48
## # ... with 24 more rows</code></pre>
</div>
<div id="multiple-linear-regression--prediction" class="section level2">
<h2>Multiple Linear Regression- prediction</h2>
<p>Prediction is done here base on below parameters:</p>
<p>carat= 0.9, colour= “J”, clarity= “SI2”, cut= “V”, certification= “GIA”, polish= “G”, symmetry= “V”</p>
<pre><code>##        1 
## 2797.542</code></pre>
</div>
<div id="summary-of-findings" class="section level2">
<h2>Summary of findings</h2>
<ul>
<li><p>From the exploratory data analysis and linear regression performed in this report, it seems that the offered price ($3100) is 10% higer than the predicted price ($2800).</p></li>
<li><p>With the offered price, he could buy the same diamond with the clarity of VS1 (few inclusions at 30X) instead of SI2 (very few inclusions at 10X).</p></li>
<li><p>From seven explanatory variables, carat is the first determinant followed by clarity and colour. On the other hand, cut has the least effect on the final price.</p></li>
<li><p>A single variable regression (price versus carat) shows the offered price is very high for selected carat. However, the clarity of the selected diamond compensate part of its expensive price (although not all of that).</p></li>
<li><p>The important trend in the dataset could be found with a glance at price-carat plot. That is carat has two ranges: 0.1-.03 and 0.8- 1.6. There are a substantial gap between price of these two catagories, the latter is four times more costly.</p></li>
<li><p>Dataset was very clean dataset with nearly no need for cleaning. For example there were not any NA and outlier data in the dataset.</p></li>
</ul>
<hr />
</div>
<div id="resampling" class="section level2">
<h2>Resampling</h2>
<pre class="r"><code>library(caret)

library(yardstick)

library(tictoc)</code></pre>
<p><strong>Data sampling versus resampling:</strong> Data sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter. Whereas data resampling refers to methods for economically using a collected dataset to improve the estimate of the population parameter and help to quantify the uncertainty of the estimate. In this part we are studying resampling. Two methods are considereing in the following paragraphs: cross validation (two sub-categories, LOOCV, K-fold) and bootstraping.</p>
<p>for each category resampling is conducted using <strong><em>caret</em></strong> package and their performance is evaluated and compared using appropriate methods.</p>
<p>In the following paragraphs three models will calculate using resampling: LOOCV, K-fold, and Bootstrap. subsequently, we will compare performance of these three models.</p>
</div>
<div id="resampling-methods" class="section level2">
<h2>Resampling methods</h2>
</div>
<div id="cross-validation-methods" class="section level2">
<h2><strong><em>Cross-Validation methods</em></strong></h2>
</div>
<div id="loocv-leave-one-out-cross-validation" class="section level2">
<h2>LOOCV (Leave one out cross validation)</h2>
<p>The LOOCV method is using all data points (excludes only one observation each time). There are advantages and disadvantages for this method:</p>
<p><strong>Advantage:</strong> Because of using <em>n-1</em> observations in each iteration, it reduces the bias (because it will not bias to selected portion).</p>
<p><strong>Disadvantages:</strong> The execution time could be so long if n (number of observation) is large. Fortunately, we have 450 observations in <em>diamond</em> dataset and my machine with 16GB of RAM easily executed this. Besides, we know that low bias means high variance and the method is more prone to overfitting.</p>
<p>Another disadvantage of this method is that tests the model performance against <em>one data point</em> at each iteration.This might result to higher variation in the prediction error, especially if some data points are outliers.</p>
<p>Divide the dataset to two sections (train and test) could show the overfitting, however, Here, we will not practice that.</p>
<p>We are using tictoc library to measure the running time for each code chunk to evaluate performance of these methods in terms of running time.</p>
<p>Below is code for performing LOOCV using caret library.</p>
<hr />
<pre class="r"><code>start_time_LOOCV1 &lt;- Sys.time()
# Define training control
train.control_LOOCV &lt;- trainControl(method = &quot;LOOCV&quot;)

# Train the model
model_LOOCV &lt;- train(price ~., data = diamonds, method = &quot;lm&quot;,
               trControl = train.control_LOOCV)
# Summarize the results
print(model_LOOCV)</code></pre>
<pre><code>## Linear Regression 
## 
## 440 samples
##   7 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 439, 439, 439, 439, 439, 439, ... 
## Resampling results:
## 
##   RMSE     Rsquared   MAE     
##   212.186  0.9673791  150.5193
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<pre class="r"><code>end_time_LOOCV1 &lt;- Sys.time()

LOOCV_run_time1= end_time_LOOCV1 - start_time_LOOCV1

#Print run time
print(LOOCV_run_time1)</code></pre>
<pre><code>## Time difference of 4.041427 secs</code></pre>
<p>As expected, sample sizes are 439 (440-1) and process repeated 440 times (n). Three important parameters are provided to evaluate performance.</p>
<p>RMSE (Root Mean Squared Error), which measures the average prediction errors made by the model in predicting the outcome for an observation. We expect low value for RMSE. Note that an outlier observation could drastically increase this error. The achieved value in this part will be compared by this values in other methods to compare these methods.</p>
<p>To tackale the problem of outlier in RMSE, MAE (Mean Absolute Error) is introduced. MAE is an alternative to the RMSE that is less sensitive to outliers. It corresponds to the average absolute difference between observed and predicted outcomes. again, the lower the MAE, the better the model. We compare this number for different models in following parts.</p>
<p>Rsquared or R2 is familiar parameter of squared correlation between the observed outcome values and the predicted values by the model. The closer to 1 the adjusted R2, the better the model. here we achieved 0.9673791.</p>
<p>This chunk of code took about 4.2 seconds to complete.</p>
</div>
<div id="k-fold-cross-validation" class="section level2">
<h2>K-fold cross-validation</h2>
<p>K-fold evaluates the model performance on different subset (there are k subsets) of the training data and then calculate the average prediction error rate.The obvious advantage of k-fold over LOOCV is K-fold is faster (needs less computation). A less obvious but potentially more important advantage of k-fold cross validation method is that it often gives more accurate estimates of the test error rate than does LOOCV.</p>
<p>There is a trade-off in selecting k (number of subsets):</p>
<p><strong><em>small K:</em></strong> is more biased (undesirable).</p>
<p><strong><em>large K:</em></strong> large variance (undesirable).</p>
<p>Here, we consider <em>k=5</em> for 440 observations to build a 5-fold model.</p>
<pre class="r"><code># Define pseudorandom seed number
set.seed(190) 
start_time_K_fold1 &lt;- Sys.time()

train.control_K_fold &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 20)

# Train the model
model_K_fold &lt;- train(price ~., data = diamonds, method = &quot;lm&quot;,
               trControl = train.control_K_fold)
# Summarize the results
print(model_K_fold)</code></pre>
<pre><code>## Linear Regression 
## 
## 440 samples
##   7 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 20 times) 
## Summary of sample sizes: 352, 353, 352, 351, 352, 351, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   214.6123  0.9670286  152.2284
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<pre class="r"><code>end_time_K_fold1 &lt;- Sys.time()

K_fold_run_time1= end_time_K_fold1 - start_time_K_fold1

#Print run time
print(K_fold_run_time1)</code></pre>
<pre><code>## Time difference of 1.207991 secs</code></pre>
<p>In this method, subsets are selected in a pseudorandom process. So we could reproduce them by setting seed number. We repeated the process 20 times for a better estimation (by setting repeat argument).</p>
<p>As it can be seen from parameters, RMSE, Rsquared, and MAE are very close to those of LOOCV model. However, calculation performed faster.</p>
<p>It takes 1.6 seconds to complete running of K-fold cross-validation method.</p>
</div>
<div id="bootstrap-method" class="section level2">
<h2>Bootstrap method</h2>
<p>Bootstrap resampling consists of repeatedly selecting (<strong><em>with Replacement</em></strong>) a sample of n observations from the original data set (dataset consists of n observations), and to evaluate the model on each copy. An average standard error is then calculated and the results provide an indication of the overall variance of the model performance.</p>
<p>Here, we consider <em>number =100</em>. It means that process of selecting (n=440) samplaes with replacement repeated 100 times.</p>
<pre class="r"><code>start_time_bootstrap1 &lt;- Sys.time()
# Define training control
train.control_Bootstrap &lt;- trainControl(method = &quot;boot&quot;, number =100)

# Train the model
model_Bootstrap &lt;- train(price ~., data = diamonds, method = &quot;lm&quot;,
               trControl = train.control_Bootstrap)
# Summarize the results
print(model_Bootstrap)</code></pre>
<pre><code>## Linear Regression 
## 
## 440 samples
##   7 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (100 reps) 
## Summary of sample sizes: 440, 440, 440, 440, 440, 440, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   229.1964  0.9626821  159.7997
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<pre class="r"><code>end_time_bootstrap1 &lt;- Sys.time()

bootstrap_run_time1= end_time_bootstrap1 - start_time_bootstrap1

#Print run time
print(bootstrap_run_time1)</code></pre>
<pre><code>## Time difference of 1.252951 secs</code></pre>
<p>Again, model parameters (RMSE, Rsquared, and MAE) are close to previous methods.</p>
<p>It takes 1.5 seconds to complete running of K-fold cross-validation method.</p>
</div>
<div id="table-of-comparison-of-these-three-models" class="section level2">
<h2>Table of comparison of these three models</h2>
<p>First, a short revies of model parameters:
RMSE (Root Mean Squared Error) and MAE(Mean Absolute Error) are metric of error.</p>
<p>The R-squared represents the proportion of variation in the outcome explained by the predictor variables included in the model
The higher the R-squared, the better the model (Maximum Rsquared is 1).
The lower the RMSE and the MAE, the better the model.</p>
<pre><code>----------</code></pre>
<table>
<thead>
<tr class="header">
<th>Method</th>
<th>LOOCV</th>
<th>K-fold</th>
<th>Bootstrap</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RMSE</td>
<td>212.2</td>
<td>214.6</td>
<td>229.2</td>
</tr>
<tr class="even">
<td>R^2</td>
<td>0.967</td>
<td>0.967</td>
<td>0.962</td>
</tr>
<tr class="odd">
<td>MAE</td>
<td>150.5</td>
<td>152.2</td>
<td>159.8</td>
</tr>
<tr class="even">
<td>run time(sec)</td>
<td>4.2</td>
<td>1.4</td>
<td>1.5</td>
</tr>
</tbody>
</table>
<pre><code>----------</code></pre>
<p>Values are very close to each other. I tweaked values in models like number and repeats in k-fold method and number in bootstrap method to see their effect. As result, it seems that small changes would not make big change in these parameters, at least for this specific dataset.</p>
<p>About timing it worth mentioning that short times (like 1 seconds) is not fix in multiple simulations and we cannot say fro these results that whether K-fold or Bootstrap is faster, however it is obvious that LOOCV has the least speed.</p>
</div>
<div id="comparing-models-using-r-squared-plot" class="section level2">
<h2>Comparing Models using R squared plot</h2>
<p>In this section, predicted values of diamonds (based on five models in hand from previous section) versus actual price of diamonds from dataset are plotted.This called R squared plot, and it is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.</p>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre><code>## [1] 0.8564319</code></pre>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<pre><code>## [1] 0.991704</code></pre>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-20-3.png" width="672" /></p>
<pre><code>## [1] 0.9673791</code></pre>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-20-4.png" width="672" /></p>
<pre><code>## [1] 0.9670286</code></pre>
<p><img src="https://ramin1ala.netlify.app/posts/2021-07-08-assignment-a01-regression-models/index_files/figure-html/unnamed-chunk-20-5.png" width="672" /></p>
<pre><code>## [1] 0.9626821</code></pre>
<p>Apart from Single regression model (which prediction only made based upon carat), all other models provided similar predictions. Same result deducted by comparing model parameters like error and R.squared. We could also calculate correlation between these predictions to determine their similarities.</p>
</div>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
